<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="author" content="lgswdn">
    
    
    
    
    
    
    <title>CVDL-Transformer | lgswdn-SA</title>
    <link href="http://lgswdn.github.io" rel="prefetch" />

    
<link rel="stylesheet" href="/css/bootstrap.min.css">
<link rel="stylesheet" href="/css/aos.css">
<link rel="stylesheet" href="/css/style.css">

    
<script src="/js/jquery.min.js"></script>

    
<script src="/js/bootstrap.min.js"></script>

    
<script src="/js/aos.js"></script>

    
<script src="/js/highslide/highslide-full.min.js"></script>

    
<link rel="stylesheet" href="/js/highslide/highslide.css">

    <style type="text/css">
        @media (max-width: 768px) {
            body {
                background-color: #f0f0f0;
                background: url('/imgs/xsbg.gif');
                background-attachment: fixed;
            }
        }
    </style>
    
    <!--<script type="text/javascript">
      if (document.images) {
        var avatar = new Image();
        avatar.src = '/imgs/logo.png'
        var previews = 'preview1.jpg,preview2.jpg,preview3.jpg,preview4.jpg,preview5.jpg,preview6.jpg,preview7.jpg,preview8.jpg,preview9.jpg,preview10.jpg,preview11.jpg,preview12.jpg'.split(',')
        var previewsPreLoad = []
        for(var i = 0; i < length; i++) {
          previewsPreLoad.push(new Image())
          previewsPreLoad[previewsPreLoad.length - 1].src = '/imgs/preview' + previews[i]
        }
      }
    </script>-->
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <!-- 背景轮播图功能 -->
    <section class="hidden-xs">
    <ul class="cb-slideshow">
        <li><span>天若</span></li>
        <li><span>有情</span></li>
        <li><span>天亦老</span></li>
        <li><span>我为</span></li>
        <li><span>长者</span></li>
        <li><span>续一秒</span></li>
    </ul>
</section>
    <!-- 欧尼酱功能, 谁用谁知道 -->
    
    <header class="navbar navbar-inverse" id="gal-header">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed"
                    data-toggle="collapse" data-target=".bs-navbar-collapse"
                    aria-expanded="false">
                <span class="fa fa-lg fa-reorder"></span>
            </button>
            <a href="http://lgswdn.github.io">
                
                <style>
                    #gal-header .navbar-brand {
                        height: 54px;
                        line-height: 24px;
                        font-size: 28px;
                        opacity: 1;
                        background-color: rgba(0,0,0,0);
                        text-shadow: 0 0 5px #fff,0 0 10px #fff,0 0 15px #fff,0 0 20px #228DFF,0 0 35px #228DFF,0 0 40px #228DFF,0 0 50px #228DFF,0 0 75px #228DFF;
                    }
                </style>
                <!-- 这里使用文字(navbar_text or config.title) -->
                <div class="navbar-brand">lgswdn-SA</div>
                
            </a>
        </div>
        <div class="collapse navbar-collapse bs-navbar-collapse">
            <ul class="nav navbar-nav" id="menu-gal">
                
                
                <li class="">
                    <a href="/">
                        <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                
                
                <li class="">
                    <a href="/archives">
                        <i class="fa fa-archive"></i>归档
                    </a>
                </li>
                
                
                
                
                <li class="dropdown">
                    <!-- TODO 添加hover dropdown效果 -->
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown"
                       aria-haspopup="true" aria-expanded="false" data-hover="dropdown">
                        <i class="fa fa-list"></i>分类
                    </a>
                    <ul class="dropdown-menu">
                        
                        
                        <li>
                            <a href="/categories/%E9%9A%8F%E8%AE%B0/">随记</a>
                        </li>
                        
                        <li>
                            <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
                        </li>
                        
                        <li>
                            <a href="/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E8%AE%B0/">个人随记</a>
                        </li>
                        
                        
                        <li>
                            <a href="/categories">...</a>
                        </li>
                        
                        
                    </ul>
                </li>
                
                
                
                
                
                <li class="dropdown">
                    <!-- TODO 添加hover dropdown效果 -->
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown"
                       aria-haspopup="true" aria-expanded="false" data-hover="dropdown">
                        <i class="fa fa-tags"></i>标签
                    </a>
                    <ul class="dropdown-menu">
                        
                        
                        <li>
                            <a href="/tags/%E6%80%BB%E7%BB%93/">总结</a>
                        </li>
                        
                        <li>
                            <a href="/tags/AI%E5%9F%BA/">AI基</a>
                        </li>
                        
                        <li>
                            <a href="/tags/CVDL/">CVDL</a>
                        </li>
                        
                        
                        <li>
                            <a href="/tags">...</a>
                        </li>
                        
                        
                    </ul>
                </li>
                
                
                
                
                <li class="">
                    <a href="/about">
                        <i class="fa fa-user"></i>关于我
                    </a>
                </li>
                
                
                
                <li class="">
                    <a href="/resources">
                        <i class="fa fa-file"></i>资源
                    </a>
                </li>
                
                
            </ul>
        </div>
    </div>
</header>
    <div id="gal-body">
        <div class="container">
            <div class="row">
                <div class="col-md-8 gal-right" id="mainstay">
                    
<article class="article well article-body" id="article">
    <div class="breadcrumb">
        <i class="fa fa-home"></i>
        <a href="http://lgswdn.github.io">lgswdn-SA</a>
        >
        <span>CVDL-Transformer</span>
    </div>
    <!-- 大型设备详细文章 -->
    <div class="hidden-xs">
        <div class="title-article">
            <h1>
                <a href="/2025/05/28/CVDL-Transformer/">CVDL-Transformer</a>
            </h1>
        </div>
        <div class="tag-article">
            
            <span class="label label-gal">
                <i class="fa fa-tags"></i>
                
                <a href="/tags/CVDL/">CVDL</a>
                
            </span>
            
            <span class="label label-gal">
                <i class="fa fa-calendar"></i> 2025-05-28
            </span>
            
        </div>
    </div>
    <!-- 小型设备详细文章 -->
    <div class="visible-xs">
        <center>
            <div class="title-article">
                <h4>
                    <a href="/2025/05/28/CVDL-Transformer/">CVDL-Transformer</a>
                </h4>
            </div>
            <p>
                <i class="fa fa-calendar"></i> 2025-05-28
            </p>
            <p>
                
                <i class="fa fa-tags"></i>
                
                <a href="/tags/CVDL/">CVDL</a>
                
                
                
            </p>
        </center>
    </div>
    <div class="content-article">
        <p>primitive：基本的结构单元（例如 cnn, 全连接, attention）</p>
<p>假如我们现在有输出的句子的当前隐藏状态 <span
class="math inline">\(s_i\)</span>，那么对于输入序列的第 <span
class="math inline">\(t\)</span> 个时间步，用一个全连接层算出是一个
alignment score <span
class="math inline">\(e_{i,t}=f(s_i,h_t)\)</span>，然后做 softmax 后得到
<span class="math inline">\(t\to i\)</span> 的一个注意力，将所有 <span
class="math inline">\(h\)</span> 带权相加得一个综合的 <span
class="math inline">\(c_{i}\)</span>. 然后将 <span
class="math inline">\(y_i,c_i,s_i\)</span> 三个再通过 RNN 得到 <span
class="math inline">\(y_{i+1}\)</span>.</p>
<p>这巧妙地将 skip link 用到 seq2seq 问题上. 这个算注意力的 FC
不是通过监督学习的，而是因为整个过程是可微的所以直接反向传递得到.</p>
<p>将这个东西化做一个更加抽象的但更加底层的东西. <span
class="math inline">\(h_t\)</span> 为存储了数据的 data vector，而我们用
<span class="math inline">\(s_i\)</span> 去找哪些 <span
class="math inline">\(h_t\)</span> 应该被注意，称为 query
vector，最终得到 <span class="math inline">\(c_i\)</span> 为 output
vector.</p>
<p>将这个问题抽象出来，并且做一个化简：有维度为 <span
class="math inline">\(D_q=D_x\)</span> 的 query vector <span
class="math inline">\(q\)</span>，维度为 <span
class="math inline">\(N\times D_X\)</span> 的 data vector <span
class="math inline">\(X\)</span>，计算过程为计算出维度为 <span
class="math inline">\(N_X\)</span> 的 alignment score <span
class="math inline">\(e_i=q\cdot X_i/\sqrt{D_X}\)</span>，然后求得注意力
<span class="math inline">\(a=softmax(e)\)</span>，然后算出 <span
class="math inline">\(y=\sum a_iX_i\)</span>.</p>
<p>注意到我们把 <span class="math inline">\(f\)</span> 优化掉了. <span
class="math inline">\(f\)</span>
本质可以看作求两个向量的相似程度，那么不妨用更简单的 cos
similarity：<span class="math inline">\(\frac{a\cdot
b}{|a||b|}\)</span>. 那么化到 <span class="math inline">\(e\)</span>
上就是 <span class="math inline">\(q\cdot X_i/\sqrt{D_q}\)</span>.</p>
<p>但是注意到我们这里擅自把 <span
class="math inline">\(D_Q=D_X\)</span>. 实际上并不是这样. 考虑引入一个
Key Matrix <span class="math inline">\(W_K\)</span>，大小为 <span
class="math inline">\(D_X\times D_Q\)</span>，那么得到 <span
class="math inline">\(N_X\times D_Q\)</span> 的 <span
class="math inline">\(K=XW_K\)</span>. 于是得到 <span
class="math inline">\(E=QK^{T}/\sqrt{D_Q}\)</span>，其中 <span
class="math inline">\(E_{i,j}=Q_i\cdot K_j/\sqrt{D_Q}\)</span>. 然后做
softmax 过后得到 <span class="math inline">\(A\)</span>.</p>
<p>然后我们其实也可能希望输出的维数不一样，设为 <span
class="math inline">\(D_V\)</span>，那么还是考虑引入 Value Matrix <span
class="math inline">\(W_V\)</span>，大小为 <span
class="math inline">\(D_X\times D_V\)</span>，那么得到 <span
class="math inline">\(N_X\times D_V\)</span> 的 <span
class="math inline">\(V=XW_V\)</span>. 然后就可以得到输出 <span
class="math inline">\(Y=AV\)</span>.</p>
<p>这就是 cross-attention 机制，将两个不同的东西融合在了一起. 里面每个
cell 都对 data 有全局的 receptive field，而对 query 只能看到自己.</p>
<p>self attention：加入一个 <span
class="math inline">\(W_Q\)</span>，并将 <span
class="math inline">\(X\)</span> 通过 <span
class="math inline">\(XW_Q\)</span> 来获得 <span
class="math inline">\(Q\)</span>. 并同一一下 <span
class="math inline">\(D_Q=D_V=D_{out}\)</span>，然后 <span
class="math inline">\(D_x=D_{in}\)</span>.</p>
<p>我们发现这个东西输入为 <span class="math inline">\(N\times
D_{in}\)</span>，并输出为 <span class="math inline">\(N\times
D_{out}\)</span>. 其本质是新的，对一族 vector 进行了一次加工，其
receptive field 为全局.</p>
<hr />
<p>考虑 cross-attetion，其对 data vector <span
class="math inline">\(X\)</span> 是 permutation invariance 的，而对于
query vector <span class="math inline">\(Q\)</span> 是 permutation
equivariance 的. 而对于 self-attention，则对 <span
class="math inline">\(X\)</span> 是 permutation invariance 的.</p>
<p>然后从某个角度讲，每个 query 生成 <span
class="math inline">\(y\)</span> 和 pointnet
有所相似性：每个东西内部做一个 FC 然后某种方式 pool 一下就好了.</p>
<p>当然这带来了一个问题. self attention 根本不知道序列的位置信息.
解决这个也很简单，搞一个位置函数，将位置 <span
class="math inline">\(n\)</span> 通过这个函数得到一个 positional
embedding 然后 concat 到 <span class="math inline">\(X\)</span> 上.</p>
<p>但是对于一个预测下一个词的任务，我们无法知道完整序列信息直接运用
self-attention.</p>
<p>于是引入 Masked self-attention Layer（Causal Mask，因果
mask，不能让未来的事情被过去的事情看到）. 我们只需要在 所有 <span
class="math inline">\(i&lt;j\)</span> 的地方强制 <span
class="math inline">\(E_{i,j}=-\infty\)</span>，这样注意力就为 <span
class="math inline">\(0\)</span>.</p>
<p>于是我们就得到了一个 RNN 的替代. 这相比于 RNN 来说，一个优势是 RNN 是
sequential 的，而上述的训练可以并行地训练！</p>
<hr />
<p>Multiheaded Self Attention Layer：<span
class="math inline">\(m\)</span> 个独立的 attention 可以独立运作，形成
<span class="math inline">\(m\)</span> 个 head，输出 <span
class="math inline">\(m\)</span> 套 <span
class="math inline">\(y\)</span>，然后再合并起来.</p>
<p>令输入维数=最终输出维数= <span class="math inline">\(D\)</span> 维.
令 <span class="math inline">\(D_H=D/H\)</span>，每个 attention 都是
<span class="math inline">\(D_{H}\)</span> 维的，然后 最后将 <span
class="math inline">\(H\times N\times D_{H}\)</span> 的 <span
class="math inline">\(Y\)</span> 叠成 <span
class="math inline">\(N\times D\)</span> 的 size 之后有 output <span
class="math inline">\(O=YW_{O}\)</span>.</p>
<p>Self Attention 本质上就是 四次矩阵乘法.</p>
<p>第一次矩阵乘法是 QKV Projoection，<span
class="math inline">\([N\times D]\times [D\times 3HD_{H}]\)</span> 得到
<span class="math inline">\(N\times 3HD_H\)</span> 然后 split and
reshape 之后 <span class="math inline">\(Q,K,V\)</span>.</p>
<p>第二次矩阵乘法是做 <span class="math inline">\(QK\)</span>
similarity，即计算 <span class="math inline">\(QK^{T}\)</span>.</p>
<p>第三次是作用上 <span class="math inline">\(V\)</span>. <span
class="math inline">\([H\times N\times N][H\times N\times D_H]\to
[H\times N\times D_H]\)</span>.</p>
<p>第四次则是最后做 <span class="math inline">\([N\times D][D\times
D]\to [D\times D]\)</span> 的 output projection.</p>
<p>多头注意力在计算上计算量并没有怎么减少，但是表达能力是更强的.</p>
<p>上述的一个问题就是需要存 <span class="math inline">\(N\times
N\)</span> 的大矩阵，memory 无法承受.</p>
<p>Flash Attention：将第二三步化作同一步，获得 <span
class="math inline">\(O(N)\)</span> 的存储复杂度. 这样就能加大 <span
class="math inline">\(N\)</span>.</p>
<hr />
<p>对比三种 process sequence 的方法：RNN
理论上是线性的可以处理比较长的东西，但是并不那么靠谱并且不并行化；而 CNN
要建立大 receptive field layer 要叠太多了；而 Self Attention
并行化并且有全局的 receptive field，但是复杂度很大，很贵.</p>
<p>Transformer：完全使用 Attention 搭网络.</p>
<p>Transformer Block：<span class="math inline">\(x\)</span> 通过
multiheaded self attention 得到 <span
class="math inline">\(y\)</span>，加 residual connection。然后做 Layer
Normalization，这使得每个 token 的参数都对应相同的分布. 然后对于每个
vector 单独做 MLP（称为 FFN 层），然后加 residual connection，加 layer
norm.</p>
<p>不同 token 间交流的时间只有 self-attention，而剩下的 layernorm 和 FFN
都是每个 token 单独完成的.</p>
<p>上面的计算只有来自 self-attention 的 4 个 MatMul，然后 FNN 有两个
MatMul. 很并行化.</p>
<p>online / offline processing：前者的任务需要 causal mask
而后者就不需要了. 训练和推理的时候 causal mask 必须同时用/不用.</p>
<p>处理语言任务的时候要学习一个 Embedding matrix（将词映成 vector） 和
Projection matrix（将 vector 映成词）.</p>
<hr />
<p>ViT：将图片切成若干个大小为 <span class="math inline">\(16\times
16\times 3\)</span> 的 patches，然后每个 patch 提一个特征，然后 256 个
token 喂给 transformer. 对于提特征，由于图很小，所以直接 MLP 也没啥.
跑完之后做一个 global average pooling 然后过 linear layer 做
classification. 需要 positional embedding 以及不需要 causal mask.</p>
<p>一些改进：Pre-Norm. 在 residual link 之后做 norm 还是不是很好学到
identity function. 所以考虑如下的形式：把 layer norm 放到 self attention
/ MLP 之前，被 residual link 包裹. 这样训练更加稳定.</p>
<p>RMS Norm（Root Mean Square）：要学一个 <span
class="math inline">\(\gamma\)</span>，然后令 RMS 为 <span
class="math inline">\(\sqrt{\frac{1}{n}\sum
x_i^2+\epsilon}\)</span>，然后 <span
class="math inline">\(y_i=\frac{x_i}{RMS}\gamma\)</span>. 这会使得最终的
RMS 为 <span class="math inline">\(\gamma\)</span>.</p>
<p>还有一个对 FFN 做优化的 SwiGLU.</p>
<p>MoE（混合专家模型）. 考虑原本是一个 <span
class="math inline">\(W_1=D\times 4D\)</span>，<span
class="math inline">\(W_2=4D\times D\)</span>
的矩阵，然后现在我们扩展成有 <span class="math inline">\(E\)</span> 个
<span class="math inline">\(W_1\)</span> 和 <span
class="math inline">\(E\)</span> 和 <span
class="math inline">\(W_2\)</span>. 每个 MLP 被称为 Expert. 然后每个
token 会被 route 到 <span class="math inline">\(A\)</span> 个
Expert（<span class="math inline">\(A&lt;E\)</span>）. 那么当参数量
<span class="math inline">\(E\)</span> 增加了并不会增加计算量，而只有
<span class="math inline">\(A\)</span> 会控制 Foward 的计算量. Routing
的学习挺困难的，因为是否选择由 Top-A 决定，这是不可导的.</p>
<hr />
<p>参考讲义：<a
href="../../../../file/CVDL/Lecture%2014%20-%20Transformer.pdf">Lecture
14 - Transformer</a></p>

    </div>
</article>




  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


                </div>
                <aside class="col-md-4 gal-left" id="sidebar">
    <!-- 此为sidebar的搜索框, 非搜索结果页面 -->
<aside id="sidebar-search">
    <div class="search hidden-xs" data-aos="fade-up" data-aos-duration="2000">
        <form class="form-inline clearfix" id="search-form" method="get"
              action="/search/index.html">
            <input type="text" name="s" class="form-control" id="searchInput" placeholder="搜索文章~" autocomplete="off">
            <button class="btn btn-danger btn-gal" type="submit">
                <i class="fa fa-search"></i>
            </button>
        </form>
    </div>
</aside>
    <aside id="sidebar-author">
    <div class="panel panel-gal" data-aos="flip-right" data-aos-duration="3000">
        <div class="panel-heading" style="text-align: center">
            <i class="fa fa-quote-left"></i>
            lgswdn
            <i class="fa fa-quote-right"></i>
        </div>
        <div class="author-panel text-center">
            <img src="/imgs/logo.png" width="140" height="140"
                 alt="个人头像" class="author-image">
            <p class="author-description"></p>
        </div>
    </div>
</aside>
    
    
    
    <!-- 要配置好leancloud才能开启此小工具 -->
    
    
    
    
    <aside id="gal-sets">
        <div class="panel panel-gal hidden-xs" data-aos="fade-up" data-aos-duration="2000">
            <ul class="nav nav-pills pills-gal">

                
                <li>
                    <a href="/2025/05/28/CVDL-Transformer/index.html#sidebar-tags" data-toggle="tab" id="tags-tab">热门标签</a>
                </li>
                
                
                
                <li>
                    <a href="/2025/05/28/CVDL-Transformer/index.html#sidebar-links" data-toggle="tab" id="links-tab">个人链接</a>
                </li>
                
            </ul>
            <div class="tab-content">
                
                <div class="cloud-tags tab-pane nav bs-sidenav fade" id="sidebar-tags">
    
    <a href="/tags/%E6%80%BB%E7%BB%93/" style="font-size: 14.00995954537321px;" class="tag-cloud-link">总结</a>
    
    <a href="/tags/AI%E5%9F%BA/" style="font-size: 11.607038997603002px;" class="tag-cloud-link">AI基</a>
    
    <a href="/tags/CVDL/" style="font-size: 8.163055642002476px;" class="tag-cloud-link">CVDL</a>
    
    <a href="/tags/ICS/" style="font-size: 9.379664019962291px;" class="tag-cloud-link">ICS</a>
    
    <a href="/tags/%E5%85%B7%E8%BA%AB/" style="font-size: 15.376906651799658px;" class="tag-cloud-link">具身</a>
    
    <a href="/tags/%E5%88%9D%E6%81%8B%E6%97%A5%E8%AE%B0/" style="font-size: 8.366472049856966px;" class="tag-cloud-link">初恋日记</a>
    
    <a href="/tags/%E8%85%BE%E8%AE%AF%E6%98%9F%E7%81%AB%E8%90%A5/" style="font-size: 9.926180412609611px;" class="tag-cloud-link">腾讯星火营</a>
    
    <a href="/tags/%E6%95%B0%E5%88%86/" style="font-size: 15.664709880601102px;" class="tag-cloud-link">数分</a>
    
    <a href="/tags/%E7%A8%8B%E8%AE%BE/" style="font-size: 12.627868996076824px;" class="tag-cloud-link">程设</a>
    
    <a href="/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/" style="font-size: 8.261256840097168px;" class="tag-cloud-link">中国的亚洲内陆边疆</a>
    
    <a href="/tags/%E6%B3%A2%E4%BC%8F%E5%A8%83/" style="font-size: 13.746932672480733px;" class="tag-cloud-link">波伏娃</a>
    
    <a href="/tags/%E9%9F%B3%E6%95%B0/" style="font-size: 19.41536505080937px;" class="tag-cloud-link">音数</a>
    
    <a href="/tags/%E9%AB%98%E4%BB%A3/" style="font-size: 17.542209997180443px;" class="tag-cloud-link">高代</a>
    
</div>
                
                
                
                <div class="links tab-pane nav bs-sidenav fade" id="sidebar-links">
    
    <li>
        <a href="https://github.com/lgswdn" target="_blank">Github</a>
    </li>
    
    <li>
        <a href="https://x.com/lgswdn_ChrisZ" target="_blank">X (Twitter)</a>
    </li>
    
    <li>
        <a href="https://www.luogu.com.cn/user/180652" target="_blank">洛谷</a>
    </li>
    
</div>
                
            </div>
        </div>
    </aside>
    
</aside>
            </div>
        </div>
    </div>
    <footer id="gal-footer">
    <div class="container">
        Copyright © 2018 lgswdn Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>.&nbsp;Theme by <a href="https://github.com/ZEROKISEKI" target="_blank">AONOSORA</a>
    </div>
</footer>

<!-- 回到顶端 -->
<div id="gal-gotop">
    <i class="fa fa-angle-up"></i>
</div>
</body>

<script src="/js/activate-power-mode.js"></script>

<script>

    // 配置highslide
	hs.graphicsDir = '/js/highslide/graphics/'
    hs.outlineType = "rounded-white";
    hs.dimmingOpacity = 0.8;
    hs.outlineWhileAnimating = true;
    hs.showCredits = false;
    hs.captionEval = "this.thumb.alt";
    hs.numberPosition = "caption";
    hs.align = "center";
    hs.transitions = ["expand", "crossfade"];
    hs.lang.number = '共%2张图, 当前是第%1张';
    hs.addSlideshow({
      interval: 5000,
      repeat: true,
      useControls: true,
      fixedControls: "fit",
      overlayOptions: {
        opacity: 0.75,
        position: "bottom center",
        hideOnMouseOut: true
      }
    })

    // 初始化aos
    AOS.init({
      duration: 1000,
      delay: 0,
      easing: 'ease-out-back'
    });

</script>
<script>
	POWERMODE.colorful = 'true';    // make power mode colorful
	POWERMODE.shake = 'true';       // turn off shake
	// TODO 这里根据具体情况修改
	document.body.addEventListener('input', POWERMODE);
</script>
<script>
    window.slideConfig = {
      prefix: '/imgs/slide/background',
      ext: 'jpg',
      maxCount: '6'
    }
</script>

<script src="/js/hs.js"></script>
<script src="/js/blog.js"></script>




</html>