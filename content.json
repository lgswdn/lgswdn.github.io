{"meta":{"title":"lgswdn-SA","subtitle":"","description":"","author":"lgswdn","url":"http://lgswdn.github.io","root":"/"},"pages":[{"title":"about","text":"关于我 这是一个用来发布个人生活/学习相关 blog 的小网站. 前信息学竞赛选手（CNOIer），毕业于上外附中与华东师大二附中. 现就读于北京大学信息科学技术学院. 邮箱：chriszhulswn@gmail.com，3300619365@qq.com 不扩列，加 qq 请注明来意. 友链 En0nya's Blog","path":"about/index.html","date":"02-22","excerpt":""},{"title":"404","text":"","path":"404/index.html","date":"02-22","excerpt":""},{"title":"Resources","text":"","path":"resources/index.html","date":"09-15","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"02-22","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"02-22","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"02-22","excerpt":""}],"posts":[{"title":"ICS-数据类型","text":"Lecture 4 Machine-Level Programming I: Basics intro Architecture（架构）：指令集. Microarchitecture：指令集的一些实现. Machine code：机器代码（binary）. Assembly code：汇编代码（text representation of machine code）. CPU 上的一些东西： PC: Program Counter，记录程序跑到哪一行 (address of next instruction, %rip). Register file: Registers. Condition Code: Condition Code. Memory: 存储了 Code, data, stack. Memory 给 CPU 传 instruction 和 data，CPU 反过来给 Memory 传 data 和 addresses. 一个 C 程序 x.c 通过 gcc -Og -S 得到汇编程序 Asm Program x.s. Asm Program 再通过 Assembler 转化为 binary 的 Object Program x.o. 然后再结合 Static Library (一些 printf 类的东西依赖这个) ，通过 Linker 得到最终的可执行文件 x. registers 64 位寄存器： rax, rbx, rcx, rdx, rdi, rsi, rsp, rbp, r[8,15]. 其对应的后 32 位是： eax, ebx, ecx, edx, edi, esi, esp, ebp, r[8,15]d 其对应的后 16 位是： ax, bx, cx, dx, di, si, sp, bp, r[8,15]w. 其对应的最后一个 byte 是： al, bl, cl, dl, dil, sil, spl, bpl, r[8,15]b 其中 rsp 存 stack pointer，不能被肆意使用，其他都可以随便用，然后可能会在某些情境下被指定使用. 其中 rax 记录函数的返回值，rdi rsi rdx rcx r8 r9 分别作为传递进函数的第 \\([1,6]\\) 个参数. operations 在 Linux 中，为 op Src Dest. 但是在 Intel 下是 op Dest Src. 书中全部采用前者，即 addq x y 为 \\(y+=x\\). q 表示四字（quadword，64位），l 表示双字（longword，32位），w 表示字（word，16位）, b 表示字节（byte）. mov 相关注意事项： mov 指令 Src 是立即数而 Dest 是内存地址，必须指定是 movq 还是 movl 还是 movw 还是 movb. movl 会自动置零更高的 32 位，而其他不会. movz（move with zero-extend），从一个较小的 src 移到较大的 dest（eg. movzwq，movbl），用 0 扩充. 注意没有 movzlq 因为等价于 movq. movs（move with sign-extend），同上，用符号位扩充. rax 相关特殊指令： cltq：针对 eax，将 eax 符号扩充到 rax 变成 64 位数. cqto：针对 rax，将 rax 符号扩充到 rdx:rax 变成 128 位数. imulq S：rdx:rax = rax * S（有符号） mulq S：rdx:rax = rax * S（无符号） idivq S：对于 128 位有符号整数 rdx:rax，除以 \\(S\\)，得到的商在 rax，余数在 rdx. 执行前必须先 cqto. divq S 为无符号. D(Rb,Ri,S) = Mem[D+Rb+Ri*S]. 注意 \\(S\\) 只能是 1,2,4,8. lea D(Rb,Ri,S), %rax 等于 rax = D+Rb+Ri*S，或者 rax=&amp;.... mov D(Rb,Ri,S), %rax 等于 rax = Mem[D+Rb+Ri*S] add，sub，imul 字面意思，加减乘. sal / shl，sar，shr 为左右移，其中 h 为逻辑，a 为算数. xor，and，or 字面意思，异或，与，或. in，dec，neg，not 分别为 \\(++\\)，\\(--\\)，\\(-\\)，\\(\\sim\\). Lecture 05: Machine-Level Programming II: Control Conditional Codes For any arithmetic operation (EXCLUDING leaq instruction) CF：Carry Flag（unsigned overflow, borrow/carry from the MSB） ZF：Zero Flag（if =0） SF：Sign Flag（&lt;0 as signed） OF：Overflow Flag (if signed overflow, eg. \\(a,b&gt;0\\) but \\(a+b&lt;0\\), or \\(a,b&lt;0\\) but \\(a+b\\ge 0\\)) Explicit Settings cmpq a b \\(b-a\\), calculate w/o setting destination. CF：unsigned comparison, \\(=[b&lt;a]\\). ZF：eq, \\(=[a=b]\\). SF：signed comparison, \\(=[b&lt;a ]\\). OF：2's compliment signed overflow testq a b \\(b\\text{ and }a\\), calculate w/o setting destination ZF：\\(=[b\\text{&amp;}a=0]\\). SF：\\(=[b\\text{&amp;}a&lt;0]\\). use testq a a to test equal. Reading setX：set the low byte of destination to: setX &amp; Description Condition sete / setne (equal) ZF / ~ZF sets / setns (negative) SF / ~SF setge / setg (\\(b\\ge a\\), \\(b&gt;a\\), signed) ~(SF^OF), &amp;(~ZF) if not equal setle / setl (\\(b\\ge a\\),\\(b&lt;a\\), signed) (SF^OF), &amp;ZF if equal seta / setb (above=\\(b&gt;a\\), below=\\(b&lt;a\\)) (~CF)&amp;(~ZF) for above, CF for below use movzbl after SetX, so that the whole register represent the result. Conditional Branches jX: similar to setX, with additional jmp (unconditional jump) cmovX: similar to setX, (if condition then move). Sometimes the compiler calculate both side results and use cmovX for conditional branches. But it must be side-effect free, and simple calculations. Loops 很简单","path":"2025/09/23/ICS-汇编1/","date":"09-23","excerpt":"","tags":[{"name":"ICS","slug":"ICS","permalink":"http://lgswdn.github.io/tags/ICS/"}]},{"title":"具身-Robotic Tasks","text":"Body Control Proprioception Robot state (Exteroception / proprioception) -&gt; (through policy) -&gt; PD control -&gt; (get torque for joints) -&gt; Motor driver -&gt; (through current) -&gt; Robot Proprioception 包含了机器人内部的各种信息，比如 joint position / velocity，IMU (Inertial Measurement Unit) 有 accelerometer (线加速度器)，gyroscope（陀螺仪，感受角速度）. 一些 Exteroception 包括了一些比如 vision 信息，lidar point cloud，RGBD 什么的. Sim2real gap 很大，所以需要在 stimulator 中加噪声. Represent body pose 可以通过 joint space 和 Link space. Joint space 只记录角度和角加速度，Link space 记录了每个 link 的 position 和速度和加速度. Link space 可以记录在世界坐标系下的东西，可以 encode spatial relation，所以在跳舞这种 task 一般用 Link space. Joint space 只记录自己坐标系下的，维度更低，相对更容易 train，维护自身平衡什么的更多. Policy 的输出是一个 residual DoF Position，\\(\\theta_0\\) 是每个关节的不动的一个 reference position，\\(\\theta\\) 是当前 position，网络输出 target position 关于 \\(\\theta_0\\) 的 shift，然后通过 \\(e(t)=\\pi(s_t)+\\theta_0-\\theta\\) 得出 error term，然后再 PD Control \\(\\tau_t=K_pe(t)+K_d\\frac{d}{dt}e(t)\\). 当然也可以用关于 velocity 的 PD control，或者直接做 torque control. 但是这两者的 sim2real gap 太大了. 还有一个事情就是 torque 的更新频率要很高（1000Hz），显然不能直接用 policy 算 torque. reward terminal reward：只有最后结果有 reward. 不容易学好. 但是真机就只能这样. 在 stimulator 中信息都可以得到，所以可以做更多的 reward. 一个四足机器人的可能的一些 reward: height（保证高度），feet contact（保证脚在地面上），upright（保证是背的法向指着天），energy（penalize 太高的 energy），joint limit（penalize 太大的 joint angle） reward 的系数并不好调. 和需要先做到哪个再做到哪个也有关系（先做到什么什么系数就大一点）. Q Learning：通过学 \\(a=\\mu(s)\\)，然后最大化 \\(Q(s,\\mu(s))\\). 可以支持更 off-policy 的东西，有更高的 sample efficiency. 比如 DDDG，SAC. 真机的强化学习可以使用 SAC 这样的方法. 因为真机环境需要很高的 sample efficiency. 耗能问题：如果纯靠 RL 学出来，为了能在 domain randomization 中存活下来，动作偏保守，在真实世界中耗能很大. Curriculum Learning：不断在学习中加入 obstacle，环境不断变困难. Hierarchical Learning：划分层级，从 high level perception module，往下走到 navigation module，再往下到 locomotion module. locomotion 有 skill library 供 navigation module 调用. (ANYmal Parkour) Privileged Learning Privileged Learning：现在考虑加入视觉信息. 但是一个问题是直接接入 visual encoder 之后其实也还是训不好. 因为一方面很大，一方面 variance 也很大，传到 visual encoder 的信息已经没什么营养了. 如果直接先训 locomotion 锁住再训 encoder 那么又会导致 locomotion 也在学 high level 的东西，显然不大好. 一个方法是，在训 locomotion 的时候直接把能通过视觉得到的一些 state 信息（称为 privilege information，即 proprioception 无法得到的 state 信息）给传进去（即训出一个 state-based model，而不是 proprioception-based model）. Policy Distill Policy Distillation：在我们有了一个 state-based model 之后，对于从 \\(\\pi_{\\theta}\\) 中采的 state \\(s\\)，我们可以用这个 teacher model 去得到 \\(a\\)，然后让 \\(\\theta\\) 去学这个 \\(a\\). 所以就直接监督学习就完了. 注意，当我们的 state 太强了，超越了视觉能看到的信息，就会效果很不好. 一个需要注意的一点是，RL 出来的 teacher 可能也不知道在一个随机的 student 的分布上应该怎么走. 如果 student 能够成功走到 teacher 的轨迹上那就可以学，否则可能怎么学都学不到 teacher 的轨迹上. Asymmetric Actor-Critic: actor-critic 中，Critic 可以拥有 privilege information，可以 state based. 但效果不如 Policy Distill. Bipedel 双足比四足难很多. 双足非常依赖 IMU. 这个也容易导致 sim2real，因为实际中 IMU 有若干噪声，所以必须在 sim 中加上对 IMU 的噪声. 一个方法是人动捕，然后将这个信息适配到机器人的上面，然后再一帧一帧修，得到机器人应该做的动作. 每个舞都是在一个基础的 model 上单独去 overfit 的，而不是很 universal 的. unitree 的小人比较轻，关节负担小，适合做一点比较大的动作. Domain Randomization 可以 randomize 一些 center of mass，mass，friction. RL in sim 然后再 2 real 实际效果有点神秘，还要调一大堆参数. Manipulation 最基本的是 pick，然后 place. 稍微难一点就 assembly，还有 peg-in hole，这种 contact-rich 的操作. GAIL 一个 on-policy 的 grasping RL：先分为若干个 stage，然后每个 stage 需要一个 RL，结合了 IL（GAIL，用 GAN 训练一个 reward generator）和 RL（PPO）. GAIL 的思想：有 demonstration 但是不想自己造 reward 怎么训 RL？GAIL 提出的想法是，使用 GAN 的思想，训一个 discriminator 来看生成的轨迹和 demo 的轨迹是否足够像，然后 reward 就用 discriminator 给的分数，然后再用一个 PPO. GA-DDPG 使用以 Q-Learning 为中心的方法. Q Learning 先学 \\(Q(s,a)\\)（用 bootstrap loss (or, Bellman Loss)），最小化 \\(E_{(s,a,r,s&#39;)\\sim D}[||Q(s,a)-(r+\\gamma Q(s&#39;,\\mu_{\\theta}(s&#39;))||_2]\\)）, \\(\\theta\\) 的目标是最大化 \\(E_{s\\sim D}[Q(s,\\pi_{\\theta}(s))]\\)，其中 \\(D\\) 为 replay buffer. GA 指的是在 DDPG 的基础上，考虑 replay buffer 里面放一些 goal oriented 的数据. Dex Manipulation","path":"2025/09/20/具身-Robotic Tasks/","date":"09-20","excerpt":"","tags":[{"name":"具身","slug":"具身","permalink":"http://lgswdn.github.io/tags/%E5%85%B7%E8%BA%AB/"}]},{"title":"ICS-数据类型","text":"Lecture 2 - Integer Unsigned &amp; Signed Byte = 8 bits. Why？为了适应字符集大小. 64 位系统和 32 位系统的一些区别是，long 类型和 pointer 在 32 位是 4 Bytes, 在 64 位是 8 Bytes. Two's Compliment：补码，即用 \\(-x_{w-1}2^{w-1}+\\sum_{i=0}^{w-2}{x_i}2^i\\) 来表示数. 在这个系统下，最小值 \\(TMin=-2^{w-1}\\)，最大值 \\(TMax=2^{w-1}-1\\). 可以发现这个系统下，负数间的比较是自然的，并且运算，左移，算数右移也是自然的. 在表达式中，如果有 Unsigned，那么 signed 都会被 cast to unsigned. casting 的时候表现上可能会 \\(\\pm 2^{w}\\). truncate：截断的时候，mod \\(2^w\\) 意义下保持不变. expand：扩展的时候，采用 sign expansion，用符号位填充. Operation 随便想一下，就可以发现加法对于 two's compliment 都是很自然的，直接当 unsigned 去加去截断，就对了. 减法取反 \\(+1\\) 一下就是了. 在 2's compliment 下，其实 signed 和 unsigned 是本质相同的. 都是考虑在 \\(\\bmod 2^w\\) 的群下做操作，然后 signed 的话溢出部分就是 \\(-2^{w}\\). &gt;&gt;k 得到 \\(/2^k\\) 后下取整的结果. 这是因为 2's compliment 后面其实都是加法，所以不会向 0 取整.然后 arithmetic shift 需要前面填充符号位. Memory, Pointer, Strings 使用 32-bit word size 的系统，memory 被限制在了 4GB. 使用 64-bit word size 的系统，memory 可以达到 18EB（\\(18\\times 10^{18}\\)）. Big Endian：most significent bit 存在更前面（Sun, PPC MAC, Internet） Little Endian：least significent bit 存在更前面（x86, ARM, Andriod iOS Windows） 将 pointer cast 成一个 unsigned char * 那么这个 pointer 就会指向一个 byte 的信息，可以用来 print byte. string 中，每个 char 都只占一个 byte 所以显然没有 ordering issue. int('0')=0x30 Lecture 3 - Float IEEE Standard single precision: 1 sign bit, k=8 exp bits, m=23 frac bits \\(x_1\\dots x_{23}\\) double precision: 1 sign bit, k=11 exp bits, m=52 frac bits \\(x_1\\dots x_{52}\\) \\(s\\ \\{e\\}\\ \\{x\\}\\) \\(bias=2^{k-1}-1\\)（127 for float, 1023 for double） \\(val=(-1)^s\\times 2^{E}\\times M\\) in the following cases (E = Exponent, M=Significand): denormalized (exp = 00..0) \\(E=1-bias\\)，\\(M=0.x_1\\dots x_m\\) \\(\\{\\pm0\\}\\cup\\pm [2^{1-bias-m},2^{bias-1}(1-2^{-m})]\\) normalized (00..0 &lt; exp &lt; 11..1) \\(E=e-bias\\)，\\(M=1.x_1\\dots x_m\\) \\(\\pm[2^{1-bias},2^{bias}(2-2^{-m})]\\) special value (exp = 11..1) \\(x=[00..0]\\implies \\pm \\infty\\) else \\(\\implies \\text{nan}\\). \\(1\\)：\\(e=011..1\\)，\\(x=00..0\\). operation special case: \\(\\infty-\\infty\\)，\\(\\infty\\times 0\\) produce \\(\\text{nan}\\). Rounding: （截断的时候）四舍五入，但是恰好为 \\(...x_{m-2}x_{m-1}x_{m}|100...0\\) (round bit=1, sticky bits=0)的时候取整到最近的偶数使得 LSB 为 \\(0\\). 注意 round 之后有可能造成 frac bits 的 overflow，需要调整 exp（可能要从 denorm 变成 norm）. Addition inverse &amp; Monocity stands except for \\(\\infty\\) and \\(\\text{nan}\\). 没有结合律和分配律. Conversion: 转成 int 的时候，直接截断（即向 \\(0\\) 取整） out of range 或者 nan 或者 inf 直接变成 \\(TMin\\). (32 bit) int to double 可以完全不丢精度. int to float 的时候需要用到向偶数取整. test","path":"2025/09/15/ICS-数据类型/","date":"09-15","excerpt":"","tags":[{"name":"ICS","slug":"ICS","permalink":"http://lgswdn.github.io/tags/ICS/"}]},{"title":"具身-Policy","text":"Notation： \\(s_t\\) state，环境&amp;物体状态 \\(o_t\\) observation，观测（图片/点云） \\(a_t\\) action，采取的策略 \\(\\pi_{\\theta}(a_t|o_t)\\)：policy \\(\\pi_{\\theta}(a_t|o_t)\\)：fully observed 下的 policy，比较理想，在理论分析上能有简化 一些传统的算法可以作为一个 supervision 去 train policy. 当我们无法建立一个 optimal policy 的时候，无法使用 imitation learning 的时候就需要用 reinforcement learning. Imitation Learning 可以采取一个 Markov 过程，只根据上一个状态做出决定. 但是实际上也是可以说去有一个记忆，根据历史的状态做出判断. 最基本的方案：behavioral cloning Distribution Shift 这个方案很早就又提出了，但是有一个问题是 distribution shift：每一步都出一点小差错，然后最后就倒闭了. 这是因为你的 state 是由 action 决定的，于是 distribution 就会变得不同. 但在数据足够大的情况下，表现还是相当好的. 一个经典的方法是直接采取 teleoperation. 这个的好处是不存在 sim2real gap. 但是取而代之的是 real2real gap，比如如何做 generalization，因为环境变化非常小. 还可以在虚拟环境中进行摇操，这些 data 就会更加丰富一点. 为了消除 distribution shift，有两种方向：可以去让 \\(p_{data}\\) 更靠近真实情况，也可以让我们更好地去 fit 到专家的轨迹. DAgger DAgger: Dataset Aggregation：很暴力地将 \\(p_{\\pi_\\theta}\\) 加入数据集. 先按原来的数据去学 policy，然后用这个 policy 在环境中去探索，然后对于探索到的 state，再让人去 label，并入数据集. 不断这样循环往复，就得到了一个能够去适应 \\(p_{\\pi_{\\theta}}\\) 的数据集. 但是 human label 很难. 有两种思路：对于有传统 optimal solution 的东西（比如 navigation） 那么显然就可以直接自己 label. 第二种思路是 from a teacher policy. 比如我们假如已经有基于有完整信息时的开环的抓取策略，那么就可以用这个策略去训练一个基于 RGB 的闭环的抓取 policy. Casual confusion 探讨为什么我们的策略会 fail to fit expert? 一个原因就是我们的 policy 是 non-markovian 的. casual confusion：比如在开车，然后学到“刹车灯亮了就去踩刹车”. 而不是学到了一些环境因素才去踩刹车. Multimodal behaviour 绕一棵树，然后一半数据是向左绕的，一半数据是向右绕的，然后 regression 的结果就会直接往中间撞上去. 一个方法是预测一个多峰的分布，比如使用 diffusion model，或者 autoregressive discretization（依次autoregressive 地去求解每个维度的概率分布） Multitask training 很多个目标一起去 train. 具体而言，就是将 goal 和 state 一起作为 condition 加进去，这样就能很多个 task 一起学，让所有 task 都学的更好. 当然这也会带来一定的挑战 ，就是没见过的 goal 带来的 distribution shift. Reinforcement Learning Intro 强化学习也 assume markovian policy Markov Decision Process (MDP): \\(S\\) 为状态空间，\\(A\\) 为动作空间，\\(T\\) 为状态转移，\\(r\\) 为 \\(S\\times A\\to r\\) 的奖励函数. 但是我们观测到的可能并不是整个环境. 于是有 partially observed MDP：令 \\(O\\) 为 observation space，\\(\\varepsilon\\) 为 emission probability \\(p(o_t|s_t)\\)：在状态下观测到 \\(o\\) 的概率. 我们的目标就是，首先我们有 MDP 的分布：\\(p(s_1,a_1,\\dots,s_T,a_T)=p(s_1)\\prod \\pi_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)\\). 然后我们就能算出奖励 \\(E_{\\tau\\sim p_{\\theta}(\\tau)} \\sum r(s_t,a_t)\\). 目标就是最大化这个期望奖励. 显然我们是不知道 \\(p(s_{t+1}|s_t,a_t)\\) 的. 有些方法会去学这些，称为 model-based. 下面讲的是不去学这个的，即 model-free. reward 函数不连续，所以不可导. 但是之后的期望收益是连续的. 所以这个期望其实很重要. Gradient 怎么对奖励导呢？首先 先把期望写作积分的形式：\\(\\int r(\\tau)p_{\\theta}(\\tau) d\\tau\\). 然后我们希望求导后的积分中还有项 \\(p(\\tau)\\)（这样才能重新转化回期望）. 由于 \\(p_{\\theta}(\\tau)\\nabla_\\theta \\log p_{\\theta}(\\tau)=\\nabla_\\theta p_{\\theta}(\\tau)\\)，所以我们对积分求导就可以直接变换成 \\(\\int p_{\\theta}(\\tau)\\nabla_\\theta r(\\tau)\\log p_{\\theta}(\\tau)=E[\\nabla_\\theta \\log p_{\\theta}(\\tau)r(\\tau)]\\). 对这个 \\(\\log p_{\\theta}(\\tau)\\) 求个导，乘法变成了加法，然后 \\(\\theta\\) 无关项也没了，只剩下 \\(\\sum \\log \\pi_{\\theta}(a_t|s_t)\\) 了. 于是就变成了 \\(E_{\\tau}[\\left(\\sum \\nabla_{\\theta}\\log \\pi_{\\theta}(a_t|s_t)\\right)(\\sum r(s_t,a_t))]\\) 期望显然不大好算. 于是直接 Monte Carlo，采 \\(n\\) 条 trajectory 即可. 我们对比一下和模仿学习的. 模仿学习其实就是一条好的轨迹给出来，然后 gradient 就是 \\(\\sum \\nabla_\\theta \\log \\pi_{\\theta}(a_t|s_t)\\). 所以本质上就是所有 reward 都是 1. 模仿学习是 offline policy. 强化学习是一种 online policy. online policy 又分为 on policy 和 off policy. on policy 严格要求 gradient 由当前 policy 生成，而 off policy 则可以根据 old policy 进行更新. 上面的 RL 就是一种 on policy. 这是因为 policy 变了，你的 trajectory state 就完全变了，你的 training data 的分布也就变了. Example: Gaussian Policy 如果我们每一步的 \\(p\\) 看作一个 gaussian，神经网络计算出来的 policy 是 \\((f,\\sigma)\\)，那么 \\(\\log p(a_t|s_t)\\) 就是一个常数加上 \\(||a_t-f(s_t)||_{\\Sigma}\\) ，于是取 gradient 得到 \\(-\\frac{1}{2}\\Sigma^{-1} (f(s_t)-a_t)\\frac{df}{d\\theta}\\). 这个和 L2 Loss 的模仿学习差不多. 但是不同点就在于一个 training data 和你的 Policy 息息相关，不同的 policy 对应的 data 的分布都是不同的. 这是最 naive 的 algo，称为 REINFORCE algo（全大写）. 但这也带来一个问题，就是 sample efficienty 太低了. 其实设定上我们还没提到之前提到的 partial observability. 但是实际上，带上那个，重新推导一下，发现只需要把 \\(p(a_t|s_t)\\) 换成 \\(p(a_t|o_t)\\) 就行了，还是一样的. Reward \\(\\frac{1}{N}\\sum \\nabla_{\\theta} \\log p_{\\theta}(\\tau)r(\\tau)\\) 的问题：首先这个 \\(r\\) variance 很大；其次实际上应该只有 \\(r\\) 的相对值有关系，而不是绝对值（有时候环境给的奖励也不是很好控制）. reduce variance 首先一个方法是，原本 \\(\\nabla p(a_i,s_i)\\) 的权重都是 \\(\\sum_{i=0}^{T} r(a_i,s_i)\\)，但是实际上我们根本不应该关心之前的 \\(r\\). 所以应该替换成 \\(\\sum_{j=i}^{T}r(a_j,s_j)\\)，即 \"reward to go\". 这也符合我们的 Markov Decision Process. baseline 即我们要取一个 \\(b\\)，然后所有 \\(r(\\tau)\\) 都减去 \\(b\\)，使得所有 \\(r\\) 的均值取在 \\(0\\). 取 \\(b=\\frac{1}{N}\\sum_{i=1}^{n} r(\\tau_i)\\). 这个 monte carlo sample 出来的 baseline 估计是无偏的. 效果上还不错. 当然也可以通过求导来算出最好的 baseline，但是一般也不那么做. Value Function Fitting 但是实际上一个更好的 reward to go 是要考虑这一步之后所有可能的路径的 expectation \\(Q^{\\pi}_{\\theta}(s_t,a_t)=\\sum_{i=t}^{T}E[r(s_i,a_i)|s_t,a_t]\\) 而不是单单这一条 trajectory 的 reward. 我们还可以定义状态的价值 \\(V^{\\pi}_{\\theta}=\\sum_{i=t}^{T}E[r(s_i,a_i)|s_t]=E_{a_t\\sim \\pi_{\\theta}(a_t,s_t)}[Q(s_t,a^t)]\\). 这个价值其实就很适合作为 baseline. 也就是说我们直接把最原本式子中的 \\(r\\) 替换成 \\(Q-V\\) 看上去就很好. 称之为 Value Function Fitting. 定义函数 \\(A^{\\pi}(s_i,a_i)=Q^{\\pi}(s_i,a_i)-V^{\\pi}(s_i)\\). 我们把 \\(Q\\) 用 \\(V\\) 来表示，就是 \\(Q^{\\pi}(s_i,a_i)=r(s_i,a_i)+E_{s_{i+1}\\sim p(s_{i+1}|s_i,a_i)} [V^{\\pi}(s_{i+1})]\\). 我们暴力一点，直接把 \\(E\\) 删掉，得到 \\(A^{\\pi}(s_i,a_i)=r(s_i,a_i)+V^{\\pi}(s_{i+1})-V^{\\pi}(s_i)\\). 实际上，我们会用一个神经网络去学习 \\(V^{\\pi}(s_i)\\). 为什么呢？因为 monte carlo 得到的原本的 \\(V\\) 噪声太大了，用一个神经网络去 fit 可以显著降低这种噪声. 我们使用的训练数据就是 \\((s_{i,t},y_{i,t}=r(s_i,a_i)+\\sum_{j=t+1}^T r(a_j,s_j))\\). 令这个网络参数是 \\(\\phi\\)，我们就得到 \\(\\hat V_{\\phi}(s_t)\\). 这样噪声还是很大. 我们不如直接用上一次训练出来的 \\(\\hat V_{\\phi}(s_{t+1})\\) 直接代替掉 monte carlo 得到的 \\(\\sum_{j=t+1=}^{T}r(a_j,s_j)\\). 这样能够继续减小 variance. discount factor 改为 \\(y_{i,t}=r(s_{i,t},a_{i,t})+\\gamma V_{\\phi}(s_{i},t+1)\\). 即时间离得远的 reward 要打一个 discount. \\(\\gamma\\) 称为 discount factor. 取 \\(0.99\\) 挺好. 并且这个天然地会去鼓励模型尽早完成任务 同时，\\(A\\) 也要改为 \\(r(a_t,s_t)-V(s_t)+\\gamma V(s_{t+1})\\). Actor-critic algo 有两种，一种是 batch，一种是 online. batch / episodic 就是，先 sample 一些 trajectory，然后再据此做后面的部分. online 的话，就是每个环境走一步，然后用 \\(N\\) 个环境得到的\\(N\\) 个步去更新一下后面的部分. 实际训练中，我们更多地使用 online，因为这样效率就会比较高，一边走一边也在训练. actor 就是 \\(\\theta\\)，critic 就是 \\(\\phi\\). critic 能够帮助 actor 的训练. actor 和 critic 可以 share 一些参数. Asynchronous Advantage Actor-Critic：（？）可以节省时间. 一般的 actor critic 长这样： sample \\(a\\sim \\pi_{\\theta}(a|s)\\)，得到一组 \\((s,a,s&#39;,r)\\). 用 \\((s,r+\\gamma \\hat V^{\\pi}_{\\phi}(s&#39;))\\) 更新 \\(\\phi\\). 计算 \\(A^{\\pi}(s,a)\\). 用 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) A^{\\pi}(s,a)\\) 做梯度更新. Generalized advantage estimation 这有一个问题，就是直接用 \\(\\hat V^\\pi_{\\phi}(s)\\) 会有 bias，但是只用 monte carlo 又有巨大的 variance. 一个方法是，monte carlo \\(n\\) 步（使得这 \\(n\\) 步 variance 不会很大），然后再用 \\(\\hat V^\\pi_{\\phi}(s_{t+n})\\). 大概 \\(n=2\\) 或者 \\(n=4\\) 什么的. 但是我们也可以搞一个类似 discount factor 的东西. 取一个 discount factor \\(\\lambda\\)，然后令 \\(\\delta_{t&#39;}=r(s_{t&#39;},a_{t})-V(s_{t&#39;})+\\gamma V(s_{t&#39;+1})\\)，并取 \\(A(s_t,a_t)=\\sum_{t&#39;} (\\gamma\\lambda)^{t&#39;-t}\\delta_{t&#39;}\\). 实际训练的时候，RL 的 gradient 的 variance 特别大，很 noisy. 一个视觉 observation 要过一个 encoder 得到 feature 之后再 RL，一般需要把 encoder 训好了，freeze 了再训 RL，否则这个 encoder 就会训不好. 调 learning rate 很难. off policy on policy 的一个问题是 太 inefficient 了. 而且有 curse of dimension：维度大的时候，很容易走好都没有什么正样本，全是负样本. off-policy actor-critic：维护一个 replay buffer，记录之前最近一段时间见过的一些 \\((s,a,s&#39;,r)\\) 的 tuple. 然后这些 tuple 都可以去当作训练的 data，大幅度提升了效率. 如何使用这些 off-policy 的数据呢？直接用肯定是错的，所以需要进行修正. 显然旧的 \\((s,a,s&#39;,r)\\) 对于新的应该全是错的，所以我们需要容忍一些错误，修正一些错误. 需要注意我们其实主要是想减少通过 \\((s,a)\\) 计算出 \\(s&#39;\\) 的这一个步骤，通过 \\(s\\) 计算 \\(a\\) 倒无所谓. 首先看这一步：“用 \\((s,r+\\gamma \\hat V^{\\pi}_{\\phi}(s&#39;))\\) 更新 \\(\\phi\\)”. 我们考虑 \\(V(s&#39;)\\) 是从 \\(\\hat Q^{\\pi}(s,a)\\) 来的. 所以我们不妨用新 policy 跑出一个 \\(s\\) 对应的新的 \\(a^{\\pi}\\)，然后用 \\(\\hat Q^{\\pi}(s,a^{\\pi})\\) 来代替. 这就要求我们不用去 fit \\(V\\)，而是去 fit \\(\\hat Q^{\\pi}(s,a)\\). 每次用 \\(r(s,a^{\\pi})+\\gamma \\hat Q^{\\pi}(s,a^{\\pi})\\) 去更新 \\(\\hat Q^{\\pi}\\). 于是解决了 \\(a,s&#39;,r\\) 的问题. 然后是这一步：\"用 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) A^{\\pi}(s,a)\\) 做梯度更新\". 这里的 \\(\\log \\pi_{\\theta}(a|s)\\) 肯定不能用老的. 所以我们还是去跑一个新的 \\(a^{\\pi}\\)，然后用 \\(\\log \\pi_{\\theta}(a^{\\pi}|s)\\). 然后一点就是 \\(A^{\\pi}(s,a)\\) 由于没有 \\(V\\) 了所以有点不好求. 所以我们直接使用 \\(\\hat Q^{\\pi}(s,a^{\\pi})\\). 兜兜转转最后回到了 \\(Q\\). 所以现在唯一的问题只有 \\(s\\) 的分布. 但是其实 \\(s\\) 的分布只是比以前更广，所以也不是个坏事，反而可能更 robust 了. 当然如果能有更高的 interactive sample efficiency 的话，还是更多的 on-policy 会更有针对性. Importance Sampling \\[ E_{x\\sim p(x)}[f(x)]=\\int p(x)f(x)dx=\\int q(x)\\frac{p(x)}{q(x)}f(x)=E_{x\\sim q(x)}[\\frac{p(x)}{q(x)}f(x)] \\] 所以我们可以从旧的分布中 sample，然后用上面这个式子得到其在新的分布中的信息. \\[ \\frac{p_{\\theta}(\\tau)}{\\bar p(\\tau)}=\\frac{\\prod \\pi_{\\theta}(a_t|s_t)}{\\prod \\bar \\pi(a_t|s_t)} \\] 带入那个 \\(\\nabla_{\\theta&#39;}\\) 的式子会发现还是有这样一个很难受的乘积. 注意到其本质等于 \\(\\frac{\\pi_{\\theta&#39;}(s_t)}{\\pi_{\\theta(s_t)}}\\frac{\\pi_{\\theta&#39;}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\). 当 \\(s\\) 的分布差的不多的时候，所以直接不管前面这项了. 所以就变成了对于 old policy \\(\\theta_k\\)，我们的 new policy 的优化目标是 \\[ L_{\\theta_k,\\theta}=E_{s,a\\sim \\pi_{\\theta_k}}[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}} (s,a)] \\] 但是如果分布差的太多那么就完蛋了. TRPO TRPO 的工作在于，找到 \\(L_{\\theta_k,\\theta}\\) 的最小的，并满足 \\(D_{KL}(\\theta||\\theta_k)&lt;\\delta\\) 的 \\(\\theta\\). 数学思想是，对 objective 和 constriant 做泰勒展开，然后取低阶项. 然后针对这个低阶项做拉格朗日乘子法. PPO TRPO 有点变态而且用的也比较少. 用的比较多的是 PPO. PPO follow TRPO 的 intuition，但是方法改成了给 KL Penalty 和 clipped surrogate objective. 这是种 soft constraint. clipped surrogate：如果 \\(A\\) 正，则将 \\(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}\\) clip 到对 \\(1+\\epsilon\\) 取 min；否则 clip 到 对 \\(1-\\epsilon\\) 取 max. adaptive KL Penalty：即加上 \\(\\beta KL(\\pi_{\\theta_{old}},\\pi_{\\theta})\\) 的惩罚项. 这个 adaptive 是指，令 \\(d=KL\\)，设一个 \\(d_{targ}\\)，如果 \\(d&gt;1.5d_{targ}\\) 就让 \\(\\beta=\\beta\\times 2\\)，如果 \\(d&lt;d_{targ}/1.5\\) 就让 \\(\\beta=\\beta/2\\). 并且采用 Generalized Advantage Estimation. PPO 训练时候的方法：每轮新采数据，但是每次采完数据之后，做 \\(k\\) 个 epoch 的 update，每个 epoch 的 update 都使用这轮采的全部数据. 和之前完全 on-policy（\\(k=1\\)）相比效率提升很多，但是 \\(k\\) 也不能太大（不然分布就差太多了）. PPO 的 number of environments 有 \\(16384\\) 个，mini-batch size 设为 \\(512\\)，\\(k\\) 设为 \\(5\\). \\(\\epsilon\\) 设为 \\(0.2\\). 反正超参数很多. 一些 trick：对于一个 minibatch 对 \\(A\\) 做一个 normalization（使得 center at \\(0\\)），然后对 state 做 normalization（和 batchnorm 类似，要记录 running mean 什么的然后 test time 要用这个 running mean），然后对 reward 做 scaling. Initialization 和 Activation 什么的也有研究. RL 需要调整很多超参！","path":"2025/09/07/具身-Policy/","date":"09-07","excerpt":"","tags":[{"name":"具身","slug":"具身","permalink":"http://lgswdn.github.io/tags/%E5%85%B7%E8%BA%AB/"}]},{"title":"具身-Vision&Grasping","text":"抓取之后要进行 Manipulation. Prehensible Manipulation：先抓取，然后物体和手的运动就在一起了然后进行操作. Non-prehensible Manipulation：把一个物体推/拖到一个什么地方，可以不用抓取/无法抓取. 4-DoF grasp：像抓娃娃机那样，只能 top-down 抓. 四个维度是 3D position 和 1D Orientation. 擅长抓取一些没有堆叠的，规则的物体. 6-DoF grasp：加上了 3D Orientation. Parrel Gripper：传统的二指的 gripper，抓取点在二指的中心. 只有一个自由度，即开合的宽度. Dex hand：灵巧手，本身可以有 21 个自由度. 3d vision / pose estimation 6D Pose：对于一个 object，定义其 pose 为从自身坐标系到 camera space 的 translation &amp; rotation（和之前定义的一样）. 对于一个物体，可以直接标注其在自身参考系中的抓取方式，然后就可以直接通过 pose 来转化成 world 坐标系下的抓取. 那怎么预测出物体的 pose 呢？首先如果已知 camera 的内参，以及物体的信息，那么可以直接通过 RGB 推测位置. 如何知道 rotation？一个方法是通过 rotation regression，直接训一个网络去预测 rotation. 一个很好的问题是如何表示 rotation. 直接使用四元数会出现一个问题：四元数到 SO(3) 群的映射是不连续的，但是网络只能预测出来一个连续的东西，所以会导致很大的问题（因为要预测一个毫无道理的突变）. 实际上，一个能构成到 SO(3) 的连续映射的东西至少要是 5 维度的. 另一个方法是使用旋转矩阵的前两列，然后做 schmidt 正交化（第三列显然不用预测）. 但是者带来的一个问题是第一列和第二列是不一样的，第二列的平行于第一列的分量是不重要的. 一个相对来说最好的方法是直接预测 \\(9\\) 维的整个旋转矩阵，然后通过 SVD 变成旋转矩阵. 这个在上一篇已有提及. 就是 SVD 成 \\(U\\lambda V^T\\) 之后然后将 \\(U(\\text{diag}\\{1\\ 1 \\det(UV)\\})V^T\\) 作为得到的旋转矩阵. 这里有 det(UV) 主要是为了防止算出来 det 不对，也就是一个正负 1 的问题. 这是对于，假如旋转是在 SO3 中全局随机的情况，那么防止这个不连续性就很重要. 但如果知识一个在 identity 小领域附近的预测（比如下一秒的姿势，肯定不会变化太多），那么这种情况下四元数 / axis-angle 就是连续的，就可以使用了. 另一个方法是，不直接做 regression，而是对于每个 pixel 预测在三维空间的坐标，然后再去拟合一个 pose. 这个监督数据如何生成？一个很好的方法是，直接将物体的每个表面点的 \\((x^m,y^m,z^m)\\) 直接作为其颜色，然后不打光地直接渲染之后就得到了一个 pixel - 三维坐标的对应. 注意这个 \\((x^m,y^m,z^m)\\) 指的是物体在自身坐标系下的，而物体在 camera 坐标系下的 \\((x^c,y^c,z^c)\\) 可以通过深度检测进行 backprojection 算出来. 拟合的本质就是求解一个 \\(X&#39;=RX+t\\). Orthogonal Procrustes：给定差了一个正交矩阵的 \\(M,N\\)，求解一个正交矩阵 \\(A\\) 使得 \\(||M^T-AN^T||_F\\) 最小的. \\(||X||_F=\\sqrt{\\sum_{i,j}X_{i,j}^2}\\). 这个问题的解是：\\(M^TN=UDV^T\\)，则 \\(A=VU^T\\). 同样，由于旋转矩阵 det 为正，所以需要 \\(V(\\text{diag}\\{1\\ 1 \\det(VU^T)\\})U^T\\). 这个平移 \\(t\\) 怎么办呢？一个方法是求出 \\(X&#39;\\) 和 \\(X\\) 的几何中心 \\(\\bar x&#39;\\) 和 \\(\\bar x\\)，那么 \\(X&#39;-\\bar x&#39;\\) 以及 \\(X-\\bar x\\) 就应该只差一个 \\(R\\) 了. 这个问题是对 outlier 非常敏感，所以需要 RANSAC 一下. ICP：Iterative Closest Point，可以调整 \\(R,t\\) 使得更加贴合. 对 regression 方法产生的结果很必要. 我们令 \\(Q\\) 为图片拍出来的 depth point cloud，这是一个 partial point cloud. 令 \\(P\\) 为物体本身的 point cloud，\\(P&#39;=RP+t\\)，于是我们就需要让 \\(P&#39;\\) 和 \\(Q\\) 尽量贴合. 当然由于 \\(Q\\) 是 full point cloud 而 \\(P&#39;\\) 是 partial 的所以需要先 RANSAC 一下，然后再开始流程. ICP 的流程是，首先先把 data 减掉 \\(\\bar P\\) 和 \\(\\bar Q\\) 然后就只差一个旋转了. 然后我们对于 \\(P\\) 中每一个点 \\(i\\) 找到最近的 \\(Q\\) 的 correspondance. 令 \\((P_{corr})_i=\\text{argmin}_{q_j\\in Q} ||q_j-p_i||\\)，然后我们的目的就是最小化 \\(||P_{corr}-RP||_F\\). 那么这和 orthogonal procrustes 是同理的，直接对 \\(P_{corr}P^T\\) 做 SVD 即可 下面考虑解决一个问题：假如我们不知道目标物体的具体建模怎么办？一个工作是，我们可以通过一些已知建模的同 category 的物体，来完成对一整个 category 的物体的 pose 的计算. 对于同一类物体，我们可以指定一个角度，指定一个物体中心点，然后将其等比放缩使得对角线长度 \\(=1\\). 对于一个物体，称这样的一个坐标系为其 NOCS. 我们还是采用直接预测每个 pixel 在三维空间中的坐标的方法. 但是与之不同的是，我们这次预测的是在这个 category 的 NOCS 坐标系下的坐标. 也就是训练的时候，我们要先将每个有建模的物体的坐标转换成 NOCS，然后再做训练. 这样的一个好处是对于同类别的物体，在 NOCS 坐标系下就有很大的相似之处，而且也不用管具体的大小. 于是，我们可以直接通过 RGB 图片来预测其 NOCS，然后得到一个在 NOCS 坐标系下的点云. 然后我们再通过 depth camera 获得一个 depth point cloud，然后再做拟合. 注意这里的拟合需要加一个 scaling 的常数，即 \\(P_{cam}=s\\times R\\times NOCS+t\\). 求解这个也是简单的. 我们还是对两个点云做归一化，只不过这时候还需要除以一个标准差. 然后再 SVD 就好了. 数据一般都是自己造的. 真实数据和合成数据会有 sim2real gap. 这个可能很大！ 一个减小的方法是 mixed reality generation，使用真实的背景，和合成的前景. 还有 domain randomization: 让真实世界成为合成数据的子集. 合成数据里可以出现帽子飘在大海上的情景，随便乱来一通的很大的数据集中，让 test 成为 train 的子集，虽说不优，但是可行. NOCS 工作的一个很大的 sim2real gap 发生在 segmentation 上，因为合成数据的 boundary 太假了. 所以采取一个方法，就是在 segmentation 的监督上加上 coco 的数据 grasping force closure：gripper 可以发力，使得增加一点摩擦力，然后物体的重力方向任意改变都不会移动 form closure：gripper 不用发力，抓取的姿势直接锁住了物体 一个成功的抓取 \\(\\subset\\) force closure \\(\\subset\\) form closure 自锁：一个物体在水平面上，以 \\(\\theta\\) 为入射角的力去推它，那么显然考虑因为有摩擦力，可以推得 \\(\\theta&lt;\\arctan \\mu\\) 时候，无论力多大物体都不会动. Friction Cone：考虑在三维的世界里，所有这样的推动方向形成了一个圆锥（即与法线夹角 \\(&lt;\\arctan \\mu\\) 的所有方向），称为 Friction Cone（摩擦椎）. Force Closure 的数学定义：考虑有若干个接触点，每个接触点都会带一个 friction cone. 每个 friction cone 可以用一个六棱锥去近似，即近似成由 \\(6\\) 个力 span 得到的空间. 每一个力是一个六维向量（因为在 \\(x,y,z\\) 维度上的力以及力矩，总共 \\(6\\) 个维度）. 我们将所有的力给写成一个 \\(6\\times n\\) 的矩阵 \\(F\\)，那么我们就要求对于任意的 \\(f\\)，存在一个正 \\(k\\) 使得 \\(Fk=f\\). 为了满足这个，我们只需要存在正 \\(k\\) 使得 \\(Fk=0\\) 就行了. 当然这是在 \\(k\\) 可以无限大的情况下的. 这个作用是可以在 synthetic data 中标注一个 grasp 是否是好的. 标注同样可以用 physical stimulator，但是仿真模拟的一个局限在于只能往上或者斜着去提，有时候得往别的方向抓或者怎么样. 而如果能 force closure，那么无论未来怎么拽怎么提都没有问题. 当然对于 physical stimulator，一个好方法是设置不同的重力方向. 无论如何，这两者是相辅相成的. GraspNet-1Billion 的抓取数据集的生成方式如下： 首先对于每个物体，在点云上用 FPS 采样一些抓取点以及角度（法线方向）. 抓取点的意思是 gripper 的中心位置. 然后用 Force Closure 去删掉不合法的抓取. 这样我们获取了每个物体的若干种抓取方式. 然后设置两百个场景，每个场景都有若干个物体，以及从若干个角度拍摄. 对于每个场景，手动标注物体的 pose（满足从任何角度看都重合则代表其正确）. 对于每个场景，一个物体抓取成功，还需要满足和其他物体没有碰撞，于是对物体的 mesh 做碰撞检测. 于是得到的数据大小为：场景数量 $$ 角度数量 \\(\\times\\) 物体数量 $$ 每个物体抓取方式数量. 注意这是只能对已知三维点云训练. 由于桌子都是绿底的，所以没法对 RGB 进行泛化. 实际上，现在可以看到，所有的场景都可以虚拟搭建，物体也可以是任意的模型. 这个数据集的一个需要注意的地方：首先摩擦系数 \\(\\mu\\) 越小，成功的抓取应该是 \\(\\mu\\) 大的时候的真子集. 这个数据集从 \\(\\mu=0.8\\) 到 \\(0.1\\) 都标注了那些成功抓取. 但是一个问题是，这个 1Billion 是针对 \\(\\mu=0.8\\) 的，而对于 \\(\\mu=0.1\\)，只有 5M 个数据. grasp detection 指标：success rate，percent cleared，planning time VGN：Volumetric Grasping Network. 输入一个 voxel grid（grid 中每个数写的是到表面的距离）（称之为 TSDF），然后对于每个 voxel，输出 quality（是否能抓），orientation（gripper 的旋转，用四元数表达），width（gripper 张开的 width） 为了获取 TSDF，就需要拍六张照片，形成 complete input，所以这个会比较耗费时间. 训练的网络就是一个 encoder-decoder. 对于 quality，监督就是，如果有 label 落进来，就是 1，否则就是 0. 需要做一点后处理：quality 需要做一个 gaussian smooth；然后如果一个位置的 TSDF 值比 finger depth 还大那么肯定是 false positive，要去掉；然后 NMS. GS-Net GS-Net 采用点云的表达. 首先一个 grasp 是一个 6 DoF 的：坐标有 3 个 DoF，可以采用表面的一个 point + 一个往里面的深度 depth 的方式；旋转有 3 个 DoF，可以采用，首先对于每个 point，在其表面的一个半球上，采 256 个点，作为方向，这是 2 DoF 的；然后还有 gripper 本身的一个 rotation，是 1 DoF 的. 这四个东西可以有一个先后顺序：首先找到 Point，然后再找到方向，然后再去看 depth 和 rotation. 对于给定的 point 和方向，我们称此为一个 view. 对于每个 view，有自己的 view quality；然后对于每个 point，根据自己的所有 view 的 quality 可以再得到一个自己的 point quality. 推理的时候就先选 point quality 好的，然后再去选方向，然后再选 view quality 好的，然后再去看每个 rotation 和 depth 是否可行. 预测 pointwise quality 的时候直接用一个 pointcloud encoder-decoder，L2 Loss 做监督学习. 同理，我们也可以得到 view quality. 然后得到 selected views 之后就要找 depth and rotation. 做这一步的时候使用一个叫做 cylinder group：所有的沿着这个 view 的 rotation 和 depth 会形成一个圆柱，然后将这个圆柱内的所有点的 feature 给拉过来，做一次 pointnet，预测每个 angle 和 depth 是否会成功. 这个网络泛化性的来源，在于最后一步其实只是看局部，然后做抉择，于是网络学习的是，对于一个局部的几何，是否好抓. Grasp Net 当然，为什么不能将抓取理解成一个生成任务呢？ 注意为什么不能是 regression：因为有很多种抓取方法，你 regression 出来的很有可能是一个 average 的 mode，which 可能不是任何一个有效的抓取. 那显然是错的. 一个对于灵巧手的抓取的模型是：首先先对点云计算每个位置的 graspness（即 quality），然后从 graspness 比较高的位置中去 FPS 出一些位置. 然后对于每个位置，取出一个领域的 feature，然后将这个 feature 喂给一个 diffusion model. Diffusion model 会生成一个长度为 21 的东西，代表着灵巧手的 21 个 DoF. 而由于 diffusion model 可以得到生成出来的东西的概率，也就是说可以基于这个去抉择使用哪个抓取. 这个网络的流程是：首先，用 sparse conv 去 predict graspness（这个点可以不可以抓） 和 objectness（即是背景还是前景）. 然后，先去用 diffusion model 生成抓取的位姿 \\(T,R\\)，然后再用 regresssion 预测 joint angle \\(\\theta\\) 深度修复 一个特殊的东西：我们的 depth sensor 在处理高光/透明物体的时候会倒闭. 有一项工作用 learning base 的方法去做了一个关于高光/透明物体的深度复原. 肯定是使用合成数据. 这里的 domain randomization 有以下：物体的 layout，物体的材质，background，illumination（这个很重要也很容易被忘记），camera viewpoint. manipulation Affordance：可攻性，哪些地方可以去 manipulate Where2act 用 learning 的方法去预测 affordance. 做法就是随便弄弄，然后把成功的方式记录下来，然后再用这些数据去训练. Pipeline：使用 2d RGB image（通过 unet 得到 pixel 信息）以及 3d 点云（通过 PN++ 得到每个点的信息），然后去看每个点能做的 action 是什么样的（推的角度？方向？置信度？） 一个 follow-up：不仅预测 affordance，还预测轨迹（比如拉门的时候轨迹是弧线而不是直线猛拉）","path":"2025/08/27/具身-Vision&Grasping/","date":"08-27","excerpt":"","tags":[{"name":"具身","slug":"具身","permalink":"http://lgswdn.github.io/tags/%E5%85%B7%E8%BA%AB/"}]},{"title":"具身-Robotics","text":"Kinematics：运动学，关心位移、线/角速度、线/角加速度等信息. 不关心其背后的力学. 即只关心位移以及其各阶导数. Dynamics：力学，关心如何通过力的作用来达成运动. 通过 \\(F=ma\\) 和运动学连接. 我们更关心 Kinematics. 实际中，通过 Control 来连接 Dynamics 和 Kinematics，而这个大多数情况已经很成熟了. Rigid Motion 只有平移和旋转. 注意镜像不是 Rigid Motion. Rigid Motion 需要保证 distance-perserving（保距） 和 chirality-perserving（保手性）. 从一个观察者 \\(s\\) 的角度，定义其 frame（参考系）为 \\(\\mathscr F_s\\). 在写算式的时候，我们将参考系的 \\(s\\) 放在上角标，如 \\(o_b^s=o_{s}^s+\\bold t_{s\\to b}^s\\). 对于一个 rigid \\(b\\)，我们可以 bind a frame \\(\\mathscr F_b\\)to this object，然后这个 frame 就会跟着这个 object 动（无论是平移还是选转）. 也就是说 object 上的点 \\(p\\)，无论 object 怎么动，其在 \\(\\mathscr F_b\\) 下的坐标 \\(p^b\\) 都不会变. rigid 的 Pose：如何将 \\(\\mathscr F_s\\) 经过平移和旋转变成 \\(\\mathscr F_b\\). Rotation 和 Translation 的 DoF（自由度）都是 \\(3\\). 于是加起来，Pose 的 DoF 是 \\(6\\). 如何描述 Transformation？我们考虑先平移，再旋转. 平移的公式是 \\(o_b^s=o_{s}^s+\\bold t_{s\\to b}^s\\)；旋转的公式是 \\([\\bold x,\\bold y,\\bold z]_b^s=R_{s\\to b}^s [\\bold x,\\bold y,\\bold z]_{s}^{s}\\). 于是在 \\(\\mathscr F_s\\) 中，这个 Transformation 等同于 pose. 对于 object 上的点 \\(p\\)，我们就有 \\(p^s=R^s_{s\\to b}p^b+t^{s}_{s\\to b}\\). 用矩阵描述这个表示，这个在 cvdl 已有提及. 从 \\(p^b\\) 变换到 \\(p^s\\) 的变换可以用二元组 \\((R^s_{s\\to b},t^s_{s\\to b})\\) 表示. 但是显然这个变换关于 \\(p\\) 非线性. 这主要是因为 translation 导致的. 于是引入 Homogeneous coordinate，直接加一个维度变成 \\((x,y,z,1)\\)，然后得到 Homogeneous coordinate 下的 transformation matrix：\\(\\left(\\begin{matrix}R^s_{s\\to b} &amp; t_{s\\to b} \\\\ 0 &amp; 1\\end{matrix}\\right)\\). 使用这个就可以直接写 \\(p^{1}=T^1_{1\\to 2}p^2\\). 然后复合也是类似的，\\(T^3_{3\\to 1}=T^3_{3\\to 2}T^2_{2\\to 1}\\)；\\(T^1_{1\\to 2}=(T^{2}_{2\\to 1})^{-1}\\). 稍微想一下就可以直到这个的用处很大，毕竟机械臂一节一节的就可以用变换传递过去，形成一个 Kinematic Chain. Links：rigid body，每个段 Joints：links 之间的连接器（关节），决定了相邻 links motion 的 DoF. Base Link / Root Link：0th link，看作固定的 \\(s\\)，以此建立 \\(\\mathscr F_s\\). End-effector link：last link，去 do the job 的部分. 比如说一个 gripper. 其 frame 称为 \\(\\mathscr F_{e}\\). Revolute Joint：1D 旋转，即只在一个平面旋转一个角度. Prismatic Joint：1D 平移，即只在一个方向移动一个距离. 一个想到的事：需要注意 frame 是 bind 在 link 上的！ Joint Space：记录着每个 Joint 的更改角度的. Cartesian Space：记录每个 Link 的 frame. 这两个 space 显然是可以互相转换的. 称从 Joint Space 到 cartesian space 的操作为 Forward Kinematics 相反，Inverse Kinematics，就是得知要操作的位置和初始位置，然后设计一个这样的 Joint Space. 根据解方程那一套理论，如果你的系统的 DoF 是 \\(6\\) 的话，只要你想要的结果是 within reachability 的，那么就可以有唯一的方案. 而有些系统的 DoF 是 \\(7\\) 的话，那么就有无穷的方案，就可以避开障碍. 人的手臂的 DoF 就是 \\(7\\). 肩关节有 \\(3\\)，手腕是 \\(2\\)，肘关节是 \\(2\\). 解 IK 很困难. 有的特定的 6-DoF 有特定的解，而其他需要通过一些 Optimization 的方法解决. SO：特殊正交群. 即旋转群. SE：特殊欧几里得群. 即 \\(\\left(\\begin{matrix}R &amp; t \\\\ 0 &amp; 1\\end{matrix}\\right)\\) 群. 机器人的计算位置有云-边-端三个. 具身机器人的控制频率在 250Hz ~ 1000Hz 左右，大概就是 4ms 要完成计算，于是有些运算就一定要放在端上. 一些比较复杂的步骤可以在云上计算. 边则是机器人肚子里的一个模型. 在端上的芯片（比如 Orion X）的算力非常小，甚至不如 3090. 用旋转矩阵来表示旋转会有若干的问题：首先有 3 个自由度但有 \\(9\\) 个数，徒增运算负担；然后运算精度误差会使得失去正交性，要时不时进行 schmidt 正交化. 当然 schmidt 正交化确实也会有时偏颇，因为和矩阵向量顺序有关，这没什么道理. 一个方法是，SVD 分解后，中间的对角矩阵应该是一个每个东西都接近 \\(1\\) 的东西，所以直接把其改成 \\(1\\) 然后再乘回去. 所以我们选择四元数！更多见高代笔记. 四元数做 normalization 也很简单直接除以 magnitude 就行了. 而且计算起来计算量也少. 四元数之间距离：即球面距离，等于 \\(2\\arccos(|p\\cdot q|)\\). 这个和所代表旋转的转角成正比. 如何对旋转做插值？首先用四元数，然后我们其实就是对一个弧做弧线上的均匀插值. 这用到 slerp 算法（spherical linear interpolation）：令 \\(\\phi=\\arccos(q_0\\cdot q_1)\\)，然后用初中几何知识可以推得 \\(q(t)=\\frac{q_0\\sin(1-t)\\phi+q_1\\sin t\\phi}{\\sin\\phi}\\). 在 SO(3) 中的均匀采样：首先四元数的理论，等同于 S(3) 中的均匀采样. 取 \\(\\mathscr N(0,I_{4})\\) 然后再 normalize. 均匀采样很重要！不均匀采样会导致 training set 和 test 的分布不同，会导致一个天然的 drop. 但其实 representation 的学问很深. 四元数虽好但是还是有可能有这种突变，以及还要综合考量各种 gradient 什么的东西. Motion Planning Control System Sensor：有自身的感知信息（preprioceptive info），以及 camera 这种视觉信息. Actuator：真正要去实施调整的地方，比如 joint motor Controller：根据信息给出 feedback 然后给出调整. Open loop（feed forward）control：直接根据 input 给出 output，没有 feedback. 在理想情况下是对的. Closed loop（feed back）control：会将出现的误差 Error 反馈回来，然后继续进行调整. 我们需要减小 steady state 的 error，然后要快速停止震荡. Overshoot：操作过头了，比如 10cm 结果变成了 11cm. Settling time：从第一次的原始误差，变化到最后的可接受的误差所需要的时间. 令 \\(\\theta(t)\\) 为 \\(t\\) 时刻位置，\\(\\theta_d(t)\\) 为 \\(t\\) 时刻目标位置，\\(\\theta_e(t)\\) 为 \\(t\\) 时刻误差，\\(e_{ss}\\) 为 \\(t\\to \\infty\\) 时的误差. Proprotional Control：选择一个常数 \\(K_p\\)，然后直接用 \\(K_p\\) 作为调整. 也就是说，令 \\(\\theta&#39;(t)=K_p\\theta_e(t)\\). 即 \\(\\theta&#39;_d(t)-\\theta&#39;_e(t)=K_p\\theta_e(t)\\)，而由于 \\(\\theta&#39;_d(t)=0\\) 所以 \\(\\theta&#39;_e(t)=-K_p\\theta_e(t)\\). 用小学常微分方程知识知道 \\(\\theta_e(t)=\\theta_e(0)\\times e^{-tK_p}\\). P Control 在 \\(\\theta_d(t)\\) 存在变化的时候显然就有问题了，以及如果用 P 作为二阶导（加速度）的 control 那么就不收敛了. 于是提出 Integral Control：\\(I=K_i\\int_0^t \\theta_e(\\tau)d\\tau\\). 结合在一起，有 PI Control：\\(PI=K_p\\theta_e(t)+K_i\\int_0^t\\theta_e(\\tau)d\\tau\\). 我们用 \\(PI\\) 可以据此列出方程 \\(\\theta&#39;&#39;_e+K_p\\theta&#39;_e+K_i\\theta_e=\\theta&#39;&#39;_d\\). 如果右侧取 \\(0\\) 那么就是一个标准的 \\(\\theta&#39;&#39;_e+K_p\\theta&#39;_e+K_i\\theta_e=0\\). 可以感受一下，如果目标是在匀速运动的话，那么 PI Control 就可以逐渐收敛，而 P Control 不可以. 我们也可以再引入 Deriative Control. 同理，\\(D=K_d \\frac{d}{dt}\\theta_e(t)\\). 当 error 在变大的时候就加强 punishing，变小的时候就更 gentle. 结合起来就是 PD Control. 我们用 PD 去 control 其运动的二阶导，来减小 overshoot，那么就是 \\(\\theta&#39;&#39;_e+K_d\\theta&#39;_e+K_p\\theta_e=\\theta&#39;&#39;_d\\). 结合在一起就形成了 PID Control. 其实再感受一下，\\(k_p\\) 相当于就是提供一个往目标的一个冲劲（stiffness），调太大会导致可能停不下来，而 \\(k_d\\) 就是可以作为一个 damping 去存在.","path":"2025/08/19/具身-Robotics/","date":"08-19","excerpt":"","tags":[{"name":"具身","slug":"具身","permalink":"http://lgswdn.github.io/tags/%E5%85%B7%E8%BA%AB/"}]},{"title":"学习笔记-ATT&CK","text":"TA0043 Reconnaissance 就是收集一些信息以便安排后续的攻击. T1595 Active Scanning：在和网站的交互中，主动去扫描这个网站的信息，包括 IP Block Scanning：扫描一个连续的 IP 段，看哪些主机/端口被使用，可以绘制一个网络拓扑结构. Vulnerablity Scanning：扫描网站的 vulnerability Wordlist Scanning：枚举发现一些子域名/隐藏网页目录/用户民 T1592 Gather Victim Host Information：收集 host 相关的信息，包括 Hardware：有些东西可能有硬件上的保护，比如 biometric reader Software：有些东西可能有 antivirus 等软件上的保护 Firmware：扫描固件信息，写入固件的恶意代码可以一直潜伏 Client Configurations：os version，architecture，timezone 等 T1589 Gather Victim Identity Information：收集一些 identity 的东西，包括 Credentials，EmailAddresses，Employee Names T1590 Gather Victim Network Information：网络信息，包括 Domain Property：可以帮助直接找到负责人又想，以及公司的一些其他域名 DNS：可以用来看到使用的第三方服务 Network Trust Dependencies：找到一些合作伙伴/提供商 Network Topology：绘制网络拓扑结构 IP Addresses：了解 IP 地址 Network Security Appliances：使用了哪些网络安全的工具 T1598 Phishing for information：钓鱼，诱使受害者发一些敏感信息，可以通过 Service，Attachment，Link，Voice T1597 Search Closed Sources：从一些搜不到的渠道获取信息，比如从威胁情报供应商获得，或者从一些地方购买技术信息情报 T1596 Search Open Technical Databases：从一些可以搜到的渠道获得技术信息. DNS，WHOIS（可以查到一些 IP blocks，contact info 等），Digital Certificates，CDN，以及一些公开的 scanning results T1593 Search Open Websites/Domains：从一些可以搜到的渠道获得其他信息，比如社交媒体，搜索引擎，code respositories T1594 Search Victim-Owned Websites：受害者所拥有的其他网站也可能透露一些信息. TA0042 Resource Development T1650 Acquire Access：直接付钱让别人帮忙获得 access T1583 Acquire Infrastructure：建立属于自己的域名，DNS，服务器，botnet 等 T1585 Establish Accounts：建号 T1586 Compromise Accounts：利用一些已有的社交媒体账号 / 邮箱账号 / 云账号 T1584 Compromise Infrasturcture：和 1583 差不多，但是也是利用一些已有的. T1588 Obtain Capabilities：获取一些有用的工具，malware 或者 certificates 这种. 或者利用 AI. T1608 Stage Capabilities：使用这些工具，比如将 malware 和 tool stage 到 infrastructure 上. 或者比如 SEO poisoning，将陷阱网站利用搜索引擎的机制排到搜索结果的前面（挺常见的感觉）. TA0001 Initial Access T1659 Content Injection：各种注入攻击. T1189 Drive-by Compromise：路过式攻击，访问网站的用户就会似 T1190 Exploiting Public-facing App：利用一个大家都能用的东西的 bug / weakness. T1133 External Remote Services：利用像 VPN / 远程管理等方法进入网络. T1566 Phishing：钓鱼. 同理. T1195 Supply Chain Compromise：在薄弱的供应商的东西上下毒. 公司会信任供应商，但是如果供应商的东西早就被攻击了就似了. T1199 Trusted Relationship：攻击信任的供应商，然后通过这条信任的道路进入网络. T1078 Valid Accounts：用偷来/被攻破的的信任的账户进入网络. TA0002 Execution T1651 Cloud Admin Command：通过 Abuse 一些可以 remotely run command 的工具来进行. T1059 Command and Scripting Interpreter：各种 shell. T1610 Deploy Container：在一个新建的 container 中进行操作，来 bypass 掉一些 defenses. T1203 Exploitation for Cli Exe：利用客户端中的一些 vulnerablities 来执行想要的命令. T1674 Input Injection：各种 injection. T1559 IPC：将恶意代码的执行藏在进程间通信（IPC）中，称为进程注入. T1204 User Execution：在用户端诱使用户执行代码，比如通过 malicious link / file / image / copy&amp;paste. TA0003 Persistence 维持住自己得到的权限 / ... T1098 Account Manipulation：得到账号后通过一系列方式修改密码和加固等，将其变为自己长期拥有的账号. T1197 BITS Jobs：利用 BITS 来在 background 中执行命令. T1547 Boot or Logon Autostart Exe：将要执行的东西能够自动执行，比如修改 Shortcut，放进 Startup Folder，放进 Login Items 什么的. T1037 Boot or Logon Initial Scripts：在登录/启动脚本中藏代码，使其能够持续发力，比如藏到 Logon Scripts / Startup Items 等. T1543 Create or Modify Sys Process：更改系统进程代码，让能够一直执行一个恶意代码. T1546 Event Triggered Exe：潜伏着，然后在触发一些 event 再进行执行. 比如利用篡改辅助功能（例如粘滞键），更改默认程序，WMI 时间订阅（A事件 -&gt; B 命令）等. T1556 Modify Authentication Process：修改 authentication 的过程，使得可以随便就登录任何想要的东西. 比如，注入一个万能钥匙使得可以登录任何一台机器，或者在用户修改密码的时候直接记下来明文密码. T1176 Software Extensions：将恶意代码藏在浏览器 / IDE 插件中. T1078 Valid Accounts：获得一些账户. 可能是本地账户（一台服务器的权限），域账户（可以登录域内所有服务器），云账户（可以窃取存储在云上的信息），默认账户（有些忘记修改默认密码的特定设备和系统）. TA0004 Privilege Escalation 尝试得到更高的权限. T1548 Abuse Elevation Control Mechanism：直接钻系统的漏洞尝试获得更高权限的办法，比如 Bypass UAC：修改注册表，让合法程序在运行的时候执行恶意代码 Setuid Setgid：找到一些能临时获取权限的程序并利用 Sudo Caching：sudo 命令过后一段时间不用再次认证，利用这段时间 T1134 Access Token Manipulation：获得一个权限比较高的 token. 可以通过直接偷到，也可以通过 SID-History 注入（拿到域管理员权限之后修改普通用户的 SID-History 属性）. T1574 Hijack Execution Flow：把要执行的合法资源替换成恶意代码. 比如 DLL Search Order Hijacking，将一个恶意的同名 DLL 放在与程序目录相近的地方，那么程序就会加载恶意 DLL. 而 Linux 下的 Dynamic Linker Hijacking，就是修改 LD_PERLOAD 环境变量，强制在加载任何程序前优先加载恶意代码. TA0005 Defense Evasion 防止被探测到. T1548 Abuse Elevation Control Mechanism：同上. T1134 Access Token Manipulation：同上. T1564 Hide Artifacts：摁藏. 比如：隐藏文件，隐藏用户，隐藏窗口，虚拟网络，藏在 antivirus 扫不到的地方，忽略 process interrupts，将资源先 fork 出来，等等. T1562 Impair Defenses：将 defensive 的东西给 disable 掉. 比如干掉一些 security software，干掉防火墙，关掉 event logging 和 command history logging. T1656 Impersonation：把人骗去干坏事. T1070 Indicator Removal：把痕迹去掉，包括 event logs, system logs, cmd history，删文件，删时间戳，网络历史，邮件信息，删 persistence，删 malware. T1036 Masquerading：伪装. 比如，模仿一个 valid code signatures，RLO（photo_high_re\\u202Egnp.js 会显示成 photo_high_resj.png），模仿一些 task &amp; service 的名称，等等。 T027 Obfuscated Files：加密或混淆可执行文件，使得一个东西难以被一些检测方式分析到. 比如插入一些 junk code，进行压缩，command line obfuscation，传递 uncompiled code，等等. TA0006 Credential Access 尝试得到账户和密码. T1557 Adversary-in-the-Middle：让两个机子的沟通经过一个能够获取中间的信息的东西. T1110 Brute Force：顾 名 思 义，直接猜，或则根据哈希破解. T1555 Credentials from Password Stores：从存储密码的地方获得密码. 比如浏览器会存储密码，一些第三方的密码管理器也会存储密码. T1187 Forced Authentication：逼你给出密码. T1056 Input Capture：顾 名 思 义. 捕捉键盘输入来得到你的密码. T1556 Modify Authentication Process：和 TA00-3 中的那个一样. T1539 Steal Web Session Cookie：顾 名 思 义. T1552 Unsecured Credentials：有些密码存在很不安全的地方，比如：文件中，registry，bash history，一些 .key 文件，聊天记录等. TA0007 Discovery T1087 Account Discovery：找到账号的一个列表，然后用来下一步的枚举，或者钓鱼，或者攻破. T1010 Application WIndow Discovery：找到运行的应用，然后这些应用可能可以表明一些可收集的数据 / security tools. T1057 Process Discovery：看系统上有哪些进程，以明白里面干了些啥. T1517 Software Discovery：看有哪些软件，可以利用. T1217 Browser Information Discovery：看浏览器的信息. 可能能获得比如 banking sites，社交媒体等个人信息，以及 servers, tools 等和内网有关的信息. T1083 File and Directory Discovery：在机器的一些文件中找东西. T1615 Group Policy Discovery：阅读 SYSVOL 文件夹内内部网络的 group policy. 了解公司网络管理的一些规定和策略. T1202 Password Policy Discovery：阅读密码策略，以辅助 bruteforce. T1046 Network Service Discovery：看服务器中运行了哪些 service. T1135 Network Share Discovery：找一些在服务器上的共享文件/文件夹. T1069 Permission Groups Discovery：查看用户组及其权限，确定哪些用户/用户组可用/有更高权限. T1018 Remote System Discovery：看当前网络还有什么别的东西，用 ping，net view 等，为横向移动做准备. T1016 System Network Configuration Discovery：了解网络设置，比如 IP 和 MAC 地址. 可以用 Arp，ipconfig，route 等. T1049 System Network Connections Discovery：找当前设备在网络上连接的其他设备. T1497 Virtualization Evasion：从虚拟机/沙盒环境中逃逸. TA0008 Lateral Movement T1534 Internal Spearphishing：利用已经控制的电脑的主人的身份去钓鱼. T1570 Lateral Tool Transfer：将一些工具/文件通过传输到别的系统去. T1563 Remote Service Session Hijacking：通过本机的一些远程服务，来控制别的机器. 比如劫持本机的 SSH，或者远程桌面. T1021 Remove Services：登录一些远程服务的账号，比如 telnet，SSH，VNC，远程桌面，以及一些云服务. T1091 Replication Through Removable Media：将一些东西复制到可移动介质上，然后在可移动介质插到别的电脑上时执行. T1080 Tainted Shared Content：将恶意代码藏在一些共享地方，别的机器访问执行共享的东西就会被攻击. T1550 User Alternate Authentication Material：有时候不需要明文密码，用一些 app access token / password hash / kerberos tickets 就可以直接得到 authentication. TA0009 Collection 收集自己想要的信息. 很多 credential access 的 technique 这里都可以用. T1560 Archive Collected Data：将想要传出来的数据给压缩/加密了，防止被发现并且减少流量. T1074 Data Staged：将获得的数据暂存在一个位置. 可以是本地，也可以是远端的. T1119 Automated Collection：用一些脚本自动收集信息. T1114 Email Collction：从 Email 中获取敏感信息. 一些 Capture 操作有 Screen Capture (T1113)，Video Capture (T1125)，Input Capture (T1056)，Audio Capture (T1123). 然后 Data 可以来自 Cloud Storage (T1530)，Configuration Repository (T1602)，Information Repositories (T1213)（这包括比如 SharePoint，代码仓库，用来存用户信息的 CRM，通信软件），Local System (T1005)，Network Shared Drive (T1039)，Removable Media (T1025). TA0011 Command and Control 如何在防止被检测到的情况下控制电脑，称为 C2. T1071 App Layer Protocol：将通信伪装成正常的 HTTP / FTP / DNS 协议通信. T1095 Non-App Layer Protocol：将通信伪装成正常的 ICMP / UDP / SOCKS 等. T1659 Content Injection：在原先的数据传输通道中进行注入攻击. T1132 Data Encoding：将 C2 信息加密，防止被检测. T1568 Dynamic Resolution：动态地调整 C2 地端口号 / IP 地址等. T1571 Non-Standard Port：用一些非传统的 Port（比如不使用 433 而是用 947）. T1090 Proxy：使用一些代理作为中转，来管理 C2 的流量. 可以使用多个代理来掩盖流量来源. T1219 Remote Access Tools：用一些远程连接的工具，比如 IDE Tunneling，远程桌面的一些软件等. TA0010 Exfiltration 把数据偷出来. 本质上就是，如何在偷出来的时候尽量不被发现. T1030 Data Transfer Size Limits：切成若干个小块. T1041 Exfilteration Over C2 Channel：利用之前建立的 C2 Channel 进行传输. T1567 Exfiltration Over Web Service：通过一些已有的 Web Service 来传输，比如上传到 code respositories, cloud storage, text storage sites (pastebin)， webhook 等. T029 Scheduled Transfer：每天传输一点点. TA0040 Impact 如何在完成上述的工作之后进一步造成重创. 比如 system shutdown！比如 rm rf！比如加密然后勒索！比如邮件轰炸！比如窃取！比如挖矿！","path":"2025/07/30/学习笔记-ATT&CK/","date":"07-30","excerpt":"","tags":[{"name":"腾讯星火营","slug":"腾讯星火营","permalink":"http://lgswdn.github.io/tags/%E8%85%BE%E8%AE%AF%E6%98%9F%E7%81%AB%E8%90%A5/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-6-西藏","text":"地理因素 许多世纪以来，在之前说的三个边疆，中国不但抵御外部的入侵，也限制人民向外发展，以免其向外发展后与中国分离。而多余南方，无论怎么发展，都只会逐渐同化并吸收当地居民。所以南方是一个开阔并有很大深度的边疆，而北方是一个想要关闭但未能关闭的边疆。但西藏和两者都不同，这与其地理环境关系很大。其难以逾越，也难以侵入。 西藏的可供农耕的地方在中部起伏地带，是一个“袋装“的地。耕种地区大致形成弧形，从雅鲁藏布江河谷到甘肃边境。交通非常困难。中部高地有草地，但是条件不如蒙古的。每片草地都可以由若干条河谷进入，所以各个文化在此处交汇。 河谷居民住在北山岭包围的绿洲中，并以森林农业、狩猎和畜牧为生。 西藏人的社会起源 在草原的边缘地带，农业进步代替了游牧社会之后，原本社会中有权益的人就会撤退到农业社会统治力量所不及的远方草原地带，退向深山的“袋”中。 西藏的农业与游牧业 游牧民族一部分来源于河谷居民，一部分来源于北方的草原。由于河谷更易于抵御攻击，逐渐成为了权力与财富的中心，于是南部和东部的游牧部落就只能在河谷的农耕部落手下放牧牲畜。只有少数的，如黄河上游的果洛族靠着甘肃的同盟，独立于河谷的部落。 边缘的游牧经济一方面使得各个河谷部落有经济上的差异，一方面又能维持河谷部落的交通，所以不会像沙漠绿洲那样分裂。 早期西藏于中国内地的联系 西藏历史相比于其他边疆，是一个晚熟的历史。甘肃西藏边疆上的非汉族，古代称之为羌，意为牧羊人。他们还没有进步成真正骑马的少数民族，一部分人和汉族合并，一部分人被挤到西部和西北部的甘藏边界。同样是有人从事农业有人从事游牧业。汉族应对他们主要防止与草原部落联合，于是羌人对汉地的威胁只有劫掠而没有战争威胁。 西藏的政治统一 西藏原来分为：东北部（和甘肃半绿洲/草原地带有关），东部（和古代长江流域的部族以及藏缅族相关），南部&amp;西南部。西藏的环境困难让统一变得非常迟缓。在统一的过程中，喇嘛教就有很显然的历史作用。 当孤立的社会已经成熟，能够互相联系，并努力向中部或周边扩展，就形成了多种势力结成的网络。这些社会能够协力促进共同利益的时候，又被各自发展的独特性所限制。所以就想需要一个容许独立但又能补偿这种情况的机构。于是宗教的引入就变得必要。 喇嘛教的政治作用 喇嘛教在统一的过程中起到了重要的作用，也是国君用来对付地方贵族的工具。宗教权力的继承是非个人的，家族只能让次要人物担任教职，于是宗教礼仪和家族利益就不能集于一体。但是不受地方家族影响的宗教，最后使得宗教取代了君王。蒙古也曾经接近发生这样的事，但是被清朝的王公-宗教的双重政治给阻止了。 但是在西藏，宗教的地位更加优越。没有新的国家组织代替宗教，因为本来西藏的地方组织就很散漫，交通困难，这是内在因素。而外在因素则是，达到这样的发展阶段后西藏历史就停滞不前了，宗教在一个固定不变的水准上维持其能够协同各方利益的优势。 藏人对西部和新疆的占领（8世纪） 8世纪的藏族强生到可以入侵中国西部和新疆。为什么向西北部发展而不是人口更多的东南发展呢？首先，扩张是必要的。如果不向外发展，一个刚建立的新国家必然要采取对内征服的形式，容易引起分裂。其次，西北部遥远的部落，人数不多但是有地理优势，可以趁君王不注意攻击其他部族。于是让西北地区参加对外侵略，用来补偿他们臣服于拉萨的可能的损失。 而新疆和西部那边的绿洲，每个绿洲势力都很小，用少量兵力即可征服，而藏族自己的地势又易守难攻。这一时期，西藏的早期佛教经改造成了初期的喇嘛教，并将孤立的社会团体团结起来。 喇嘛教的早期支配地位 9-11世纪，突厥民族兴起，藏族被逐出新疆。同时，西藏内部发生分裂，拉萨的藏族重新调整了疆界，而北部的集团（党项/西夏）占据了中国西北部的一块地区。被逐出新疆时也带回来了很多宗教影响。后来朗达玛赞普的统治下进行了宗教反抗运动，结局是寺庙没有被毁，反而王国被推翻，分裂。寺院比众多邦国更能够一致行动。 11世纪喇嘛教产生了伟大的改革运动，这可能是印度和克什米尔来的圣人努力的结果。而这个时期回教已侵入新疆绿洲和西北部过度地区。新疆绿洲完全改信回教，而在次绿洲地区，那些没落的宗教的一部分人改信回教，一部分宗教团体被藏族庇护。这说明喇嘛教的改革可能也有应对回教的意义。 蒙古势力控制时期 成吉思汗于1227年征服西夏。1270，忽必烈决定了西藏的臣属地位。 掌握宗教统治权的教主，如果发现一个区域的新宗派兴起力量很大，那么避免其产生过大的影响力，则直接依赖外部帝国的势力。于是教主是西藏内部停滞不前，以及外在帝国控制西藏的象征。 元朝灭亡后，宗教分裂，各个宗派都要求其独立地位或者优先权。有些党派与明朝建交，有些与草原上的蒙古族建交。汉族势力范围已经从宁夏甘肃那里退出，于是与拉萨的交通依赖于云南与四川；而蒙古地区已经分裂成各方的部落联盟。半绿洲地带居民包括了藏族、蒙古族、回教徒、汉族，而这片区域是一个可以联系各大帝国之间的枢纽地区。 宗喀巴就来自于这个区域。15世纪，他创立了黄教，发源了现在西藏教主继承的两大系统：班禅喇嘛和达赖喇嘛。这一宗教改革的结果之一是压倒了印度输入的南部佛教，而加强了北部宗教的势力。宗喀巴的宗教胜利反映了在拉萨及其控制区以北的一个新区域的崛起。 后来鄂尔多斯土默特部扩展到宁夏，推向甘肃和西藏的过程中，开始信仰喇嘛教并在政治上利用它。这最后导向了喇嘛教在蒙古的复兴。 清朝统治下的达赖和班禅 五世达赖是头一个具有完全大法王的达赖喇嘛。达赖喇嘛的统治权可以反应北部藏族势力向拉萨和其他地区的衍生。北部藏区人少但是战略地位很强，他们先与蒙古联合，又与满清联合。班禅喇嘛的地位比达赖喇嘛要更崇高。这可能是因为15世纪，北部藏族进入南部时，扎什伦布地区的寺院领袖就与他们建立联系，获得了很高的权力，并携手取得了拉萨。之后，北部藏区降为地方区域（就像东北降为行省一样）。 教主转世的政治作用比宗教意义更大。以前藏人用这个机制降权力控制在家族手中，后来清朝推动宗教机构的非个人化，选择不重要的家庭的小孩作继承人，使他无法用新的地位来建立新的地区和党派。 近代中英权益在西藏的冲突 皇室的目的是让边疆停滞与固定状态，并维持一个平衡发，防止边疆压迫内地或者团体越出中国而进入边疆的漩涡。 最后产生新形势的还是西方放入亲之后，影响了清朝衰弱的内在的“自然”变化。其中一个内容就是次帝国主义。 邻近甘肃的青海省、以及领巾云南的西康省，形成了“内藏”区域；而拉萨统治的外藏和外蒙古相似。汉族想让内藏置于直接统治之下，但外藏这种活动则于英国产生了冲突。 印度东北边疆是一个死的边疆，没有军事危险，只需将中俄的势力摒在这个区域外。英国统治印度所必要的威望，不准许印度在任何方面能看到一个能与英国相比的强国，不希望当地居民可以脱离英国的势力圈。所以在英国看来，这是一个需要阻碍思想传播的边疆。英国只是希望保持西藏的现状，使得西藏依赖英国的援助。 而近代中国的政策则是相反，希望在西藏建立向外发展的边疆以提高其威望。于是出于这种原因，造成了那些年英国援助达赖喇嘛而中国支持班禅喇嘛的政策。后来次帝国主义的放弃，联合抗日的政策改变了这一点。","path":"2025/07/23/阅读笔记-中国的亚洲内陆边疆-6-西藏/","date":"07-23","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-5-新疆","text":"绿洲地理及农业 中亚地区的沙漠绿洲有利于农业发展。这里的绿洲是由沙漠分隔而各自孤立的绿洲。由于下雨不多，所以灌溉非常必要，而夏季天山的冰雪大量融化流入塔里木河，形成利于灌溉的地理条件。 而介于黄河流域和沙漠绿洲之间的是草原绿洲，其历史则徘徊在农业和游牧之间。农业出现的更早，但是后续两种方式交替出现。 从定居发展到游牧 驯化牲畜的机会更多出现在在草原边缘从事农业但仍然右狩猎活动的社会。相比于游荡的放牧者和猎人，定居者更容易学会去驯化动物。 人们会用被捕获的动物作为诱饵来吸引野兽。饲养作为诱饵的动物，进一步出现阉割的技术。这对放牧是必要的，防止大量雄性之间的争斗。后来有学会如何利用动物的乳和毛，这使得原本从事多种经济活动的人能够完全依赖牲畜，走向游牧社会。 中国和中亚之间的次级绿洲 甘肃和宁夏这块地方的次级绿洲，不像沙漠绿洲一样完全互相孤立，也没法连成一大片精耕区。在一开始，黄土地带原始农业获得有力的历史发展，让次级绿洲区域上的农业也凌驾于狩猎和畜牧经济上。但是，在贫瘠土地上，粗耕旱作和饲养牲畜是很重要的。这些居民逐渐与汉族不同，并受到敌视，不能扩充农业地理范围，只能被迫开发其他资源。于是，他们开始脱离绿洲，依赖于草原。 在这些差异化中，“种族”问题是不那么重要的。次级绿洲的居民在体态上和汉人没有区别。直到与农耕民族为敌的游牧民族出现之前，体态上的差异都是几乎没有的。而在后来的发展中，才开始有了“距离长城越远，体态差异越大”的原则。 汉族向中亚的渗透 汉族可以控制汉地边缘的类似绿洲的地区，但是却不能让这些地区与汉地合为一体，原因在于距离和交通。这个区域可以同化，控制，但不能完全结为一体，使得这块区域并不稳定，只能在这里时断时续地建立据点。塔克拉玛干边缘绿洲的精耕农业特定以及沙漠缺乏牧场对游牧民族的妨碍，使得这块地方倾向于中国社会和文化；而准噶尔盆地可以被草原民族直接进入，而汉族则被阻拦于沙漠之外。 所以与在长江南方这种合并不同，在这块地方只能是一个间断的控制。 行商线路与贸易 前面说过绿洲之间虽然很相似，但是彼此分离独立。将这些绿洲连接起来的第一条道路是丝绸之路。后来又有了穿过塔克拉玛干北部绿洲的交通。控制这条塔克拉玛干北缘的路就可以控制塔克拉玛干南北的新疆绿洲，有更重要的战略意义。这条路可以让商队从近东直达中国，防止受到游牧民族的侵袭。 拉铁摩尔认为，丝绸之路的贸易只是结果。其最初是为了在汉人向半绿洲地区发展的时候，为了攻防草原的侧翼，占据更远的据点所产生的线路。而这些贸易也有利于中国与绿洲统治者的政治联系，防止他们倒向草原势力。 宗教对社会与政治的影响 公元1世纪，佛教在新疆的建立迅速而稳固。因为佛教带来了绿洲社会自身无法产生的东西：寺庙让绿洲们有了共同财产权，寺庙的高级僧侣也形成了联系各个绿洲王室的纽带。 相比较之下，在中国佛教的发展就失败了。拉铁摩尔认为，这是由于绿洲的小规模社会不存在一个像中国那么庞大的官僚阶级。在中国，官僚阶级有自己的阶级利益，也有儒学这样的职业规则。所以佛教势力发展到让他们感受到危险的底部，就会大受攻击。 后来也有像摩尼教，景教等宗教传入中国，也同样没能在当时的中国普遍发展。这些宗教在新疆发展的时候信教的往往是在那的外国商人，这些宗教也只是把交通线路是上的与自己有关的团体联系起来，和佛教不形成竞争。 回教 回教发源于一个企图调和绿洲和游牧民族的文化，建立一个商人、农民、牧人之间的共同观念。回教在西域得势的时候，其政治性非常浓厚，并用武力强迫佛教徒改变信仰。回教造就了一个西域民族共有的观念，形成了一个民族的感觉，其中突厥文占有重要地位。 中亚的满族与回族 清朝的统治是回民最畏惧的权力。明朝的时候，在西北地区，他们凌驾于汉族之上，而在清朝，他们的地位则受到了威胁。于是在清朝的时候爆发过很多次“回乱“。最大的一次战争是在 1862-1877 年，这和云南的”回乱“与太平天国运动几乎同时。这是 19世纪末一系列推翻满清的离心运动之一。 甘肃中汉人的人口占优势，但是发源自绿洲的回教让绿洲的回民有较高的统一性和实施能力，于是回民就可以利用中央政治力量的衰弱来压倒甘肃内的汉人。而新疆不仅因甘肃的叛乱与内地暂时隔绝，其本身也有许多”回乱“。新疆的这些回民在北部绿洲占多数，其中许多人较晚皈依回教，故他们的语言是汉语，文化和体态也更接近汉族。新疆的回民叛乱和甘肃的”回乱“是类似的。 清朝对新疆的统治没有完全安排妥当，因为没有哦继续征服西土耳其斯坦的绿洲。于是西部独立绿洲的和卓部不时企图在新疆重建势力。和卓部首脑后来被阿古柏伯克篡夺。他以纯粹绿洲回教的方式，自立为首长，沿天山南路向东发展。 但后来，回教又迅速崩溃了。甘肃回民虽说获得了政治胜利但还是无法摆脱对汉贸易的依赖。不同的政教家庭开始争夺回民内部的领导权和回汉关系的控制权。新疆回民无法向南部绿洲移动，阿古柏伯克争夺领导权的企图造成了更激烈的战争。 清朝平定太平天国运动之后，就派左宗棠收复新疆。其中一个回汉合作的办法是让对回民收的税必须通过回教领袖，来利好有实权的回人家族。攻入新疆后，清朝军队主要要组织北面的汉语回民与南面的突厥语回民联合。南面的回民惧怕阿古柏伯克，北面的回民经常被草原民族攻击。 近代新疆的政治经济状况 1911-1928年，杨增新统治新疆。与蒙古和东北不同，铁路网所造就的“次帝国主义”无法到达新疆。汉人在新疆自诩为征服者，但是他们并没有什么实际的力量，只能以保守的态度维护重要人员的利益。贸易方面，只有奢侈品贸易是有利可图的。官吏和汉商合作，商人剥削民众。 绿洲的一个历史循环是，若技术能力为常数，人口繁荣到顶点的时候，每个绿洲的灌溉工作能力不再增加，而很多土地因为淤积和盐碱的关系休耕，于是他们就转而侵入草原。在上述时期，有些突厥语的南部农民向叛乱后人口减少的北路迁徙，而有些直接迁徙到游牧民族最适合耕种的农场上，引起了政治上的紧张。 中国边疆发展的高潮 1929年是“次帝国主义”的高潮。国民党的目标是，联合实业家和旧地主，在外国势力全面控制中国前，将内地和边疆统一起来，以边疆的发展补偿沿海地区丧失的权益。在东北，试图将苏联赶走的企图被苏联用武力打破，后来又被日本公开侵略。日本的侵略表明了次帝国主义的政策不能让中国免受真正的帝国主义的侵略。后来抗战的统一展现才让边疆关系出现新得转机，回到孙中山最重要的推翻一切形式的帝国主义的原则。 回到1929，这个时期各民族开始逐渐转变为紧张和敌视。一方面这是因为国民党的次帝国主义思想传递到新疆的汉族统治阶级，一方面也有前面一章所说的政治紧张。为此，当局开始购买军火。 1925年，哈密取消了土邦制度，代之以中国的直接统治。官方对哈密人保证税率不会增加，但是丈量的单位减小，导致了实际上税收增加了，引起了叛乱。在这样的第一次交火中，当局的新武力旧溃败了，枪械被叛军所缴获，叛乱的危险散布到各地。甘肃的一个领袖马仲英攻入哈密。 苏联的影响 苏联逼迫外蒙紧密联合，但在新疆，苏联没有推行政策的必要。 土西铁路的完成使得新疆有了更好的对外贸易。 有些派别呼吁苏联来保护自己的利益，而苏联的利益是不希望边界有战事。 被援助的人还是汉人，可能是旧统治阶级、撤退到苏联的东北军人，和一些其他地方人士。苏联供给军火，并以阿尔泰军队的名义开入新疆。后来，苏联军队在秩序恢复之后立即撤退。 已经恢复的秩序只能通过清楚严重不平之事的方式来维持，于是各民族间获得了较大的平等。这个政策改革受到了苏联政策的支持，也增加了贸易。这是一种“反式联合”的现象，即鼓励民众行动寻求合作，而并非力量夺取。随后就是抗日的联合战线，并且中国本身也开始和苏联接近。 一个比较重要的事实是，绿洲和草原的交替崛起（其中夹杂着中国的统治）是两个文化不能协调导致的。各种社会形式导致了中国机械化工业崛起的困难，但也只有工业才能联合不同的经济形势，以建立更高级的社会结构。 在那个时候，仍然可以说是苏联深深影响着这块地方。于是，拉铁摩尔将汉人在中亚统治的结束看作旧式绿洲循环的完成。继而到来的是一个完全不同的统一现象。而苏联势力是受其影响的民众所引入的，而与苏联的接近也是事实所迫的。","path":"2025/07/19/阅读笔记-中国的亚洲内陆边疆-5-新疆/","date":"07-19","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-4-满洲","text":"原作：拉铁摩尔，《中国的亚洲内陆边疆》 ，第五章 满洲历史上的分裂 东北可以分为三个地区：南部辽河下游的耕地，西部的草原，以及东北部的森林。这三个地区时互相有所渗透，边界特征并不明显。 满洲是一个外来名词，来自于西方侵略者。“满洲国”的概念是一个政治虚构，强迫东北民众承认被征服的地位。满洲本身只是一个地理概念。 东北南部与中国的关系 气候上和环境上，东北南部与中原地带类似，都适合精耕，且水运方便。但是在贸易上和中原的往来不方便。所以直到较晚的时期，这个地方更加高丽化。 在南部“汉边”之外的地区也有精耕区域，但是精耕区域的分布比较散，而每片小的精耕地区又在森林或草原居民附近；汉族也无法容易地越过山地来到那片区域。于是汉边就成了一个孤立的区域。沟通这个区域和中原的路径可能会受热河山地的威胁，所以和山东的海运就成了一个重要的通道。 东北地区北部及东部的环境与经济条件 新石器时期开始，农耕在这个森林地带成为了最重要的中心活动，但同时也有采果子和养猪和打猎。男子更多负责打猎和砍伐，女子更多负责农耕和养猪，是社会中最稳定的分子，所以这是一个更多以女性为中心的社会。 再往北部，气候较冷。松花江上游河流缓慢，于是捕鱼得以发展。在往北，气候更加寒冷，农耕和养猪的工作开展较少，反之渔猎技术逐渐发作战，男子的活动比女子的活动就更重要。 环绕东北地区顶部，养猪就不再有了，而是依靠驯鹿。驯鹿的运输是大量的，所以在这里梗依赖于狩猎的经济。再往北部，西伯利亚的森林变成版北冰洋的苔原。在这里驯鹿更加重要了，相当于别的地方的羊和马。他们在苔原上放牧驯鹿为生。 清朝始祖努尔哈赤 努尔哈赤的祖先起源于金朝的女真的外围部落。爱新觉罗也是从金朝的金字对应的。努尔哈赤早起征战的时候自称为后金。满这个概念是清朝才有的。 努尔哈赤的事业发自汉边的边缘。努尔哈赤也是一个没落的贵族，青年时想为丧生的父亲复仇。早时，努尔哈赤也只是一个一个联军的小将领，但当帝国的掌权者和有势力的人被既得利益所累，他们的利益遭到重大损失之后，努尔哈赤这样的小贵族就获得了利益和权力。 16世纪末东北地区的政治 满足是12及13世纪建立金朝的女真的后裔。14世纪的明朝，在东北地区的势力不如蒙元时期，于是一些北方部落重新回到了他们认为其先祖所在的南方。 其中一些部落来到了汉族贸易及政治势力所及的地方，融入汉族的生活。这种转变并不困难，因为他们原本就是有多个生活方式的农耕文化，于是他们要做的只是调整文化的偏重。由于这样的原因，他们的南下比汉族北上更简单一些。当他们接近汉族的时候，他们成为汉族和森林居民之间的中介，并可以控制新兴贸易。当然这个利益背后是他们减少了移动性，受到汉族的行政制度的支配。这制造出了一种封建组织，但又不是并非像真正的封建制度（真正的封建制度建立于个人的附庸关系，而他们并没有作皇帝个人直属附庸的资格）。 这还带来了一个变化，由于农田职业的细分，没法在想之前一样女人农耕男人狩猎。男子需要在田中工作，而狩猎成了贵族的权力。王公占据土地，拥有自己的农田，用奴隶，佣工和佃农承担农活，而打猎成为了自己的娱乐活动。 东北边疆上汉族统治的衰微 &amp; 努尔哈赤的工业与清朝的建立 努尔哈赤的祖先们根本不可能想象政府汉族。他们最大的支援是能受中国皇室的接待，因为这有助于提升他们的地位，比做边疆官吏的属员地位更高。他们依赖于发展农耕，以及贸易。他们更倾向于将势力伸向原来的野莽之地，索取真皮和猎获品。就算边疆不安，其他民族侵入的环境已备，保护既得利益的本能限制了非汉族的边疆人物的政治眼光和活动范围。就像俺答汗，他用忠诚敲诈获得了权力，但为了保护自己的特殊利益，并不会将自己的权力作为征服中国的企图。 于是，只有努尔哈赤这种小的贵族阶层可以自由利用汉族统治衰微导致的新兴的不平衡势力。 由于明代末年的腐败，导致努尔哈赤的父亲的权益受到更高贵族的侵害。父亲的死亡让他在同阶层中更不利，所以他要发展自己的势力，领导并庇护地位更低于他的人。通过不断给予部属权力和利益，一批新贵族被早就。这批新贵族必须不断与旧贵族对抗以保卫得到的权益。 努尔哈赤占领了汉边边缘上的军事战略地区。草原与森林居民联合抗击努尔哈赤，但是失败了。这可能是因为努尔哈赤出身自森林，又熟悉汉族对草原地区的影响。控制了森林居民就可以将森林和河谷居民并入满足，控制了草原居民就可以进入草原深处。1616 年，努尔哈赤自称为金汗，标示着一种部族的领导权。1618年，他发表了讨明檄文，宣告独立。努尔哈赤逝世的时候已经征服了汉边的大部分。 清朝开国时的军事与政治组织 从征服边疆到大规模征战，一个重要的问题是对汉人的利用。这个问题的要点是，当外族压力增加且无法从中国获得援助，那么边疆汉族社会就会脱离中国，投入到入侵者的政治力量中。 原本满族也只是若干个部落。为了将这些部落形成更强大的军事力量，努尔哈赤发明了旗制。他从原有的“牛录”出发，将部落人口按旗编入统一的牛录。旗没有明确的地域。八个旗组成了一个有职业军队的国家。 关于吴三桂降清，拉铁摩尔认为不能忽略吴三桂本来就是东北\"汉边\"人，他在那里有降清的旧友以及产业关系。而他降清的目的，也友想要从满族手中夺取满汉集团，控制汉边区域的目的。 汉人在满洲的影响 明朝时候，朝廷限制向汉边的农业发展，防止那里发生独立。而在清朝，他们需要留下可靠的人员防卫边疆，还要阻止早期满足部落与草原部落建立联合的传统。于是他们派遣贵族和蒙古王公向那里大力发展汉族的农耕、城池和工艺。这不会发生分离的倾向，因为这些人的政治目的不是独立而是升迁。 对草原与森林居民的影响 满族从通古斯语的森林以及河谷部落中征兵，以避免被在汉边征募的汉人势力压倒。满清给边远部落以“新满”的名号，以类似于旗但是比较松散的方式阻止他们；而对于草原，则利用联姻的方式。 通古斯族一直以来，在进攻时为“内侵“，败退时为”回撤“，并不会像清朝/金朝在灭亡时，边缘部属直接分裂解散。但是通古斯族一直保持着自己的社会方式。西伯利亚通古斯族，12世纪和17世纪的生活方式没有什么变化，虽说这些传统制度在20世纪已经被现代科技摧毁。但在努尔哈赤时代，清朝的通古斯族有功能良好的制度，可以到南方服役，可以升为将军，可以做官。 19世纪的满洲 这里主要探讨的是俄国的影响。19世纪，清朝作为一个中国的朝代，和明朝一样，限制汉人向东北发展。但是俄国在西伯利亚的发展使得移民东北防止俄国的入侵变得有必要。19世纪中，随着俄国本部的西方化，西伯利亚作为有殖民地属性的地区，急需商品的殖民地市场与农业移民。于是他们找到了蒙古和满洲。 这和西方在沿海的活动有所差别。俄国的影响是一个旧关系的转变，不像沿海的活动是一种突然的接触。东北地区则是俄国的陆上发展和各国（主要是日本）的海上发展相遇的地方。 但是移民防止俄国人入侵的政策失败了。汉族只在东北的南部和中部大量发展，吸收了东北汉旗的大部分，使得汉边更像一个内地而不是边疆。但仅仅于此了。东北方面，汉人的社会力量不能够充分占据东北部分的北部。而北蒙古方面，汉人与中国内地也没有足够的经济联系而成了游牧经济的附庸。 铁路的影响 铁路对东北很特别，因为在东北的铁道里数比别的地区高。这增加了中国在东北的利益，但是却减少了中国在东北的统治。因为这些铁路所增加的农产品的输出，都是通过外国的贸易用外国的货币供外国铁路投资以盈利，这一套系统都是在西方人的手里。整个中国实际上都是受到外来势力的控制。 特别的，在这样的情形下，有一些集团自行分出，受雇佣从事商业活动，也经营自己的事业，获得利益，并剥削其他中国人和边疆民族，形成了次帝国主义的现象。而士大夫阶级来自控制农业经济的地主阶级，他们支持着官僚。这两个阶级形成了对立关系，构成了内战时期的背景。 在满洲，两条主要铁路分别由俄国和日本控制。张作霖是上述中的旧阶级，但是他却不能按旧的方式做，因为他实际上是处于俄国和日本的包围下。 后续俄国的退出，导致了日本得到了控制东北的专利。注意到，外国帝国主义没能直接统治中国其实来源于列强内部的侵轧。列强原本支持日本在东北的活动，是为了抗衡俄国。而当俄国被淘汰之后，东北就陷入了比别的地方更加殖民化的地位。 于是在东北地区的防卫上，中国无法利用别的列强势力与日本抗衡。苏联放弃了一部分帝俄国时代对中国东部铁路的专利权，这让中国方面可以去建设一个中国的铁路网，防止日本侵略的深入。中国人通过没收土地的方式增加了对蒙古草原的压迫，也给了新兴阶级剥削贫苦移民的机会。但另一方面，这种亚帝国主义和剥削也是一件对抗帝国主义的防卫措施。 日本的地位 首先先分析日本本身：日本在西方化后，权力集中在了贵族的手中。这些贵族是封建的贵族，同时也是资本的贵族，而不是被新兴的资本主义阶级取缔。贵族阶级控制着征服金融和工业以及军队，于是军队和政府是联系在一起的。这增强了对原料的控制而非购买的需求，于是对外扩张紧跟着贸易的发展而来。 日本无法和之前的民族一样，在历史上继承满洲，而只是在地理上控制满洲。日本仍然在压迫农民，这使得国内市场无法得到有效开发。而日本有需要将满洲的生活水平压得比农民的生活水平更低，否则会引起军队的不满。于是满洲市场也无法开发。 这种对封建压迫的固执导致了其不得不不断征战，从高丽，到满洲。不像努尔哈赤那样，日本人不愿意分享其侵略所得，反要让他们付出代价，于是无法获得在满洲的中国人的自动服役。努尔哈赤到后面是步步积累，越打越容易；而日本的侵略方法则是硬干，越干越难。 拉铁摩尔认为，日本的侵略会更早达到利润耗尽的失败点。在此之后，由于列强原本放弃了联合监督的制度，后面再想恢复也是不可能的了。这会使得东北的边疆出现一个稳定的局势。在此之后，需要放弃次帝国主义的做法，而是用新工业技术来整合农田、草原、森林经济。 读者注：从后续的历史上来看，拉铁摩尔的在1939年的预测并不正确。相反，日本以东北为起点，在二战中对中国开展了大规模的侵略。我认为这和当时日本与中国的军事实力差距远超历史上任何时期有关。但是，其大体的思想在抗日战争胜利的结果上有所体现。日本的封建固执以及对殖民地区的压迫不断成为自身侵略的阻碍，和历史上别的民族的征服有着本质的差别。","path":"2025/07/16/阅读笔记-中国的亚洲内陆边疆-4-满洲/","date":"07-16","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-3-蒙古","text":"原作：拉铁摩尔，《中国的亚洲内陆边疆》 ，第四章 黄河流域与蒙古地区早期文化的差异 蒙古草原最大的差距就是，游牧成了统治地位的制度。灌溉农业可以造成最高的经济以及人口的集中，二序幕是一种散漫的经济制度，社会也没法集中。 在从黄土地带向外发展的时候，环境允许中国人取得一切土地，吸收并同化所遇见的民族。但是走进草原的时候，环境却不利于汉族了，这使得原本生活在上面的少数民族更有效地抵抗。于是各种相对落后的制度，不但不能被客服，反而更形成强化。 在最开始，由于底力转变其实并没有明确的界限，并且半灌溉地区也不是不能种田，农民和游牧民的差异并没有那么大。 草原游牧社会的兴起 从什么时候开始，差异变得开始显著了呢？一个很重要的阶段和驭马技术的差异相关。中原地区的人们使用马厩饲养的马驾车作战，而游牧民族使用在草地上放牧的马骑马作战。草地放牧的马工作能力更小些，于是需要数目则更大。这又引起了对于牧场以及方便转移牧场的社会组织的需要。 中原地带确立灌溉的主要地位的时间非常早。公元前 2000 年中原地带就已经形成了高度发展的农业。其后的 1000 年内，汉族与少数民族之间的战胜记载增多，但是没有提到这些少数民族是骑马游牧的民族。而有关起码名族的记载始于公元前 500 年，这个时候少数民族开始骑马侵扰边疆了。这些骑马民族的名称与之前的少数民族的名称相同，于是可以确定在那个时候游牧民族的迅速兴起。游牧民族可能是因为来自汉族的军事排挤等原因，从半畜牧半灌溉全面转向了游牧社会。 在公元前3，4世纪，汉人学习游牧民族的骑射战术，并以此建立起政治势力（赵武灵王的“胡服骑射”改革，以及刘邦学习匈奴的战术）。这说明这个时期中国和少数民族的生活方式已经可以分庭抗礼了，并已经使得汉人需要在某些情况下去学习它了。 草原社会兴起的功能解释 中国历史对早起草原社会兴起的记载几乎没有。大概有这些原因：草原游牧社会没有留下像大禹一样的神话的文化英雄；游牧经济技术起源很晚，且未能与中国文化调和；游牧民族本身也没有文字记录的材料。 当游牧民族开始完全草原游牧生涯的时候，马就很重要了。它使得减少农耕，增加畜牧所需要的机动性能够有了更大的范围和速度。骑马的发展程序是：放弃过渡性文化而转变成完全的草原文化；天然依赖完全放牧的牲畜，没有存储的饲料；机动性的需要增加；对管理马匹所需技术的特殊要求；完全掌握了骑术和马群的管理。 当然，还有一套类似的军事技术的进步。草原上没有树，于是弓的木材需要节省，并且骑者也能方便携带；于是就有了短弩；然后为了在骑马时也能射箭，于是有了马镫。 一种技术只有在适合一个社会的需要时，才能显现出其重要性。关于这些技术的探讨，有人认为可能是别的有这些技术的民族侵入并占据主要地位导致的，或者技术是从别的民族那里学来的。但是侵入者无论怎样有力，如果没有合适的生活方式都不可能留在草原上；短弩在社会没能充分发挥其特点的时候无论学和发明都没有意义。于是真正重要的问题，是社会与技术的互相影响，而不是技术造就了社会或者为了对付汉族而发明技术。 草原社会经济与中国本部情形的比较 草原社会经济最讲究的是移动权，而“所有权”本质是循环移动的权力。从部落上来讲，控制大规模的移动循环并留给其他部落次等的移动循环，能获得优势。而在部落内部，则是将最好的牧场的使用权分配给贵族，虽说贵族没有完全的所有权。部落的领袖是其部下移动权的保护者，而又向其部下征税以及劳役。 就像平原上劳力控制权阻碍了矿冶和工业的发展一样，这个移动权也影响着草原经济的发展。草原的某些地区的草可以割下来作为干草，在初春季节具有很大的价值，但是割草回涉及到牧场所有权和限制迁移的问题，个人不愿意建立这种会让他在部落迁移的时候失去社会保障的私人所有权，而部落也会维护这种移动社会的机制阻止这种做法。同时，有的很大的牧场有时候没人放牧，因为需要很深的水井，但是这种水井同样也会因为其超高的价值而固定所有权，这不符合社会的一般利益。 于是在今日的外蒙古，我们可以看到这种情况不复存在，因为上述的机动性制度已经过去了。于是牲畜的质和量都得到了普遍的增加。 草原社会对同中原的贸易需求，比反过来的贸易需求要更大。因为在这样的必需品分配较为普及的社会中，必须要用这个社会以外的奢侈品来区分贵族与平民；同时，对盐和铁的缺乏，也很需要同中原的贸易。于是中原的商人很容易控制了草原的对内和对外贸易。 手工业和早起工业的发展也是一样，受到了移动性的影响。蒙古铁匠的地位很高，但是如果想在原料产地住下来并较大规模地发展工艺则是不可能的。所以，当需求达到某一个程度的时候，这些工业制品就着重依赖于贸易了。草原上的工业发展因此被阻止。 草原历史的阶段特征 在草原部落的移动需求得到协调的和平时期，就会出现牲畜过剩。这种情况下，部落领袖就会利用部下的机动性，趁着中原朝代衰弱的时候侵略，或者中原朝代强盛的时候进行贸易，或者在领地招徕汉族的居民前来耕种。到了一定时候，这些违反草原准则的现象，又会引起草原社会与农业社会不协调的问题。草原首长要治理农业和游牧混合的社会的时候，就会无法调和各种职责（对侵入草原的中国边境的统治者也是一样的），而这种农业和游牧的利益的冲突就会破坏这个混合的社会，出现混乱。然后再在两方退回到对自身有利的环境中时回归平静。 （读者注：一个典型的案例就是南北朝时期，最著名的莫过北魏孝文帝，最终也未能成功） 在破坏与清理的过程中，最终站出来的新的政治人物多半是统治阶级底层的人物，因为他们对政治有所了解，而又不像更高层的政治人物在混乱中被摧毁。成吉思汗就是这样的人（地位较低的贵族）。并且决定性的地理位置是在完全的草原地带，因为这个地带没有受到短暂的混合社会的影响。 游牧经济的种类以及羊的重要性 游牧经济有很多细节。比如在潮湿的土地上羊和骆驼长不好；马需要石灰质的突然，骆驼需要含盐的土地；羊吃草比其他牲畜咬的深，于是应该先让牛马吃再让羊吃。这就影响了不同牲畜的不同地位。 马在战争中有用，骆驼在远距离和贫瘠的地方有用；牛的乳和肉最多，力气也更大。 但是羊提供的经济价值是最多的。羊毛可以织盖蒙古包的毛毡，羊皮可以做衣服，羊乳可以做奶酪和奶油，羊肉可以吃，羊粪可以烧。 当然羊不是万能的。羊走很慢，也没法负责运输。牲畜运输的技术发展是区分蒙古草原上养羊的游牧民族，和中原过渡地区养羊的混合经济的民族的重要特征。 很多军事优势就是为了保证羊群和牧羊地的。 财富与移动性 自给自足的经济资源的固定性，和对各种资源的取得与利用造成的移动性，以不同的比例成分混合在一起。这里面的极端可能是马上的战士，或者在戈壁中骑骆驼的人，或者是精耕的田地。最后一者在草原上被孤立，并且被王公所占有。但是马上的战士累计的战利品过多就会丧失移动性，骑骆驼的逃避者无法获得财富，而被保护的绿洲限制了保护者的移动性。于是没有一个方式是可以永远自洽的。 游牧民族的历史循环前面已经提到过，无非是通过移动性强盛，累积财富之后征服中原，随后又依赖笨重而容易被攻击的农业机构，脱离了本身权力的根源，随后被叛乱或者新的游牧民族击败；那些小的官吏或者地位较低的贵族获得草原权力的根源后，又以移动性开发财富。 这种历史循环在满清结束。蒙古民族成了满清的附庸，一个定居社会的游牧附庸必须要减少其游牧循环，而代之以更严格的土地制度。因为统治者必须要了解每个部落能出多少兵，也要了解到什么地方征集；统治者要掌握牧场和移动性的支配权。 领地附庸制度保持土地单位的分裂，从而分割部落组织。用类似推恩令的方法，王工数量增加，土地进一步分割。而边界争端问题不再以部落冲突的形式解决，而是只能调解，防止土地的吞并。 同时由于中国商人很容易地控制了蒙古地对内对外贸易，而王公也很轻易地参与贸易分享利润。于是他们通过限制蒙古商人的移动，阻止蒙古商人阶级的兴起，而自己横在中国商人和消费者之间，对交易征税。 喇嘛教（元朝） 蒙古人在13，14世纪入主中国时，开始接受喇嘛教，来维护国家的统一，并团结蒙古民族。 为了不必依从于中国的士大夫阶级，成吉思汗令维吾尔族人制定书写文字和文官制度，并任用一部分天主教维吾尔人和回教徒。忽必烈继续推行这个政策，想引入一些天主教教士，建立于中国文化不同的蒙古文化。但是这都没有成功，这些外来宗教没有一个能于中国官僚制度严密有利的规范相制衡。 马可波罗的记载中没有提到中文，这说明了在中国的那么多年，他大概完全依靠蒙古文或者波斯文。另外的材料显示，这个时期，各类公文常用半蒙古化的中文书写。 喇嘛教（明朝） 喇嘛教是一种掺和佛教和涅斯托利教和摩尼教教义的混合宗教。喇嘛教在明朝再度传入，最初的教徒是住在中国边境上的蒙古人。 朱元璋远征蒙古，消除了蒙古民族短期内再度入侵的危机，并且将边疆的保护工作交给内蒙古的附庸部落。这些附庸部落没有能力伤害中国，并且为了自身利益也不愿意与草原部落携手。 使得喇嘛教再度引入的人是俺答汗。在统治前期，他统一了其他漠南蒙古部落，并侵扰明朝边境，后来又转向和平，设市贸易，以获得利益，被明朝封为顺义王。在西方，他的势力一直延伸到了西藏。 引入喇嘛教的起因和成吉思汗类似。因为他需要有一个联合的力量来统治混合经济，而转向宗教。并且他也不希望用中国的元素，于是喇嘛教便成了答案。并且，拥有产业的寺庙，也可以使得具有混合经济和社会的地区，获得较为妥善的额管理。这是因为寺庙能以团体法人的资格获得产权，处于游牧&amp;定居的贵族家庭之间，提供一种中间性的经济规范。 喇嘛教（清朝） 蒙古势力变得重新强大，但后来满足联合东蒙古防止察哈尔部发展成更大的势力。后来内蒙古很快投降满族，这使得满足直接拥有了一个内边疆结构。后来，西蒙古试图西向西藏，东越外蒙古建立新的帝国，但是后来还是被满足乘虚而入。 在西蒙古侵入西藏的时候，他们以活佛为附庸，赋予喇嘛教一种新的政治作用。在这个时期，蒙古的野心家推选其亲戚担任宗教要职，以完成政教结合。西藏的第四世达赖喇嘛就是蒙古人。而满清在争取统治权的时候，继续利用这种政权与教权的联合。他们建立了蒙古的双重政治，一个是以西藏为归趋的宗教，一个是以朝廷为归趋的王公。他们规定王公的子侄不能被选为活佛，而又用制度控制活佛的选择，以维持这种双重政治。活佛在精神和世俗方面都有权利，是帝国的另一种附庸。 满族统治下的蒙古 前面已经讲述过在满清统治下，部落之间有了固定的边疆，移动性减少。 中国商人还是掌控着蒙古族的贸易，并且各类工匠也是汉人当的，这进一步摧毁了蒙古自给自足的经济，从而依赖于汉商。这也导致人口的过剩，过剩的人口被召庙所吸收，成为喇嘛。有些高级喇嘛和王公以及贵族，对社会的作用从改进生活逐渐转移到与汉商合作，分享汉商贸易的利润。他们通过集体责任（部落对每一个人的债务负责，每年都要偿还汉商的高利贷），以及义务劳动。义务劳役其实也是在帮汉商免费做事。 近代的蒙古 19世纪后半期的蒙古，经济逐渐崩溃，社会底层情况已经很糟糕了。尽管牲畜很多，但是普通牧民消耗拥有的是很少的。但是游牧民族的底层活得比农业经济的底层获得好，因为君主是以看待牲畜的眼光看待部属的，所以总而言之也不会压榨到不能活的地步。 游牧民族穷人移动受到限制，牲畜所有权从牧人转移到王公手上，后来又到汉商手上。这是一种和租佃类似的关系。牲畜被交给牧民看管，然后连同繁殖增加的数量一起收回，构成了对牧民的剥削。 喇嘛教的社会功能（而不是教义）使得蒙古变得不好战。召庙的不动产强化了部落之间的稳定性，东西蒙古之间的宗教冲突也不存在了。于是游牧民族的战争废止了，代价是经济的倒退以及社会的奴隶化。 20世纪，变化开始变快。这是因为铁轨带来了很多移民，并使得谷物输出的方向改变（原本谷物运出草原没有利处，因为在草原外牲畜会把谷物给吃完；而运进草原的时候牲畜可以直接吃草。铁道改变了这一点），这使得草原内的农耕经济也得以发展。 进入蒙古和中国内地的新势力不允许新的移动性历史循环发生了。外蒙古先是被沙俄入侵，后来又被苏联资助和军队控制。原本中国在处于西方帝国主义的压迫之下时，也利用铁道和火器对蒙古实施近似次帝国主义的政策。而当日本帝国主义入侵的时候，两个名族就出现了为共同利益而简历里新的合作关系的可能性，联合抗日。这两种历史上一直冲突的经济，在现代条件下可以互相为用。","path":"2025/07/13/阅读笔记-中国的亚洲内陆边疆-3-蒙古/","date":"07-13","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-2","text":"原作：拉铁摩尔，《中国的亚洲内陆边疆》 ，第三章 中国文化发源于黄土地带 中国历史的根源分布很集中，一个是北边的黄河流域中部的主要重心，一个是南边的长江流域中部的次要重心。一开始是北边文化更占优势，而到各种势力的总和决定发展方向的时候，南方的发展则颇为积极，并且比较稳定；而北方的发展不均衡，时进时退，长城咸就是这种变动的表现。 中国最古代文化的重心是黄河流域中部的黄土地带。为什么不是黄河流域下游，或者南方的长江流域呢？这是因为黄河下游形成巨大的沼泽地，而长江流域不仅有沼泽，还有茂盛的森林。而只有很复杂的倒闭生产灌溉技术才能将这些地方发展成有利的农业地区，容纳大量人口。而在没有更简单的经验之前，这难以做大。 为特福认为，典型的中国民族一定是在黄土地去获得第一个最重要的进步，不是因为那里土地肥沃，而是因为容易耕作。黄土地区的小米喝小麦虽说不够多，也不如稻米收获大，但是耕种起来更容易。在黄河东转前后的支流的河谷地区中，可以取水而不受泛滥之害，于是进不到了最原始的耕作。 古代中国文化鱼黄土地带的土壤气候之关系 黄土的土壤适合耕种：黄土层没有石头，于是可以用最原始的工具耕作；突然的垂直节理允许在黄土崖边建造摇动，冬暖夏凉，地势优越（可以躲避盗匪）；黄土的大量空隙可以迅速吸收水分；腐烂的旧草根转化为肥料，由雨水或河水回升到表面上后变成溶解的天然肥料。 而黄土地带的气候，旱季不至于太干旱，让人们不需要被迫他迁。并且，在新石器时代，挖一条小水渠也不是困难的。最初简陋的取水灌田工作可以由一个家庭完成，但当规模变大时，就需要人们协力挖掘更大的渠道，修筑堤坝。 土地利用的进步使得共同劳动不可避免，于是在那个时代，通过共同劳动可以更加高效地实现权力。如果一个人能够决定多少人到一个地点去取得新的土地和水，那么这个人就掌握了统治的实权。 从黄土地带向外的早期发展 拉铁摩尔认为向外的扩展主要是由河道的。大禹的水利传说，则标志着中国人从原始的农业技术进入较为广大地域的农业发展时代。 当他们向下游的平原发展时，河水很容易造成泛滥，造成沼泽并改变河道。这就需要社会成熟到能够大规模从事筑堤工程，才能在那里建立永久性的农业。原本居住在这块地方的以捕鱼等方式生活的居民，后来也逐渐归并到中华民族中。 这样的路径对长江流域也是类似的。北部环境的优良条件让人们优先进行到较大规模的社会组织。北部的一些人将这些耕种技术带到了南方（却与北方的老家在社会上独立），使得南方也成为了一个文化中心。在一段时间内，南方的社会与政治发展能力也组建起了大的社会并长期发展。 在南北两个地区都形成了农业景观，其标志是大型的，筑有围墙的城市。精耕土地的边缘是粗耕区域，再往外就没什么有意义的农业活动了。 但是往更北方的草原边缘就是另一种情况。内蒙古地区更多是粗耕并采取混合农业。汉族曾屡次越过长城，但是却也犹豫不决，而草原民族入侵中原也不可能在长城以南建立草原经济。 通过在长城以南的发展，我们可以发现，中国的每个地区都支持一个进步的文明，而第一个主要发展的地区是对文明初期发展阻力最小的。在黄土地区的灌溉基础上的精耕制度是第一个获得充分发展的趋势，于是在之后的历史上也趋于一致化而非多元化。 而长城以北的发展，显著的差异取代了一致化。北方草原的游牧经济和南方完全不同，于是产生了一个对掌握长城边疆的民族的永恒的矛盾，即应当选择何种经济形式。 中国历史的形式 拉铁摩尔认为，促成国家的权力的重要因素是水利工程。建立并维持灌溉制度所需要的水利工程，无论是一个怎样富有的地主都是无法私人完成的，所以就需要国家经营。于是国家从事水利工程的能力就进一步地成为了政治力量地基础。国家也要有大量存粮，存粮需要有一个便于保护的重心，即城池。而每一个区域存粮的一部分又集中在某些重要地方的仓库，由政府支配。 存粮的设置让灌溉地区比粗耕地区有利于围城和守城，并且也可以支持大量z的劳役工作。存粮的运输又进一步地依赖运河地建造。这些运河一方面是交通要道，一方面又是灌溉系统的命脉。 于是运河的地位就变得非常重要。一个朝代若有能力维持并保护运河，那么其货物运输量超越一切海道运输。并且对运河的控制也可以早就主要经济地区与主要政治地区的均衡的成熟形态。能统治中国的朝代的政治及军事首都都在北方，以监视不同化的边疆，而与主要经济地区的联系则依赖于运河。 贸易，矿冶与官僚 茶和丝是农业活动的特产，可以在商业中随意流动，而无法通过普通农业活动生产的盐铁则需要特别的管理。所以在历史上，国家赋予/转让盐铁的经营是一件有很大利益的事情。盐的重要性不言而喻，盐的禁运也是1936年共产党被从江南地区逼走的主要办法。 矿业被税收制度所深深影响。拉铁摩尔认为，矿业的发展地区远离密集发展的农耕区，而官僚对矿业并不注意，只视其为财源。每当一个矿发财了，则税收立刻提高，于是采矿技术的改良便寸步难行。采矿发财了会被官府征税与监察，而不开发矿产则没有罪，于是矿物资源的开发受到阻碍。 各种事务上代表国家利益的是官僚，他们并不像农民和商人一样大多数情况都久居在一个区域，而是随国家的需要移动，管理不同的区域的政事。 中国历史的循环 集中人工修缮水利与农耕（利润增加）-&gt; 生产达到高峰，一切平稳，统治者集中力量维持秩序（并抑制别的努力）-&gt; 雇佣多余人工的新活动被阻止，人力过剩，人们没有工作而不能生活 -&gt; 农民暴动，推翻政府 -&gt; 残余士大夫阶级重新组织管理，建立新的朝代。 （这个道理真的多吗？感觉这并不是主要因素啊。） 19世纪，西方的入侵 这章主要讲述了西方人眼中，何为典型的中国社会。 商人的地位低于地主阶级。在古代中国，将资本单独投资于生产和销售很困难。在借贷关系上，农民也更愿意向地主借钱，并且担保也更可靠，因为地主直接长官农业。于是在政治上，地主的更有力量。 文学教育将士大夫和官僚连接为孪生的统治阶级。难以学习的文学分割了社会的阶级，但是却联系着各个地区。文人的地位很高，因为无论如何应用的军人，也必须熟知颇为高深的文学，以及有一群文书幕僚辅佐，以控制一个地区的生产税收系统。就算是外来的征服者也不能摧毁中国的文人，因为无论谁统治，实权其实还是在文人手中。 于是，典型的中国人有两种，一种是地位较低的农夫，一种是文人。“文化”是一种腐败而雍容多利的特殊阶级的专利品。 而当西方入侵的时候，旧秩序中有钱有势的官僚阶级竭力反对，而商人从西方能获得的礼仪则比从原士大夫阶级能获得的礼仪要更为丰厚。产生士大夫的家庭中，一部分人通过经商的方法兴旺起来，而另一部分紧紧抓着旧方法而日趋没落。","path":"2025/07/10/阅读笔记-中国的亚洲内陆边疆-2/","date":"07-10","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-中国的亚洲内陆边疆-1","text":"原作：拉铁摩尔，《中国的亚洲内陆边疆》，第一、二章 历史上的大陆及海洋时代 中国的古代历史上，重要事件都是发生在内陆地区。这在旧欧洲，中东等地区也是一样的，而地中海区域的海洋活动旧很多了。 新海权时代产生于欧洲的原因，和近代资本主义的发生发展有很大的联系。与之冲突的旧社会结构仍保持着路权时代的权益，于是资本主义的社会力量则与海权取得了联系。 陆权与海权对中国历史的影响 在哥伦布时代之前，中国的外务基本旨在长城边境，而海外的外务直到明朝才有所发展，例如欧洲的火炮在那个时候传入中国。满人入关是长城边界上发生决定性作用的最后一浪，而在此之后的近代，海上涌入中国的势力已经不可抗拒。 我们可以看到日本侵略东北表现了海上势力与陆地势力的直接冲突。这是让原本的内陆边疆受到海上势力支配的企图。 中国的边疆扩展 在中国古代，汉族挤在两河流域附近生活，并没有移民于长城之外。在早期的中国历史上，移民于政府方向都是从北到南，从西向东。 而在近代，则有了汉族向东北地区的大规模移民。但这个移民并不是古代中国的方式，而是近代修建铁路所促成的移民，是由西方工业化和随之而来的政治运动造成的移民。原本汉人的人口压力，经济和军事力量都无法完全控制东北地区，使得东北地区完成中国化。而近代向蒙古草原和东北的森林移民，则完全是新式军械，工业，金融以及贸易活动的产物。 内蒙古地区的边疆和东北地区的发展有类似的过程，但程度不明显，程度也不怎么深刻。这也是因为在这个边疆上，铁路建设不如东北发达，没有外来的投资。再往西去，则完全没有铁路。在甘肃，西藏这块区域，即使在近代，边疆关系也还是更接近于唐代的形式。 长城边疆的地域构成 长城线是一个很重要的边界。长城内外气候不同，语系也不同。长城内农业发达，人口众多；而长城外人口少，较为稀疏，过游牧的生活。 群体生活方式对个人有很大的影响。一个在蒙古的游牧人的体态，语言，宗教，都是是由“他是一个牧人而非农夫”的事实所决定的。”种族“的差异本质上是通过像饮食这样的日常生活方式所表现出来的。 当然环境的差异也不是在长城的每一段都如此明显。这和历史上长城本质上是由战国时期各国的长城所拼接起来的事实相符。","path":"2025/07/08/阅读笔记-中国的亚洲内陆边疆-1/","date":"07-08","excerpt":"","tags":[{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"}]},{"title":"阅读笔记-波伏娃-2-道德观与解放","text":"道德观 相对自由：与萨特所坚持的绝对自由不同，波伏娃提出相对自由. 这个相对指相对于处境，在环境之下的自由. 波伏娃认为个体所处的环境会限制他或她超越自我的能力. 比如妇女，黑人，犹太人，工人等等，他们的行为是受到限制的，需要在既定的处境下才能进行评价. 但是，一旦出现解放的可能性，不去挖掘这种可能性，就是对自由的逃避. 认识到了被压迫的处境却仍无动于衷，应当受到道德谴责，例如，那个年代，妇女常常是她们被压迫处境的同谋. 波伏娃对个体自由的可能性提供了若干的例子. 愚蠢的人，即实现不了从本质到存在的运动的人. 本质到存在的运动即自由选择和实践，愚蠢的人不去探索自己的自由的选择，或是随波逐流，或是听信他人. 他们在逃避自由的过程中拒绝为自己的生活负责，责怪于环境与他人的逼迫，实则自己不愿意做出选择. 这样的人容易被操纵，成为乌合之众. 严肃的人，即为了目的而消灭自身主体性的人. 他们有限地选择了自由，试图被认同为某个对象，而不是在一个无限地运动中广泛选择自由. 他们成为了目的的奴隶，忘却了每个目的也是新的起点. 实际上，人类是在不断地实现自由，每一次自由的选择与实践的目标都是延续和拓展自由，让“人之为人”不断地展开. 冒险者，即只顾去自由地积极地行动的人. 和严肃的人恰恰相反，他们意识到了人的本质是其行动，其自由选择与行动造就了人之为人，但是却没有为行动的后果负责，没有将自己投入到具有人类意义的目标中. 冒险者是一种唯我论的存在自由者，他们没有考虑行动对别人的自由的影响. 波伏娃认为，冒险者的态度离本真的道德观很近，因为两者都是基于“积极地实践着自身的主体性”. 充满激情的人也很接近于本真的道德观. 充满热情的人将自己的存在投注到一个对象上，但是与严肃的人不同，他们将热情对象看作自己生命延申的方式. 但是为什么这并非本真的道德观呢？和冒险者类似，还是唯我论的问题. 这里提到一个词，\"占有\". 将一个对象占有，变为自己的东西，那么这个对象就会变为固定而封闭的存在. 充满热情的人在深处渴望缩减和对象的距离，渴望占有，但是一旦占有，对象就“死了”，所以充满激情的人就陷入了这样的一种矛盾. 解决这样的矛盾的方式是充分意识到目标必须回到自由. 对象并不是自己的衍生，只有自由才是. “我承认你是一个自由的人，即使我深爱你\"，真正的爱，是爱他人的不同. 另一方面，如果筹划以一种静止的状态结束，那么个体就没有获得真正的价值观. 如果个体以写作为目标，那么他只有在不停的写作中实现存在，如果只是写了f一本书就认为筹划结束了，那么这个目标就是一个没有自由的物体，它的目标就没有包含自由. “当人们放弃了单纯的本质而把自身的处境当成一种存在时，人们就获得了真正的道德观”. 这里说的是，不应该将存在投注到一个热情对象上，而是抛掉任何的本质，将自己看作一个处境中自由抉择的存在，并且为自己的行动的后果负全部的责任. 真正的道德建立在这样的一个基础上. 一个重要的东西是他人的自由. 这里是一个逻辑性的东西：如果我的自由是一种例外的状态（别人没有自由），那么所有的道德，爱……都只对一个人有意义，那么我的意义也只不过是我自身的幻觉. 于是我的意义就无法成立，变成了虚无，所以我必须承认他人的自由. 与自由的他人产生碰撞的我，才是真正的拥有自由. 波伏娃通过假设个体实现自由与接受他人自由的过程是一致的，来肯定他人的存在，并且个人对此负有责任. 选择自身的自由，也就是选择他人的自由. 主张个体的自由和努力让他人获得自由是一回事. 于是道德观得以建立：将自己和他人都看作在处境中自由抉择的存在，为自己的选择负责，并努力维护他人的自由. 压迫与解放 压迫与解放是上面的道德观的很重要的一个实例. 如果将一个人视为客体，没有为他打开未来，隔断其个体与其自由，压迫就产生了. 在一个压迫的体系中，被压迫者“注定无望地打发时间，仅仅是为了支持集体的存在”，而压迫者“启发人类，因为他们相信自己可以超越人类的智慧”. 每个人都具有固有性和超越性，而压迫者试图通过使被压迫者的超越性不可获得来否认这一事实. 但是压迫者就是自由的吗？从上面的道德观，我们可以很自然地得出“不是”的结论. 这在主奴辩证法中也有类似的说法. 一方面压迫着要不断去说服被压迫者他们的地位是自然形成地，迷惑被压迫者，阻止他们被解放. 另一方面，个体的自由与他人的自由是联系在一起的，压迫与被压迫也是这句论断的重要例子. 当被压迫者被迷惑，看不到解放是一种自由选择的时候，革命也是无意义的. 这里借用了马克思的思想. 革命与社会的和谐发展是无法共存的，因为革命的目的不是为了保持和谐发展，而是推翻世界的中心并打破它的延续性. 在黑格尔的主奴辩证法中，革命是历史自然的一部分；而在马克思的思想中，革命打破了历史的自然进程. 弃权者就是同谋者. 打破这样的谎言对解放来说是必要的，如果没有受到压迫的个体不去帮助被压迫者摆脱谎言的束缚，那么这个个体实际上也是压迫者. 波伏娃特别提到了“时间”这个问题. 就像个体在与他人的交往中才能存在一样，每个时刻对于整体的时间都是特殊的. 解放中，每个个体的每个行为都是独特的，都是朝着自由的方向努力的，所以我们要利用每一个机会并依靠每个人的自身处境来应对压迫. 这里本质上说的就是拒绝只依赖于整体（无论是人数还是时间），而是需要每个人每个时刻的选择. 另一方面，解放的目标也应当是自由. 解放中对待不公正的方法为我们打开了未来，解放的结果也不是静止的，而是我们必须经常地赢取解放.","path":"2025/07/03/阅读笔记-波伏娃-2-道德观与解放/","date":"07-03","excerpt":"","tags":[{"name":"波伏娃","slug":"波伏娃","permalink":"http://lgswdn.github.io/tags/%E6%B3%A2%E4%BC%8F%E5%A8%83/"}]},{"title":"阅读笔记-波伏娃-1-小传","text":"原作：《波伏娃》，[美] 萨莉·肖尔茨 今天阅读了小传部分，大概就是介绍了波伏娃的一些生平. 西蒙·波伏娃早些时候，对她产生深刻影响的人是她妹妹埃雷娜·波伏娃，以及扎扎. 波伏娃和扎扎在童年时代认识，关系很亲密. 扎扎和波伏娃都出生在信仰天主教的中产阶级家庭. 扎扎在学生时期，不顾家庭安排的婚姻与别人相爱，后来又很早去世了. 波伏娃认为，扎扎在面对家庭的责任，以及自己的自由的责任的选择上非常矛盾. ”爱得太深“ 的扎扎，最后在死亡中获得解脱，因为扎扎不能摆脱自己的处境获得自由. 波伏娃从小就有了这样的自由的意识. 她在教自己妹妹的过程中，感受到了自己为别人服务，在创造一些真实的东西. 通过和妹妹分享自己的知识，让时间锁在别人的记忆中，有双倍的安全. 她称其妹妹为同谋，臣民与傀儡. 波伏娃很早就以文学与哲学作为目标. 书中引用了《一个规矩少女的回忆》中的一段波伏娃对备考哲学教授资格考试的生活的描述，我觉得里面有一些文字很有意思. 波伏娃称自己的性别给予了自己很大的满足感. 自己得到学生的尊重与赞许让自己骄傲，他们的友好让自己不再保持挑战者的姿态，而从美国女性身上看到的挑战者姿态则让她自己沮丧. 她将男人看作同志而并非敌人. 她不否认自己作为女人具有”女人味“的方面，觉得自己不比那些美女差. 她认为扎扎和妹妹感性大度而具有想象力. 她认为自己有一颗女人的心和男人的头脑的结合，是独一无二的，唯一的一个. 其实从这些文字可以看出来，她对女性解放的重点放在了每个人的个体上. 与对立与挑战不同，她认为女性应当充分意识到自己个人的独特性. 我感觉这无论在什么时代都是重要的. 然后就是说到波伏娃大家都能想到的萨特. 当时她和萨特都在备战哲学教授资格考试，并且最终成绩仅次于第二年参加的萨特. 波伏娃将萨特称为自己一直梦想的伙伴，是自己的幽灵. （实际上，我感觉这是某种哲学的智性恋. 但我不懂所以权当乱说.） 波伏娃和萨特在两个人作为伴侣的过程中，也有着不少和他人的感情故事，这其中也有不少的矛盾，两人很多次几乎失去对方. 1980 年萨特的去世给波伏娃很大的打击，几近崩溃. 30年代，欧洲政治局势日益紧张，波伏娃开始对政治事件感兴趣. 39年萨特入伍后，两人开始重新审视自己的职责. 波伏娃进入了自己所称的”道义时期“，担任了所一起创建的杂志《现代》的编辑，并写了许多作品. 1947 年，波伏娃前往美国做演讲的时候，认识了作家阿尔格伦，并和阿尔格伦成为了情人。波伏娃在《达官贵人》（获得了龚古尔文学奖）中将这段恋爱糅合进了故事，但是在《环境的压力》中则描写了更真实的事情。通过波伏娃给阿尔格伦的信在分手前后的语气变化中可以看出其与其哲学著作不同的，热情而又脆弱的一面，以及在工作爱情社会责任之间寻求平衡的心态. 《达官贵人》中体现了其对解放的激动，左翼中的派系斗争，探讨爱情关系的可能，以及对斯大林政权的失望. 1947年，波伏娃开始写《第二性》，并在两年后发表. 这项有关女性处境的大型研究让她赢得了国际声誉，并被称为女权主义运动第二次浪潮之母. 但是她直到 70 年代才将自己成为女权主义者. 萨特和波伏娃在 50 年代开始了广泛的旅游，包括古巴，巴西，美国，苏联和中国. 1957 年波伏娃发表了《长征》，但是这本书写的很烂，主要是为了赚钱. 1958 年波伏娃发表了自传《一个规矩少女的回忆》. 这指的是她在中产阶级价值体系成长起来，又对这套体系进行挑战. 波伏娃在文学创作中经常写出一个矛盾而悲惨的女性的形象. 《灵魂的最高位置》中，主角尚塔尔在日记中将自己描绘成巴尔扎克小说中的人物，并将自己想象为自主的，以及认为是自己学生心目中的楷模. 但是在结尾的时候，读者看到尚塔尔实际上仍然被中产阶级价值观所影响，主角有其自欺欺人的一面. 短篇小说集《被毁灭的女人》有三个故事，三个故事无一例外都刻画了主角悲惨的命运. 实际上，由波伏娃的哲学思想可以明白，这些小说都是在告诉大家，女性应当为自己负责，以摆脱强加在她们身上的限制了她们超越性的社会期望. 1986 年，波伏娃身体状况因为酒精依赖而恶化. 4月14日，波伏娃去世. 在一段时间内，波伏娃仅仅被描述为萨特的助手，人们去研究了波伏娃对萨特的影响（尽管其本人生前一直否认这种影响）. 但后来，人们认识到了波伏娃不可忽视的，独特的贡献.","path":"2025/06/29/阅读笔记-波伏娃-1-小传/","date":"06-29","excerpt":"","tags":[{"name":"波伏娃","slug":"波伏娃","permalink":"http://lgswdn.github.io/tags/%E6%B3%A2%E4%BC%8F%E5%A8%83/"}]},{"title":"毕业歌的背后-二渡千日，踏浪新朝","text":"先贴一贴歌词以及和弦. （bD，bb）再一次尝试调慢校不准的时钟 （f，bA）写下的小说飘散在已错位的天空 （bD，be）夏日的阳光 又倾泻在台阶上 （bD，Fsus4-&gt;F）多少次探出手妄想未来的模样 （bD，bb）早退的观众为终幕献上的鲜花 （f，Fsus4-&gt;F）无人问津地沉寂倚在演员表之下 （bb-&gt;bBm7，F7-&gt;bA）亮灯后 舞台上 跑调的音符开始褪色 （bG△7-&gt;bEm，A-&gt; bD）故事飞跃千日之隔的时差 （bE，bB）繁花突然散开的瞬间 （C，G）跃进不再回转的夏天 （bA，bE）只剩下向前的新航线 未知的遥远 （bA，bB7）忽然间月光在海面映出了那些年 （bE，bB）流萤衔着褪色邂逅串作星链 （C，G）蝶翼抖落牵恋和我对上了视线 （bA，bE）往日的相片 揉碎的诗篇 （bA，bB，bE）铺作梦中红线 擦肩而过后又奔向明天 其实这中间本来是有一段对雨天的描写，从回家路的回忆连接到毕业的展开的. 但是那段旋律设想本身就有点奇怪，并且实在也不知道怎么编配了，并且词感觉也是硬挤出来的，综合这些原因就把这段给删掉了. 其实两个月前就有写毕业歌的想法了，但是那个时候还没有确定要写成什么样子的基调. 那时候我喜欢上的曲子也有几首 —— 旋律忧郁的《海萤》，清新轻快的《はぐ》，以及让感动到流泪的，汇聚着中学生活青春美好的《新しい季節に》（这里权当给大家推歌了吧）. 这几首歌我都有特别喜欢的地方，但最后几经思考，我的中学生活和大多数人是大相径庭的. 若要是让我去思索和机房外的同学的生活（也包括初中的同学们），那更多的则只有没有和大家一起参与更多的日常与活动的遗憾了. 这种情绪以前从未细想过，但在毕业这个特殊的时间节点下，又突然爆发出来. 我最后的选择希望更贴近于《海荧》的感觉. 但是又觉得，毕业其实本该是一件，表明我们应该向未来前行的事情. 并且，毕业的情感也不应该是什么沉重的东西. 我内心所能感受到的遗憾，更多的是被愉悦和幸福包被的少部分空虚感. 关于曲子的构思，我的想法在于前半程表达出在两个千日中，都作为“早退的观众”，营造一种和大家的时间差的意象. 而在 PV 中，人物也以黑白的“褪色”的形象出现，表达出我已不属于这两所学校. 但进入副歌前，高考结束了，二渡千日的时差也随之结束了. 在繁花散开的景色中，我们一起走向了毕业，一起走向了人生的下一旅程，一起重聚，踏浪新朝，一起分散. “一起分散” 是一个很奇怪的词语，但这确实就是我最真实的感受. “分散” 是为数不多的，能和大家一起做的事情了. 所以如果只看后半段，这其实就是一首非常普通的毕业歌，连和弦以及第二段的大提琴也都是最经典的卡农.（一个小故事是我曾经在初中的音乐课上演奏过卡农）. 而这个“普通”，也是追求了这么多时间“独行”的我，在这个特殊的时间节点，矛盾而又真实的渴望. 但前半段的错位与独行，也又是我因为个人的追求，从不想，也从不可能放弃的东西. 我从来没有为这样的选择而后悔过，我也坚信我做出了我最正确的选择. 而我在这整个两千多天的中学生涯中，就是在这样的矛盾中前进的. 我在晚上，因为没做出题而难受的时候，又多少次想念起了初中温暖的二班. 五月八号回来的时候，和一年未见的九班同学重聚的时候，这种我之前未能意识到的感动让我在当天晚上反复回味. 我是何其幸运，能在这两千天中，遇到最好的大家. 而能一起分散—— 并没能做到. 由于要参加期末考试，我无论是 SFLS 还是 EFZ 的毕业典礼，都未能参加. 大家都能高兴呢 —— 在这繁花散开的瞬间，在跃进不再回转的夏天之时，这次，晚了一步的认识我. 只不过，比起这长达千日的时差，这一两天的错过，也不算什么了吧. 这点小小的遗憾，就让它这样保留着吧. 如果伴随着一些额外的突如其来的小幸福，也更好. 2025.06.20，于京沪高铁.","path":"2025/06/20/毕业歌的背后-二渡千日，踏浪新朝/","date":"06-20","excerpt":"","tags":[]},{"title":"2024-25 预科第二学期总结","text":"和上学期不同的是，这学期相对来说就没有太新鲜的东西了. 和我日常社交的朋友们和上学期期末差不多，上的课更多了一些但其实也没啥区别，娱乐方面除了退坑 BA 之外也没变化了. 随便说几句本学期的课程吧. 首先是数学课. 今天早上刚刚得知自己数学分析的分数. 其实也不意外. 期中考试炸了之后，期末考试当天状态也完全不行. 虽说考场上是有惊无险，但分数直接反应出来当时的各种完全不该犯的错误. 希望这 5 学分的令人并不满意的绩点不会造成什么后果吧. 希望如此. 高代其实期中也炸的差不多了，但是期末卷子出的很简单，导致最终结果其实挺好. 虽说其实大家都很高，我的分数虽说也高但是是相对低的，但是都这个分数了也无所谓了. 同样的是程设. 期末考试的神秘状态导致只有 90，是所有认识的人中最低的，但是 90 和 95 又有什么区别呢. 这并不是任何重要的东西. 考得高不能作为骄傲的资本，那考的稍低一点也又能怎样呢. 说完不高兴的，说点高兴的. 其实音数，CV 啥的我还不知道分数. 但是应该都很高. CV 期末虽说做了一张烂卷子，但是反应在分数上其实也还好 —— 只不过破了彩虹的梦（说道理在 CV 课上追求彩虹其实也有点搞笑但是我好像还真是这么想的...无所谓了）. 音数分数没出，但是应该很高. 哦，原来还有 AI 基础... 这门课已经不是课了，反正也不会低到哪里去，已经不想管这个东西了. 乐队方面，猫尾草这学期有了两次演出. 第一次是元火春季迎新晚会，排了影色舞和空之箱；第二次是吉协乐队专场，排了ダレモ和猛独侵袭和泪滴. 第一次其实挺完美的，第二次因为不知道什么原因大家可能都不是很在状态. 我 cos 了波奇，但就是因为如此，戴上了墨镜导致几乎什么都看不到了，很可怕！然后就弹错了一堆一堆的. 但总的来说还是很不错的. 下学期也要给大家带来更多的更好的 live！ 215 去了沪萝但是寒假真的算这学期吗. 但是为什么我 823 的 ppp 的时候我要军训？啊啊啊啊啊啊. 邦多利这学期打的没有上学期勤奋了，出勤倒是多了一些. 好吧但是五一后也没怎么出了，主要是上半学期冲击 w4 的时候出的多一点，之后也少了. 邦多利达成了 28 极，但是之后也没怎么推了. 主要是意识到期末的时候不能推歌，不然会变得不幸... 暑假期间想达成邦多利的红全 FC 成就！对邦多利的热爱还会继续的哦！这学期也入坑了 d5j 但其实玩的不多. 邦多利的经验然我在 d5j 上手还挺快，打算红 qfc 之后开始更多地投入在 d5j 上，因为难度更高的谱面更多一些. 其实其他方面也没有什么值得称道的了. 确实磨磨蹭蹭写了一首歌，这首歌的背后的一些自己的想法也贴在这个博客（链接）. 暑假想要更多地读点书. 去了解一点有趣的历史，去涉及更加广博的知识. 并且也得开始涉猎一些更加前沿的东西，为下个学期进组轮转做准备了. 有关毕业的感想，其实很多多少也只是碎碎念. 能说出来的在歌的那个博客也差不多都说了，也没有太多能整理成文的东西. 至少下个学期开始也要和大家一起前进了. 嗯，就这样. 大家暑假快乐.","path":"2025/06/17/2025上半年学期总结/","date":"06-17","excerpt":"","tags":[{"name":"总结","slug":"总结","permalink":"http://lgswdn.github.io/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"数分笔记-傅里叶级数","text":"Stirling 公式：\\(n!=\\sqrt{2\\pi n}(\\frac{n}{e})^ne^{\\frac{\\theta_n}{12n}}\\)，\\(\\theta_n\\in(0,1)\\). Fourier 级数：\\(f(x)\\sim \\frac{a_0}{2}+\\sum_{n=1}^{+\\infty} a_n\\cos nx+b_n\\sin nx\\)，其中 \\(a_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)\\cos nxdx\\)，\\(b_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin xdx\\). 原理是考虑 \\(\\langle f,g\\rangle=\\int_{-\\pi}^{\\pi}f(x)g(x)dx\\) 的内积空间，\\(1,\\cos nx,\\sin nx\\) 构成正交系. 然后考虑对于式子左右同时积分之后，假如右侧一致收敛到左侧，那么加上 Riemann-Lesbegue 引理直接得到 \\(a_0=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)dx\\)，然后两侧同时乘 \\(\\cos kx\\) 之后得到 \\(a_n\\)，同时乘 \\(\\sin kx\\) 得到 \\(b_n\\). 傅里叶级数性质其实很垃圾. 存在连续函数，使得其傅里叶级数在有理点发散. 但是至少对应于所有连续函数，其傅里叶级数几乎处处收敛，也够了. \\(f(x)\\in C\\)，证明 \\(a_n=b_n=0\\) 等价于 \\(f(x)=0\\). 由于 \\(a_n=b_n=0\\)，所以任意三角多项式 \\(T(x)\\) 都有 \\(\\int_{-\\pi}^{\\pi} f(x)T(x)dx=0\\). 如果 \\(f(x_0)&gt;0\\) 那么就有一段小领域 \\(U(x_0,\\delta)\\) （\\(\\delta&lt;1\\)）的 \\(f(x)&gt;0\\). 令 \\(T(x)=1+\\cos(x-x_0)-\\cos \\delta\\). 那么在 \\(\\frac{\\delta}{2}\\) 领域内 \\(T(x)&gt;1\\)，而在 \\(\\delta\\) 领域外 傅里叶级为均方差最小的三角多项式. （点点收敛性）若一个点左右导数都存在，那么其傅里叶级数收敛到 \\(\\frac{f(x-)+f(x+)}{2}\\). 迪利克雷核：\\(D_n(t)=\\frac{\\sin(n+\\frac{1}{2})t}{2\\pi \\sin \\frac{1}{2}t}\\). 通过一系列推导我们可以证明傅里叶级数的部分和 \\(S_n(x_0)=\\int_0^{\\pi}D_n(t)[f(x_0+t)+f(x_0-t)]dt\\). \\(\\int_0^{+\\infty}\\frac{\\sin x}{x}dx\\). 引入 \\(\\phi(t)=\\frac{1}{2\\sin \\frac{t}{2}}-\\frac{1}{t}\\)，则迪利克雷核 \\(D_n(t)=\\sin ((n+\\frac{1}{2})t)(\\frac{1}{\\pi}\\phi(t)+\\frac{1}{\\pi t})\\). \\(2\\int_0^{\\pi} D_n(t)=1\\)，于是代入得到 \\(\\frac{2}{\\pi}\\int_0^{\\pi}\\sin((n+\\frac{1}{2})t) \\phi(t)+\\frac{2}{\\pi}\\int_0^{\\pi}\\frac{\\sin((n+\\frac{1}{2})t)}{t}=1\\). 而由 R-L 引理知 \\(n\\to +\\infty\\) 的时候前者 \\(=0\\)，于是直接得到 \\(\\frac{2}{\\pi}\\int_0^{\\pi}\\frac{\\sin((n+\\frac{1}{2})t)dt}{t}=1\\). 换元为 \\((n+\\frac{1}{2})t\\)，即可得到 \\(\\int_0^{+\\infty}\\frac{\\sin x}{x}dx=\\frac{\\pi}{2}\\). Riemann 局部：一个点的收敛性只和其局部小邻域相关. 实际上，一个点傅里叶级数收敛等价于在局部上，令 \\(S_0=\\frac{f(x^-)+f(x^+)}{2}\\)，\\(\\int_0^{\\delta}\\frac{f(x+t)+f(x-t)-2S_0}{t}\\sin (n+\\frac{1}{2})tdt\\to 0\\). \\(S_n(x_0)=\\int_\\delta^{\\pi}D_n(t)[f(x_0+t)+f(x_0-t)]dt+\\int_0^{\\delta}D_n(t)[f(x_0+t)+f(x_0-t)]dt\\). 前者并非瑕积分，所以直接运用 R-L 引理知 \\(\\to 0\\). 而后者仍然用 \\(\\phi(t)\\) 的技巧，于是得到后者 \\(\\to \\frac{1}{\\pi}\\int_0^{\\delta}\\frac{f(x+t)+f(x-t)-2S_0}{t}\\sin (n+\\frac{1}{2})tdt\\). 点点收敛性：一个点左右导数均存在，则傅里叶级数收敛. 由于左右倒数存在，所以 \\(\\int_0^{\\delta} \\frac{f(x+t)-f(x^{+})}{t}dt\\) 并非瑕积分，直接 R-L 引理即可. 单调收敛性：一个点在两侧邻域上分别单调有界，则傅里叶级数收敛. 不妨设单增，并考虑使用第二中值定理. 我们考虑使用 \\(\\epsilon\\) 法证明其 \\(\\to 0\\). 首先由于 \\(\\int_0^{+\\infty}\\frac{\\sin x}{x}dx\\) 敛，所以 \\(\\int_0^{A}\\frac{\\sin (n+\\frac{1}{2})t}{t}dt\\) 有界 \\(M\\)，所以任意的 \\(\\xi_n\\) 和 \\(\\delta\\) 都满足 \\(\\int_{\\xi_n}^{\\delta} \\frac{\\sin (n+\\frac{1}{2})t}{t}dt\\) 有界 \\(2M\\). 然后由于 \\(f(x+t)\\) 单调收敛至 \\(f(x^{+})\\)，故存在 \\(\\delta_0\\) 使得 \\(f(x+\\delta_0)-f(x^{+})&lt;\\frac{\\epsilon}{4M}\\). 在 \\([\\delta_0,\\delta]\\) 上可以直接使用 R-L 引理知存在 \\(N\\) 使得 \\(n\\ge N\\) 时 \\(&lt;\\frac{\\epsilon}{2}\\). 而在 \\([0,\\delta_0]\\) 上使用第二中值定理，\\(=(f(x+\\delta_0)-f(x^+))\\int_{\\xi_n}^{\\delta_0}\\frac{\\sin (n+\\frac{1}{2})t}{t}dt&lt;\\frac{\\epsilon}{2}\\). 于是加起来 \\(&lt;\\epsilon\\). 实际上任何有界变差函数（即两个单调函数的差）都傅里叶级数收敛. 因为两个收敛的东西减一减肯定也收敛. 接下来就是两个很重要的定理：Weierstrass 第二逼近定理，和 Parseval 等式. Weierstrass 第二逼近定理：任意连续周期函数可以被三角多项式列一致逼近. （对于任意可积周期函数）Parseval 等式：\\(\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)dx=\\frac{a_0^2}{2}+\\sum (a_n^2+b_n^2)\\). 等价表述（平方收敛性）：\\(\\int_{-\\pi}^{\\pi} (f(x)-S_n(x))^2\\to 0\\). Weierstrass 第二逼近定理的证明中，一个关键在于这个三角多项式列其实是 \\(S^*_n(x)=\\frac{\\sum_{k=0}^n S_k(x)}{n+1}\\). 对应的，我们有 Fejer 核 \\(\\Phi_n(t)=\\frac{\\sin^2\\frac{n+1}{2}t}{2\\pi (n+1)\\sin^2\\frac{t}{2}}\\). 可以推导得到 \\(S_n^*(x)=\\int_{-\\pi}^{\\pi}f(x+t)\\Phi_n(t)\\). Fejer 核是一个好核：其 \\(\\int=1\\)，且 \\(&gt;0\\)，且任意小邻域之外的积分可以被 \\(n\\) 控制. 具体而言，\\(\\int_{\\delta}^{\\pi}\\Phi_n(t)\\le \\frac{1}{2\\pi(n+1)\\sin^2\\frac{\\delta}{2}}\\) 可以被 \\(n\\) 控制. 所以考虑 \\(\\int_{0}^{\\pi}(f(x+t)-f(x))\\Phi_n(t)dt\\)，由于 \\(f\\) 为闭区间上连续函数所以一致连续，可以找到一个 \\(\\delta\\) 使得对于任意 \\(x\\) 邻域内 \\(\\max (f(x+t)-f(x))&lt;\\epsilon\\). 然后又由于邻域外能被 \\(n\\) 控制，所以一致收敛. 然后看 Parseval 等式. Parseval 等式 / 平方收敛性的证明直接用到了 Weierstrass 第二逼近定理. 由于 \\(S_n\\) 为平方误差最小的三角多项式，考虑先用连续函数 \\(g\\) 去逼近 \\(f\\)，然后再放缩到 \\(g\\) 的 \\(S_n^*\\). 即 \\(\\int_{-\\pi}^{\\pi} [f(x)-S_n(x)]^2dx\\le \\int_{-\\pi}^{\\pi}[(f(x)-g(x))+(g(x)-S_n^{*}(x))]^2dx\\)，然后再利用 Minkowsky 不等式放缩成 \\((\\int (f(x)-g(x))^2)^{1/2}+(\\int (g(x)-S_n^{*}(x))^2)^{1/2}\\)，而两者都 \\(\\to 0\\)，故 \\(\\to 0\\). 有了 Parseval 等式这个工具，就可以证明 Fourier 级数的逐项积分以及一致收敛性. 逐项积分：任何可积函数的 Fourier 级数都可以逐项积分. 一致收敛性：导函数可积的函数的 Fourier 积分一致收敛. 逐项积分是平方收敛的直接推论. 考虑逐项积分本质就是需要证明 \\(\\int [f(x)-S_n(x)]dx\\to 0\\)，那么考虑先放缩成绝对值，然后再用 Cauchy 不等式放缩成 \\((\\int [f(x)-S_n(x)]^2dx)^{1/2}(\\int 1dx)^{\\frac{1}{2}}\\) 然后就好了. 而由于所有 \\(\\int_{l}^r |f(x)-S_n(x)|\\le \\int_{-\\pi}^{\\pi}|f(x)-S_n(x)|\\)，对于 \\([l,r]\\subset [-\\pi,\\pi]\\)，而后者由上面知收敛，所以其实这个逐项积分的收敛是一致的. 所以这给出一个推论，就是 \\(f(x)\\) 的变上限积分的 Fourier 级数应当一致收敛. 于是这直接给出一致收敛性的证明：对 \\(f&#39;(x)\\) 做变上限积分得到 \\(f(x)\\)，Fourier 级数也应当一致收敛.","path":"2025/06/13/数分笔记-傅里叶级数/","date":"06-13","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"高代笔记-群","text":"跳过最简单的一些定义和基本性质. 陪集：对于 \\(G\\) 的子群 \\(H\\)，以及 \\(G\\) 中元素 \\(x,y\\)，若 \\(y\\in Hx\\) 则记 \\(x\\) 左等价于 \\(y\\)；\\(y\\in xH\\) 则记 \\(x\\) 右等价于 \\(y\\). \\(Hx=\\{hx\\mid h\\in H\\}\\) 称为右陪集，\\(xH=\\{xh\\mid h\\in H\\}\\) 称为左陪集. 定义 \\(H\\backslash G\\) 为所有右陪集 \\(\\{Hg\\}\\) 组成的集合，\\(G/H\\) 为所有左陪集 \\(\\{gH\\}\\) 组成的集合. 由由于映射 \\(Hg\\to (Hg)^{-1}\\) 给出这两个 集合的双射，所以这两个集合的基数相等，记为 \\((G:H)\\)，称为 \\(H\\) 在 \\(G\\) 中的指数. 一个看上去显然的结论：\\(|H|\\cdot (G:H)=|G|\\). 这其实也告诉我们所有 \\(|H|\\) 都整除 \\(|G|\\). 对于群内元素 \\(\\sigma\\)，定义 \\(ord(\\sigma)=|\\langle \\sigma\\rangle|\\) 为 \\(\\sigma\\) 的阶. 这等价于说 \\(\\sigma\\) 最少 \\(ord(\\sigma)\\) 次幂得到 \\(1\\). 由上述结论知 \\(ord(\\sigma)\\mid n\\)，并且素数大小的群为循环群. 轨道：\\(Gx=\\{gx\\mid g\\in G\\}\\). 稳定子：\\(Stab_G(x)=\\{g\\in G\\mid gx=x\\}\\). 同样可以定义左右等价对应的商集 \\(G\\backslash X=\\{Gx\\}\\)，\\(X/G=\\{xG\\}\\). 将 \\(X\\) 分解为若干个轨道，称为 \\(X\\) 的轨道分解. 若所有 \\(Stab_G(X)\\) 的交为 \\(1\\)，则称 \\(G\\) 作用忠实. 即不存在 \\(g\\) 稳定所有的 \\(x\\). 如果所有 \\(Stab_G(X)\\) 都为 \\(1\\)，则称 \\(G\\) 作用自由. 即不存在 \\(g\\) 稳定任何的 \\(x\\). 现在考虑轨道和稳定子的关系. 令 \\(H=Stab_G(x)\\)，则有 \\(H\\backslash G\\to Gx\\) 的双射，其中 \\(gH\\to gx\\). 令 \\(X\\) 的所有轨道的稳定子为 \\(H_1,\\dots,H_n\\)，那么每个轨道的大小就是 \\((G:X_i)\\)，于是有 \\(|X|=\\sum_{i=1}^{n} (G:H_i)\\). 定义 \\(X^G=\\{x\\in X\\mid \\forall g\\in G, gx=x\\}\\) 为 \\(X\\) 在 \\(G\\) 作用下的不动点，其对应了只有一个元素的轨道 \\(Gx=\\{x\\}\\). 对于 \\(p\\) 群（\\(|G|=p^m\\) 的群，\\(p\\) 为质数），有 \\(|X|\\equiv |X^G|\\pmod p\\). （对 \\(X\\) 做轨道分解，对于轨道 \\(Gx\\)，若 \\(x\\) 不为不动点，则 \\(|Gx|\\mid |G|=p^m\\) 为 \\(p\\) 倍数. 所以 \\(|X|\\equiv |X^G|\\pmod p\\).）并且若 \\(G\\) 非平凡，则其中心 \\(Z_{G}\\) 也不平凡.（定义群作用 \\(x\\to gxg^{-1}\\)，于是 \\(Z_G\\) 相当于不动点集合，证毕）. Cauchy：对于 \\(p\\) 为 \\(|G|\\) 的素因数，则存在 \\(p\\) 阶循环子群. 证明：考虑令 \\(X=\\{(g_1,\\dots,g_p)\\mid \\prod g_i=1\\}\\)，以及循环位移群 \\(Z/pZ\\). 考虑 \\(X^{Z/pZ}\\) 的意思即为 \\(\\{(g,\\dots,g)\\mid g^p=1\\}\\). 另一方面，我有 \\(|X|=|G|^{p-1}\\equiv 0\\pmod p\\) 且 \\(\\{1,1,\\dots\\}\\in X^{Z/pZ}\\)，那么也就是说 \\(|X^{Z/pZ}|=kp&gt;1\\). 所以存在元素 \\(g\\neq 1\\) 使得 \\(g^{p}=1\\)，即证. 正规子群：\\(H\\lhd G\\)，即 \\(\\forall g\\in G\\)，\\(gHg^{-1}=H\\). 若一个群没有非平凡正规子群，则称群 \\(G\\) 为单群. 大小为素数的群必然为单群. 对于群内任两个子群 \\(H,K\\)，令 \\(HK=\\{hk\\}\\). 当然 \\(HK\\) 不一定是子群. 但若是子群，则必有 \\(HK=KH\\)，这是因为 \\(hk\\) 可以和 \\((hk)^{-1}=k^{-1}h^{-1}\\) 一一对应. 并且若 \\(HK=KH\\)，则也必然是子群. 因为对于 \\(h_1,h_2,k_1,k_2\\) 存在 \\(k_1h_2=h&#39;k&#39;\\) 故 \\(h_1k_1h_2k_2=h_1h&#39;k&#39;k_2\\)，封闭. 取逆类似. 但是如果已知 \\(H\\) 为正规子群，则 \\(HK\\) 一定是子群. 考虑 \\(hk=kk^{-1}hk=k(k^{-1}hk)\\in KH\\)，故 \\(HK=KH\\)，故为子群. 中心化子群：\\(Z_{G}(K)=\\{g\\in G\\mid \\forall k\\in K, gk=kg\\}\\). 正规化子群：\\(N_{G}(K)=\\{g\\in G\\mid gK=Kg\\}\\). 对于群同态 \\(f\\)，定义 \\(\\{g\\in G\\mid f(g)=1\\}\\) 为群同态的核. 群同态的核为一个正规子群. 证明：为子群显然，正规性的考虑 \\(f(x)f(g)f(x^{-1})=1\\) 故 \\(xgx^{-1}\\in \\ker f\\). 对于正规子群 \\(N\\lhd G\\). 则 \\(G/N\\) 成群，其中 \\((xN)(yN)=xyN\\). 并且有商映射 \\(q:G\\to G/N\\)，\\(\\ker q=N\\). 然后就是诱导同态（几个同构定理）. 对于 \\(N\\lhd G\\)，\\(N\\subset \\ker f\\)，则 \\(f:G\\to G&#39;\\) 诱导唯一的同态 \\(\\bar f:G/N\\to G&#39;\\)，使得 \\(f=\\bar fq\\). 对于 \\(N\\lhd G\\)，\\(N&#39;\\lhd G&#39;\\) 而 \\(f(N)\\subset N&#39;\\)，则 \\(f\\) 诱导唯一的同态 \\(\\bar f:G/N\\to G&#39;/N&#39;\\) 使得 \\(q&#39;f=\\bar fq\\). 对于 \\(N\\lhd G\\)，带入 \\(N=\\ker f\\) 则有诱导同态 \\(\\bar f:G/\\ker f\\to \\text{im } f\\) 为同构. 半直积：考虑群 \\(H,N\\)，我们希望在 \\(N\\times H\\) 上定义二元运算，并且能将 \\(N,H\\) 嵌入这个更大的群 \\(G\\)，\\(G\\) 中每个元素都是 \\(nh\\)，且 \\(N\\) 为正规子群. 于是 \\((nh)(n&#39;h&#39;)=n(hn&#39;h^{-1})hh&#39;\\)，所以乘法应该定义为 \\((n,h)(n&#39;,h&#39;)\\to (n\\varphi_h(n),hh&#39;)\\)，其中 \\(\\varphi\\) 为 \\(h\\to Aut(N)\\) 的群同态. 写作 \\(G=N\\rtimes_{\\varphi} H\\). 那么有 \\(N\\lhd N\\rtimes_{\\varphi}H\\). 例如，对于二面体群 \\(D_{2n}\\)，转角为 \\(2\\pi/n\\) 的旋转群 \\(\\langle\\sigma\\rangle\\) 和镜射群 \\(\\langle\\tau\\rangle\\)，做半直积得到 \\(\\langle\\sigma\\rangle \\rtimes \\langle\\tau\\rangle=D_{2n}.\\) 导出子群：定义 \\(G_{der}=\\langle aba^{-1}b^{-1}\\rangle\\). 那么有一些简单性质：其为特征子群，且对于到交换群 \\(A\\) 的群同态 \\(f\\) 有 \\(G_{der}\\subset \\ker f\\). 群的交换化：即商映射 \\(G\\to G/G_{der}\\). 容易发现这样得到的商群 \\(G_{ab}\\) 交换. 例如，二面体群 \\(D_{2n}\\) 的导出子群为 \\(\\langle \\sigma^2\\rangle\\)，所以其交换化要么是 \\(Z/2Z\\) 要么是 \\((Z/2Z)^{\\oplus 2}\\). 例如，\\(GL(n,F)_{der}=SL(n,F)\\)，而其交换话对应其行列式. 参考讲义：李文威-代数学讲义","path":"2025/06/06/高代笔记-群/","date":"06-06","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"CVDL-Visual Generative Model","text":"生成模型本质上，是给定一个服从某个分布 \\(p_{data}(x)\\) 的数据集，然后学到一个 \\(p_{model}(x)\\) 来尽量拟合 \\(p_{data}(x)\\)，使得一方面可以学到 \\(x\\) 的 probability，另一方面可以从 \\(p_{model}\\) 中去采样 \\(x\\). 一个天然的训练目标是 MLE，即最大化 \\(\\sum_i \\log p_{model}(x_i)\\)，这样就显示地将 \\(p_{model}\\) 给建模出来. 但是这不意味就可以从中采样了. 而要想要进行 sample 也未必一定需要去能够 estimate density. 后者称为 Implicit density estimation，即可以采样但是不可以去直接计算出 density. 而有 density 这个概念的称为 Explicit density estimation，于是就可以通过优化这个的方式去训练网络. 这分为 tractable density（可以直接 forward 出概率），也有 approximate density（用近似的方式优化概率）. Discriminative Model 本质上学的是，给 Label \\(Y\\) 和 Inputs \\(x\\)，学 \\(P(Y|X)\\) 的条件概率，去给定 \\(X\\)， sample 出 \\(Y\\). 而生成模型，则学的是 \\(P(X)\\) 或 \\(P(X,Y)\\). 这里的 \\(X,Y\\) 的联合分布概率，而非条件概率，并从这样的模型中去 sample 出 \\(X\\) 或者 \\((X,Y)\\). 一个非常简洁的模型就是一个 Pixel 一个 Pixel 去生成，去坐一个 autoregressive 的生成，这个和序列的 generative 没有啥区别. 但是显而易见效果很差. 第一个能看的模型是 Autoencoder. 这是一个 self-supervised model. \\(x\\) 通过 encoder 变成 latent space 上的一个 feature \\(z\\)，然后再通过 decoder 变成 \\(\\hat x\\)，然后以 \\(||x-\\hat x||\\) 作为 Loss. 但是这是不能直接随便选一个 \\(z\\) 然后 decode 的. 这是因为符合要求的 \\(x_{data}\\) 在 \\(x\\) 空间中占无限小（实际上是一个 manifold，流形），于是映过去的 \\(z_{data}\\) 也在 \\(z\\) 空间中占无限小，所以直接从 \\(z\\) 空间随机一个肯定是错的. 我们只有确保 \\(z\\) 是一个可采样的分布才可以去做生成. VAE：一个很简单的解决方法是直接令 \\(p(z)\\) 为高斯分布. 我们只需要操控 encoder 学完之后是标准正太分布即可. 当然这有很多的不合理性，所以也是 VAE 的一个很大局限，但至少高斯分布是一个合理的可采样的分布了，比随便乱采合理不少. 然后 decoder 的 \\(p_{\\theta}(x\\mid z)\\) 可以写成一个预测 \\(\\mu\\) 和 \\(\\Sigma\\) 的网络，相当于预测它的高斯分布.（？） 对于模型 \\(\\theta\\)，\\(p_{\\theta}(x)=\\int p(z)p_{\\theta}(x\\mid z)dz\\)，这是算不出来的. 如果用蒙特卡洛进行随机采样估计，虽说是无偏的，但是噪声太大了. 我们做点变形，\\(p_{\\theta}(x)=\\frac{p_{\\theta}(x,z)}{p_{\\theta(z\\mid x)}}=\\frac{1}{p_{\\theta}(z\\mid x)} p(z)p_{\\theta}(x\\mid z)\\). 所以我们看能不能求出 \\(p_{\\theta}(z\\mid x)\\). 这个本质就是我们的 encoder 的形式. 将 encoder \\(\\phi\\) 也写成一个高斯分布，\\(q_{\\phi}(z\\mid x)\\). 推导出来之后，\\(\\log p_{\\theta}(x^{(i)})\\) 等于 \\(E_z[\\log p_{\\theta}(x^{(i)}\\mid z)]-D_{KL}(q_{\\phi}(z|x^{(i)})||p(z))+D_{KL}(q_{\\phi}||p_{\\theta})\\). 最后一项我们根本不知道，所以丢掉，得到 Lowerbound！然后去最优化这个 Lowerbound 即可. 这个称为 ELBO. 一个问题就是我们 \\(z\\) 是从高斯分布中 sample 出来的，这个 sample 怎么求导呢？Reparameterization trick: sample 的方式等价于 \\(z=\\mu+\\epsilon\\sigma\\)，\\(\\epsilon\\) 为从 \\(N(0,1)\\) 中 sample 看作常数，于是就可导了. 这个 Loss 项的第一项表面上就是某种 L2 Loss，称为 Reconstruction Loss；然后第二个 Loss 项表面上是控制 \\(z\\) 空间的分布为标准正太分布. 如果强制让 Decoder 的 \\(\\sigma=1\\)，那么 VAE 和普通 Autoencoder 的表面上就是多了一个控制分布的 loss. 但是我们发现这两项 loss 是打架的. 因为你不可能在让 \\(z\\) 有 \\(x\\) 的特征的情况下还让对于任意 \\(x\\) 都能给出标准正态分布. 所以 VAE 在这里就是矛盾的，所以就卡在这里了，生成质量就很不好. 不过 VAE 确实可以给出一个 Approximate 的 density estimation，即这个 ELBO. L2 是一个很不好的东西. 因为它会默认带一个 Gaussian 分布. 这些都生成的很不好，显然会有高斯噪声. GAN 则分为 discriminator 和 generator. 训练主要是一个 minimax game.我们希望 minimax \\(\\min_{\\theta g}\\max_{\\theta d}\\) 真图被判别为真图的期望概率 + 假图被判别为假图的期望概率. 对于 discriminator，就是最大化 \\(E_{x\\sim{p_{data}}}[\\log D(x)]+E_{z\\sim p(z)}[\\log(1-D(G(z)))]\\)，而对于 generator，就是最小化 \\(E_{z\\sim p(z)}[\\log(1-D(G(z)))]\\). 但是这个任务 discriminator 更简单，于是很快 generator 就没梯度了. 于是我们修改为增加自己生成真图的概率，即 \\(E_{z\\sim p(z)}[\\log(D(G(z)))]\\)，这样如果是 \\(0\\) 的时候梯度会很大. 非常难以训练. 当时有很多篇论文研究如何训练，形成了 Gan Zoo. Progressive GAN 是逐层训练 GAN，不断增加分辨率. 当分布 Diverse 的时候，GAN 做不到. 所以生成人脸中，分布比较 narrative 的前景就比较好，但是背景就很烂. Mode Collapse: GAN 没有义务去生成足够有丰富性的图，而是可能会'打游击'，来来回回生成一张图，被干了之后再换一张图. 而 VAE 就不会这样. FID: 拿一个另外一个 pretrained 的网络抽生成的图的 feature，然后假定 feature 遵从 gaussian distribution，然后比较和 real data 的 feature 分布. GAN 的 latent space 中有线性的，语义的结构，可以做语义层面的加减法. Diffusion Model 回到对 Density 的 modeling，也是一个 approximately density. Diffusion 的每一次加噪/去噪都是弄一个高斯噪声，和 VAE 有一定相似性. 每步都加高斯，最后一堆高斯分布的叠加几乎就是任意的分布. 没时间讲了. 参考讲义：Lecture 15 - Generative Model","path":"2025/06/04/CVDL-Generative Model/","date":"06-04","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"高代笔记-张量代数","text":"我们对于域 \\(F\\)，可以自然得到其向量空间 \\(A\\)，其支持加法和纯量乘法. 但是如果我们希望 \\(A\\) 能够有一个我们对“代数”的想象，成为一个环，那么我们还应当希望 \\(A\\) 具有一个乘法的结构. 回忆环的乘法的性质，乘法本质上就是一个 \\(A\\times A\\to A\\) 的双线性映射. 用张量的语言描述，这也就等价于 \\(A\\otimes A\\to A\\) 的线性映射. 具有这样的结构的 \\(A\\) 称为 \\(F-\\)代数. 其满足以上性质的子空间&amp;子环称为其子代数. 举例：\\(F[X]\\) 就是 \\(F-\\)代数，\\(M_{n\\times n}(F)\\) 也是 \\(F-\\)代数. 那么我们能不能更加广泛地得到一个对于 \\(F-\\)代数的构造呢？于是我们引出了张量代数. 张量代数：对于 \\(F\\) 的向量空间 \\(V\\)，定义 \\(T(V)=\\bigoplus_{n\\ge 0} V^{\\oplus n}\\). 那么 \\(T(V)\\) 形成了 \\(F-\\)代数结构. 其中，线性映射 \\(V^{a}\\otimes V^{b}\\to V^{a+b}\\) 映 \\((x_1\\otimes \\dots\\otimes x_a)\\otimes(y_1\\otimes \\dots y_ b)\\to x_1\\otimes \\dots \\otimes x_a\\otimes y_a\\otimes\\dots y_b\\). 然后通过这个定义，就可以直接延拓到 \\(T(V)\\) 上的乘法，因为 \\(\\oplus\\) 和 \\(\\otimes\\) 之间的分配律. \\(T(V)\\) 某种程度上也还是一个 \\(F\\) 的向量空间. 假如 \\(V\\) 的基为 \\(v_1,\\dots,v_n\\)，那么 \\(T(V)\\) 的基就应该是对于所有 \\(m\\le n\\) 且 \\(i_1,\\dots,i_m\\) 遍历 \\(I^m\\)，所有的 \\(v_{i_1}\\otimes \\dots \\otimes v_{i_m}\\). 这无非是“多维数组”结构的一个推论. 也就是说，\\(T(V)\\) 可以对应所有 \\(m\\) 维数组的直和. 然后再看诱导同态. 其实这和范畴论中函子的概念是很类似的. 任何线性映射 \\(\\varphi:V\\to W\\) 都可以诱导唯一的 \\(F-\\)代数同态 \\(T(\\varphi):T(V)\\to T(W)\\). 然后看一下商代数. 对于 \\(F-\\)代数 \\(A\\) 的理想 \\(I\\)，\\(I\\) 应当为其子代数，称商环\\(A/I\\) 为 \\(A\\) 对 \\(I\\) 的商代数. 张量代数中比较特殊且重要的研究对象是对称代数与外代数. 在 \\(T(V)\\) 中，定义 \\(I_{sym}\\) 为形如 \\(x\\otimes y-y\\otimes x\\) 的元素生成的理想，\\(I_{\\wedge}\\) 为形如 \\(x\\otimes x\\) 的元素生成的理想. 定义 \\(Sym(V)=T(V)/I_{sym}\\)，\\(\\bigwedge(V)=T(V)/I_{\\wedge}\\). 然后我们规定一下 \\(I_{sym}^m=I_{sym}\\cap V^{\\otimes m}\\)，\\(I_{\\wedge}\\) 同理，那么就可以得到 \\(Sym^m(V)=V^{\\otimes m}/I^m_{sym}\\)，\\(\\bigwedge^m (V)=V^{\\otimes m}/I^{m}_{\\wedge}\\). 从 \\(V^{\\otimes m}\\) 到 \\(Sym^m\\) 和 \\(\\bigwedge^m\\) 的映射是商映射. 记 \\(x_1\\otimes\\dots\\otimes x_m\\) 在其中的像分别为 \\(x_1\\dots x_m\\) 和 \\(x_1\\wedge\\dots\\wedge x_m\\). 于是，作为外代数，乘法分别用 \\(\\cdot\\) 和 \\(\\wedge\\) 表示. 于是对于 \\(x\\in V^{\\otimes a}\\) 和 \\(y\\in V^{\\otimes b}\\)，\\(xy=yx\\) 而 \\(x\\wedge y=(-1)^{ab}y\\wedge x\\). 我们回到张量积的泛性质. 对于对称线性形式 \\(C\\in Mul(V^m;M)\\)，其诱导同构 \\(Hom(Sym^m(V),M)\\)；而交错线性映射 \\(C\\) 则诱导同构 \\(Hom(\\bigwedge^m(V),M)\\). 直观感受一下，\\(Sym^m(V)\\) 就是一个对称的多维数组，\\(\\bigwedge^m(V)\\) 就是一个交错的多重数组，即作用一个 \\(\\sigma\\) 后相当于乘上 \\((-1)^{sgn(\\sigma)}\\). 从基的角度看，\\(\\dim \\bigwedge^m(V)=\\binom{n}{m}\\)，因为所有满足 \\(i\\) 递增的 \\(v_{i_1}\\wedge\\dots\\wedge v_{i_m}\\) 构成了基. 首先其生成是显然的，证明只需要考虑线性无关. 若 \\(\\sum c_{i_1,\\dots,i_m}v_{i_1}\\wedge\\dots\\wedge v_{i_m}=0\\)，则对任意 \\(i_1&lt;\\dots&lt;i_m\\)，令子空间 \\(V&#39;=\\sum Fv_{i_k}\\)，定义 \\(V\\to V&#39;\\) 的线性映射 \\(\\varphi\\) 为 \\(\\varphi(v_{i_k})=v_{i_k}\\) 而其他 \\(\\varphi(v_j)=0\\)，那么 \\(\\varphi\\) 所诱导的 \\(\\bigwedge^m(\\varphi)\\) 会将所有其余的像映成 \\(0\\) 而只有 \\(v_{i_1}\\wedge\\dots\\wedge v_{i_m}\\) 映为本身. 带入上式得到 \\(c_{i_1,\\dots,i_m}=0\\). 这件事情放到对称代数去理解的话，我们会发现这个对称代数完全等同于 \\(F\\) 上的多项式环 \\(F[X]\\)，因为\\(Sym^m(V)\\) 等同于 \\(F[v_1,\\dots,v_m]\\)！同构的各个方面都是显然的. 于是我们接下来将目光着重放在看上去更加独特的外代数上. 外代数的 \\(sgn(\\sigma)\\) 不禁让我们想到行列式. 它的确和行列式有密切的关系. 对于 \\(V\\) 和 \\(\\varphi\\in End(V)\\)，行列式 \\(\\det \\varphi\\) 的定义是对于所有 \\(n\\) 元交错形式 \\(D(\\varphi(x_1),\\dots,\\varphi(x_n))=\\det \\varphi D(x_1,\\dots,x_n)\\). 于是这个定义很轻松地可以转化为外代数的描述：\\(\\bigwedge^n(\\varphi)=(\\det \\varphi)\\bigwedge^n(id)\\)，并且这个式子能成立的原因也可以直接使用 \\(\\dim \\bigwedge^n(V)=1\\) 来证明. 而另一方面，取基 \\(v_i\\) 后用矩阵的写法展开，\\(\\bigwedge^n(\\varphi)(v_1\\wedge\\dots\\wedge v_n)=\\sum a_{i_1,1}\\dots a_{i_n,n} v_{i_1}\\wedge\\dots\\wedge v_{i_n}\\). 由于其交错性，当有 \\(i_{j}=i_{j&#39;}\\) 时 \\(v_{i_1}\\wedge\\dots\\wedge v_{i_n}=0\\)，而对于 \\(i_j\\) 为排列的时候 \\(v_{i_1}\\wedge\\dots\\wedge v_{i_n}=(-1)^{sgn(\\sigma)}v_{1}\\wedge\\dots\\wedge v_{n}\\)，于是就能得到 \\(\\det\\) 的表达式. 参考讲义：李文威-代数学讲义","path":"2025/06/04/高代笔记-张量代数/","date":"06-04","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"高代笔记-张量积","text":"张量积旨在解决这样一个将多重线性映射转化为普通线性映射的问题：对于已确定的向量空间 \\(V_1,\\dots,V_k\\)，我们能否找到一个空间 \\(L _{univ}\\)， 使得可以多重线性地先将 \\(V_1\\times \\dots V_k\\to L_{univ}\\)，然后就可以将任意多重线性映射 \\(M:V_1\\times \\dots \\times V_k\\to L\\)，转化为 \\(L_{univ}\\) 到 \\(L\\) 的线性映射 \\(\\varphi\\). 而对于线性映射，我们就有更多的工具来解决问题. 我们先将目光聚焦在最为基本的双重线性映射问题上. 即对于向量空间 \\(V,W\\)，我们希望能够确定一个空间 \\(L_{univ}\\)，配有双重线性映射 \\(B_{univ}:V\\times W\\to L_{univ}\\)，使得对于双重线性映射 \\(B:V\\times W\\to L\\)，都可以转化为 \\(\\varphi:L_{univ}\\to L\\)，即 \\(B(v,w)=\\varphi(B_{univ}(v,w))\\). 构造 \\(B_{univ}\\) 的方式如下：首先我们先生成一个最泛的向量空间 \\(F^{\\oplus(V\\times W)}\\)，因为里面的所有元素应当可以表示成 \\(\\sum a_{v,w}(v,w)\\). 然后再用商掉一个核的方式来强制让某些值为 \\(0\\)：定义 \\(N\\) 为由所有 \\((v+v&#39;,w)-(v,w)-(v&#39;,w)\\)，\\((v,w+w&#39;)-(v,w)-(v,w&#39;)\\)，\\((tv,w)-t(v,w)\\)，\\((v,tw)-t(v,w)\\) 生成的子空间，则定义 \\(L_{univ}\\) 为 \\(F^{\\oplus(V\\times W)}/N\\)，并配有 \\(B_{univ}(v,w)=(v,w)+N\\). 容易证明 \\(B_{univ}\\) 的双线性性. 然后我们再从 \\(B\\) 所唯一确定的线性映射 \\(\\Phi:F^{\\oplus(V\\times W)}\\to L\\) 诱导得到 \\(\\varphi:L_{univ}\\to L\\). 我们定义二元运算符 \\(V\\otimes W\\) 得到 \\(B _{univ}(V,W)\\)，其中 \\((v,w)\\) 映到 \\(v\\otimes w\\). 对多重线性映射，上述的描述是类似的，于是不再赘述. 我们可以先通过上述定义来推导出关于张量积的一些基本性质，然后再看在取基的情况下，其一个广为人知的“多维数组”的形象. 张量积有如下（直观上显然）的性质： 结合律：\\(A\\otimes B\\otimes C\\) 同构于 \\(A\\otimes (B\\otimes C)\\)，也同构于 \\((A\\otimes B)\\otimes C\\). 单位元：\\(V\\) 同构于 \\(F\\otimes V\\)，也同构于 \\(V\\otimes F\\). 交换律：\\(A\\otimes B\\) 同构于 \\(B\\otimes A\\). 与直和的交换：对于有限的直和分解 \\(V=\\bigoplus V_i\\)，\\((\\bigoplus V_i)\\otimes W\\) 同构于 \\(\\bigoplus (V_i\\otimes W)\\). 下面关于的证明，无非是多次使用了，多重线性映射 \\(V_1\\times \\dots \\times V_k\\to X\\) 可以诱导出 \\(V_1\\otimes\\dots\\otimes V_k\\to X\\) 的线性映射. 结合律：考虑只需要先描述出两个线性映射 \\(\\alpha:A\\otimes (B\\otimes C)\\to A\\otimes B\\otimes C\\) 和 \\(\\beta:A\\otimes B\\otimes C\\to A\\otimes (B\\otimes C)\\) ，然后验证 \\(\\alpha\\beta=\\beta\\alpha=id\\) 即可. 关于 \\(\\beta\\)，由定义知 \\(A\\times B\\times C\\to A\\otimes(B\\otimes C)\\) 三重线性，故直接诱导出 \\(\\beta\\). 而关于 \\(\\alpha\\)，首先先固定 \\(a\\in A\\)，则 \\(Bil_a:B\\times C\\to A\\otimes B\\otimes C\\) 关于 \\(B,C\\) 双线性，则诱导 \\(\\varphi_a:B\\otimes C\\to A\\otimes B\\otimes C\\). 于是得到映射 \\(A\\times (B\\otimes C)\\to A\\otimes B\\otimes C\\)，可以验证其关于所有变元都线性，故该映射是双线性映射，诱导出 \\(\\alpha:A\\otimes (B\\otimes C)\\to A\\otimes B\\otimes C\\). 而 \\(\\alpha\\beta\\) 映所有 \\(a\\otimes b\\otimes c\\to a\\otimes b\\otimes c\\)，而关于 \\(\\beta\\alpha\\)，固定 \\(a\\) 后映 \\(b\\otimes c\\to b\\otimes c\\)，于是映 \\(a\\otimes (b\\otimes c)\\to a\\otimes(b\\otimes c)\\). 故两者都为 \\(id\\). 单位元的验证也是考虑诱导出线性映射. 映 \\(v\\) 为 \\(1\\otimes v\\)，即得到了 \\(V\\to F\\otimes V\\) 的线性映射. 而双线性映射 \\(B(t,v)=tv\\) 则直接诱导 \\(\\varphi:F\\otimes V\\to V\\)，其中 \\(t\\otimes v\\to tv\\). 容易验证两者的合成为 \\(id\\). 交换律则可以定义双线映射 \\(V\\times W\\to W\\otimes V\\)（\\((v,w)\\to w\\otimes v\\)）诱导出 \\(V\\otimes W\\to V\\otimes W\\). 反方向是类似的. 容易验证两者的合成为 \\(id\\). 关于和直和的交换：\\(Bil(\\bigoplus V_i,W;L)\\) 同构于 \\(\\prod Bil(V_i,W;L)\\)，同构于 \\(\\prod Hom(V_i\\otimes W,L)\\)，同构于 \\(Hom(\\bigoplus(V_i\\otimes W),L)\\). 这诱导出同构 \\((\\bigoplus V_i)\\otimes W\\to \\bigoplus (V_i\\otimes W)\\). 直和的性质直接让我们我们得到其与“多维数组”的联系： 对于 \\(V\\) 有基 \\(v_1,\\dots,v_n\\)，即 \\(V=\\bigoplus_i Fv_i\\)；\\(W\\) 有基 \\(w_1,\\dots,w_m\\)，即 \\(W=\\bigoplus_j Fw_j\\). 则 \\(V\\otimes W=\\bigoplus_{i,j}(Fv_i\\otimes Fw_j)\\tilde=\\bigoplus_{i,j} F(v_i\\otimes w_j)\\). 即 \\(V\\otimes W\\) 以 \\(\\{v_i\\otimes w_j\\}\\) 为基，\\(\\dim V\\otimes W=\\dim V\\times \\dim W\\). 那么根据关于 \\(F\\otimes V\\) 的结论，对于 \\(v=\\sum a_iv_i\\)，\\(w=\\sum b_jw_j\\)，我们映 \\((v,w)\\) 为 \\(\\sum_{i,j} a_ib_j(v_i\\otimes w_j)\\). 容易想象所有 \\(V\\times W\\to L\\) 的双线性映射都可以表示成这个 \\(\\dim =nm\\) 的“二维数组”到 \\(L\\) 的线性映射. 对于多重线性映射也是一样的. 下面看一下矩阵的 Kronecker 积. 首先对于 \\(f_i:V_i\\to W_i\\)，唯一存在 \\(\\bigotimes f_i:\\bigotimes V_i\\to \\bigotimes W_i\\). 证明还是直接考虑 \\((v_1,\\dots,v_n)\\to \\bigotimes f(v_i)\\) 的多重线性映射可以直接诱导出上述的 \\(\\bigotimes f_i\\). 考虑 \\(n=2\\) 的情况，即我们有 \\(A:V\\to W\\) 和 \\(B:V&#39;\\to W&#39;\\)，则我们理应可以存在一个 \\(A\\otimes B:(V\\otimes V&#39;)\\to (W\\otimes W&#39;)\\)，而 \\(A\\otimes B\\) 称为矩阵的 Kronecker 积，这个 \\(nm\\times nm\\) 的矩阵应该长成一个分块矩阵，其中第 \\((i,j)\\) 块为 \\(a_{i,j}B\\). 参考讲义：李文威-代数学讲义","path":"2025/05/31/高代笔记-张量积/","date":"05-31","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"CVDL-Transformer","text":"primitive：基本的结构单元（例如 cnn, 全连接, attention） 假如我们现在有输出的句子的当前隐藏状态 \\(s_i\\)，那么对于输入序列的第 \\(t\\) 个时间步，用一个全连接层算出是一个 alignment score \\(e_{i,t}=f(s_i,h_t)\\)，然后做 softmax 后得到 \\(t\\to i\\) 的一个注意力，将所有 \\(h\\) 带权相加得一个综合的 \\(c_{i}\\). 然后将 \\(y_i,c_i,s_i\\) 三个再通过 RNN 得到 \\(y_{i+1}\\). 这巧妙地将 skip link 用到 seq2seq 问题上. 这个算注意力的 FC 不是通过监督学习的，而是因为整个过程是可微的所以直接反向传递得到. 将这个东西化做一个更加抽象的但更加底层的东西. \\(h_t\\) 为存储了数据的 data vector，而我们用 \\(s_i\\) 去找哪些 \\(h_t\\) 应该被注意，称为 query vector，最终得到 \\(c_i\\) 为 output vector. 将这个问题抽象出来，并且做一个化简：有维度为 \\(D_q=D_x\\) 的 query vector \\(q\\)，维度为 \\(N\\times D_X\\) 的 data vector \\(X\\)，计算过程为计算出维度为 \\(N_X\\) 的 alignment score \\(e_i=q\\cdot X_i/\\sqrt{D_X}\\)，然后求得注意力 \\(a=softmax(e)\\)，然后算出 \\(y=\\sum a_iX_i\\). 注意到我们把 \\(f\\) 优化掉了. \\(f\\) 本质可以看作求两个向量的相似程度，那么不妨用更简单的 cos similarity：\\(\\frac{a\\cdot b}{|a||b|}\\). 那么化到 \\(e\\) 上就是 \\(q\\cdot X_i/\\sqrt{D_q}\\). 但是注意到我们这里擅自把 \\(D_Q=D_X\\). 实际上并不是这样. 考虑引入一个 Key Matrix \\(W_K\\)，大小为 \\(D_X\\times D_Q\\)，那么得到 \\(N_X\\times D_Q\\) 的 \\(K=XW_K\\). 于是得到 \\(E=QK^{T}/\\sqrt{D_Q}\\)，其中 \\(E_{i,j}=Q_i\\cdot K_j/\\sqrt{D_Q}\\). 然后做 softmax 过后得到 \\(A\\). 然后我们其实也可能希望输出的维数不一样，设为 \\(D_V\\)，那么还是考虑引入 Value Matrix \\(W_V\\)，大小为 \\(D_X\\times D_V\\)，那么得到 \\(N_X\\times D_V\\) 的 \\(V=XW_V\\). 然后就可以得到输出 \\(Y=AV\\). 这就是 cross-attention 机制，将两个不同的东西融合在了一起. 里面每个 cell 都对 data 有全局的 receptive field，而对 query 只能看到自己. self attention：加入一个 \\(W_Q\\)，并将 \\(X\\) 通过 \\(XW_Q\\) 来获得 \\(Q\\). 并同一一下 \\(D_Q=D_V=D_{out}\\)，然后 \\(D_x=D_{in}\\). 我们发现这个东西输入为 \\(N\\times D_{in}\\)，并输出为 \\(N\\times D_{out}\\). 其本质是新的，对一族 vector 进行了一次加工，其 receptive field 为全局. 考虑 cross-attetion，其对 data vector \\(X\\) 是 permutation invariance 的，而对于 query vector \\(Q\\) 是 permutation equivariance 的. 而对于 self-attention，则对 \\(X\\) 是 permutation invariance 的. 然后从某个角度讲，每个 query 生成 \\(y\\) 和 pointnet 有所相似性：每个东西内部做一个 FC 然后某种方式 pool 一下就好了. 当然这带来了一个问题. self attention 根本不知道序列的位置信息. 解决这个也很简单，搞一个位置函数，将位置 \\(n\\) 通过这个函数得到一个 positional embedding 然后 concat 到 \\(X\\) 上. 但是对于一个预测下一个词的任务，我们无法知道完整序列信息直接运用 self-attention. 于是引入 Masked self-attention Layer（Causal Mask，因果 mask，不能让未来的事情被过去的事情看到）. 我们只需要在 所有 \\(i&lt;j\\) 的地方强制 \\(E_{i,j}=-\\infty\\)，这样注意力就为 \\(0\\). 于是我们就得到了一个 RNN 的替代. 这相比于 RNN 来说，一个优势是 RNN 是 sequential 的，而上述的训练可以并行地训练！ Multiheaded Self Attention Layer：\\(m\\) 个独立的 attention 可以独立运作，形成 \\(m\\) 个 head，输出 \\(m\\) 套 \\(y\\)，然后再合并起来. 令输入维数=最终输出维数= \\(D\\) 维. 令 \\(D_H=D/H\\)，每个 attention 都是 \\(D_{H}\\) 维的，然后 最后将 \\(H\\times N\\times D_{H}\\) 的 \\(Y\\) 叠成 \\(N\\times D\\) 的 size 之后有 output \\(O=YW_{O}\\). Self Attention 本质上就是 四次矩阵乘法. 第一次矩阵乘法是 QKV Projoection，\\([N\\times D]\\times [D\\times 3HD_{H}]\\) 得到 \\(N\\times 3HD_H\\) 然后 split and reshape 之后 \\(Q,K,V\\). 第二次矩阵乘法是做 \\(QK\\) similarity，即计算 \\(QK^{T}\\). 第三次是作用上 \\(V\\). \\([H\\times N\\times N][H\\times N\\times D_H]\\to [H\\times N\\times D_H]\\). 第四次则是最后做 \\([N\\times D][D\\times D]\\to [D\\times D]\\) 的 output projection. 多头注意力在计算上计算量并没有怎么减少，但是表达能力是更强的. 上述的一个问题就是需要存 \\(N\\times N\\) 的大矩阵，memory 无法承受. Flash Attention：将第二三步化作同一步，获得 \\(O(N)\\) 的存储复杂度. 这样就能加大 \\(N\\). 对比三种 process sequence 的方法：RNN 理论上是线性的可以处理比较长的东西，但是并不那么靠谱并且不并行化；而 CNN 要建立大 receptive field layer 要叠太多了；而 Self Attention 并行化并且有全局的 receptive field，但是复杂度很大，很贵. Transformer：完全使用 Attention 搭网络. Transformer Block：\\(x\\) 通过 multiheaded self attention 得到 \\(y\\)，加 residual connection。然后做 Layer Normalization，这使得每个 token 的参数都对应相同的分布. 然后对于每个 vector 单独做 MLP（称为 FFN 层），然后加 residual connection，加 layer norm. 不同 token 间交流的时间只有 self-attention，而剩下的 layernorm 和 FFN 都是每个 token 单独完成的. 上面的计算只有来自 self-attention 的 4 个 MatMul，然后 FNN 有两个 MatMul. 很并行化. online / offline processing：前者的任务需要 causal mask 而后者就不需要了. 训练和推理的时候 causal mask 必须同时用/不用. 处理语言任务的时候要学习一个 Embedding matrix（将词映成 vector） 和 Projection matrix（将 vector 映成词）. ViT：将图片切成若干个大小为 \\(16\\times 16\\times 3\\) 的 patches，然后每个 patch 提一个特征，然后 256 个 token 喂给 transformer. 对于提特征，由于图很小，所以直接 MLP 也没啥. 跑完之后做一个 global average pooling 然后过 linear layer 做 classification. 需要 positional embedding 以及不需要 causal mask. 一些改进：Pre-Norm. 在 residual link 之后做 norm 还是不是很好学到 identity function. 所以考虑如下的形式：把 layer norm 放到 self attention / MLP 之前，被 residual link 包裹. 这样训练更加稳定. RMS Norm（Root Mean Square）：要学一个 \\(\\gamma\\)，然后令 RMS 为 \\(\\sqrt{\\frac{1}{n}\\sum x_i^2+\\epsilon}\\)，然后 \\(y_i=\\frac{x_i}{RMS}\\gamma\\). 这会使得最终的 RMS 为 \\(\\gamma\\). 还有一个对 FFN 做优化的 SwiGLU. MoE（混合专家模型）. 考虑原本是一个 \\(W_1=D\\times 4D\\)，\\(W_2=4D\\times D\\) 的矩阵，然后现在我们扩展成有 \\(E\\) 个 \\(W_1\\) 和 \\(E\\) 和 \\(W_2\\). 每个 MLP 被称为 Expert. 然后每个 token 会被 route 到 \\(A\\) 个 Expert（\\(A&lt;E\\)）. 那么当参数量 \\(E\\) 增加了并不会增加计算量，而只有 \\(A\\) 会控制 Foward 的计算量. Routing 的学习挺困难的，因为是否选择由 Top-A 决定，这是不可导的. 参考讲义：Lecture 14 - Transformer","path":"2025/05/28/CVDL-Transformer/","date":"05-28","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"CVDL-Object Detection","text":"axis aligned bounding box：要求和坐标轴平行. 虽说这样其实肯定不是最 tight 的 bounding box. 但是在 2D 中歪着有点奇怪，不是很 necessary. 一个 Bounding Box（bbox） 可以用 \\((x,y,h,w)\\) 四个自由参数来描述. 如果图像里面只有单一物体，做 object detection 直接先 CNN 抽特征，然后分两个 branch 出来，一个头做 classification 得到类别，一个头做 regression 得到 \\(x,y,h,w\\). 后者也需要做适当的 activation 来确保在范围内，比如 sigmoid 后乘上对应最值. classification 使用 softmax 的 CE loss，而 regression 使用 L2 loss，然后加权得到总 loss. 注：L2-loss 是 \\(\\sum \\Delta^2\\)，叫 L2-loss；而 L2-norm 是 \\(\\sqrt{\\frac{1}{n}\\sum \\Delta^2}\\)，叫 Rooted mean squared loss L2 的好处：gradient 和 loss 成正比，收敛更快. 当然不是很鲁棒. 而 L1 更鲁棒但是在 \\(0\\) 处不可导，然后导数也是不变的. L1 一开始导数不至于太大，但是在 \\(0\\) 处就不行了. RMSE 在 \\(0\\) 处也不可导. Smooth L1 Loss：\\(|x|&lt;1\\) 时用 \\(0.5\\times x^2\\) 的 L2，\\(|x|\\ge 1\\) 时用 \\(|x|-0.5\\). 好处是在 \\(x=1\\) 导数也是连续的. Region Proposals：找到可能有物体的区域（Regions of Interest，RoI），然后在这些区域中做 single detection. 古早时候使用 selective search. R-CNN：先 search 出来 RoI，然后通过插值 wrap 成 \\(224\\times 224\\) 的图，然后再过预训练过的 conv net. 这样一个问题是丢掉了全局的上下文信息. 比如如果 bbox 比 RoI 更大那么就不好修了. 而且很慢. Fast R-CNN：不去 crop image，而是对于一个 proposal，去 crop feature map. 具体而言，先用 backbone 提取特征，然后对于一个原图上做出的 proposal，先等比算出在 feature map 上的位置，并吸附到 feature map 的格点上. 然后我们要将这样一个大小任意的 \\(h\\times w\\times c\\) 的 tensor，转化成一个大小固定的 tensor. 这个方法很粗暴，直接 roughly divide 然后 max pooling 成 \\(2\\times 2\\times c\\). 这个过程称为 RoI Pool. 由于最后 crop 出来的 tensor 会不大，所以可以塞到一个 batch 里面，会快很多很多. 现在速度瓶颈成了 region proposal. Faster R-CNN：新增一个用来用 feature map，并用一个 network 做 regional proposal 的 RPN. 由于 feature map 的 resolution 低，而且 feature map 有充分的周遭信息，所以 sliding window 的 aspect ratio 和 scale 的可能性也不需要那么多了. 每个 cell 作为中心，去生成 \\(K\\) 个不同 ratio 和 scale 的 Anchor Box，然后预测每个 Anchor Box 是否有物体，以及做一个 regression 去预测 bbox 的位置的 correction（若第一个位置的 ground truth 是否否则 box correction 的 loss 不计算）. 每个预测出来的 bbox，去找到所有 ground truth 中 IoU 最大的框作为自己的 ground truth 然后再通过 IoU 判断是否有物体. 一共 \\(hwK\\) 个 Anchor Box. 在 test time，选取置信度最高（是否有物体的输出，objectness）的若干个 box 进入到最后的 RoI Pool 和计算. 训练很复杂，有四个 loss：RPN 的 objectness loss，RPN 的 regression loss；以及最终网络的 classification loss，和 regression loss. 训练的时候先训 RPN，然后 freeze 住前面的网络，再训后面的网络. 然后再 backward 到前面，继续先训 RPN，再训后面的，循环往复. 还有很多细节：比如没物体的 anchor box 肯定会更多，所以正负样本不平衡，需要调整 objectness loss 的正负权重比例（一般是 \\(1:3\\)）. 以及 anchor box 怎么选，也是很重要的超参. 这是一个很工程的东西，需要很多的理解. FPS: Frame per second，一秒能处理几张图. 注意到在点云中也有一个叫 FPS 的 Furthest Point Sampling. YOLO：210bpm16分直交互接交互楼梯. 很快，可以做到 100 FPS. 比大多相机更快！ 把 Faster R-CNN 的两个 network 给捏一起. 将图片直接划分成 \\(7\\times 7\\) 的格，每个格子预测五个五元组表示的 bbox \\((x,y,h,w,c)\\)，\\(c\\) 为 置信度. bbox 的 NMS：以 classification confidence 作为置信度. 然后设置一个 IoU threshold \\(\\tau\\). 在 NMS 之前先丢掉一些置信度 \\(&lt;x\\) 的. 然后先选置信度最高的 bbox 加入 result，然后把和自己预测的同类的且 IoU 大于 \\(\\tau\\) 的丢掉. 然后不断这么做. 由于 NMS 的基于比较的选择不是可导的所以不能端到端. 端到端的工程量小很多，但是需要多很多的数据和算力. Average Precision：将 Category 分开处理，每个 category 将所有检测结果按置信度从高到低排序. 然后对于每个前缀，计算其 precision 和 recall. 这样能在平面上画出一个曲线 \\(P\\)，\\(P(x)\\) 表示 recall=x 的 precision. precision 的计算方法要先设定和 ground truth 的 IoU threshold，然后 precision 就看大于这个 threshold 的比率. 然后最后的 AP 比如取 \\(\\frac{1}{11}\\sum P(\\frac{i}{10})\\). 这个和 IoU threshold 有很强的关系. mAP：不同 category 不同 IoU 的 AP 平均值. 实例分割 Top-down：先 object detection 再做 binary segmentation. Bottom-up：先做 grouping 生成 mask 再做 classsification. 在 2D 中 Top-down 表现更好. Mask R-CNN：提出了 RoI Allign. 原本 RoI Pooling 一方面分辨率太低了，这增加分辨率就好了. 但很重要的是吸附操作有 information loss. 这也导致了实际上 bbox 也有问题. 于是直接不吸附，然后对于每个按均匀划出的格点，做双线性插值即可，然后最后再做一个 pooling. 这让每个 bbox 更加精细了. ROI Align 主要提升了精度，所以 AP75 提升很多，AP50 提升相对少一些. 还有一个 implementation 的东西就是每个位置的mask的值不是binary，而是 class， 然后再用 classification 的 result 去选. 参考讲义：Lecture 13 - Detection and Instance Segmentation.","path":"2025/05/21/CVDL-Object Detection/","date":"05-21","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"高代笔记-矩阵的标准型","text":"模论的笔记中，有关结构定理的形式化的表达可能能让我们产生一个关于线性映射的零化多项式的联想：对于每个子空间和线性映射在这个子空间的作用，取一个零化多项式，那么所有零化多项式的 LCM 就应该是线性映射的最小多项式. 将最小多项式的倍数形成的主理想商掉之后，就能划分出一个不能被更小的多项式零化的子空间，然后对剩下的子空间继续这么做... 接下来严谨地探讨这件事情. \\(F\\) 模就是 \\(F-\\)向量空间. 下面有一个很神秘的想象，就是去把 \\(F\\) 模升级成 \\(F[X]\\) 模 \\(M\\). 所谓升级，就是仅仅拓展纯量乘法的定义. 下面给出一个升级方式：先指定一个线性映射 \\(T\\). 对于 \\(v\\in M\\)，定义 \\(X\\cdot v=Tv\\). 于是对于多项式 \\(f(X)\\)，\\(f(X)\\cdot v\\) 即将 \\(f(T)\\) 作用于 \\(v\\). 这种定义首先是符合模的要求的. 其次确实所有 \\(Ann(v)\\) 的交恰好对应了 \\(T\\) 的零化多项式. 首先看 \\(T\\) 和 \\(M\\) 的对应关系. 在同构等价类下，\\(n\\) 维线性映射 \\(T\\) 和 \\(n\\) 维 \\(F[X]-\\)模形成了双射. 映射方式即前面所述. 单性显然，满性也显然（因为 \\(F[X]\\times V\\to V\\) 的映射要是线性的）. 首先我们注意到 \\(\\deg \\le n\\) 的最小多项式的存在性，所以不会存在自由子模. 那么根据前面的结构定理，应该唯一存在一列 \\(f_1\\mid f_2\\mid \\dots \\mid f_k\\)，使得 \\(M=(F[X]/f_1)\\oplus\\dots\\oplus (F[X]/f_k)\\). 模的直和分解应该直接对应于 \\(T\\) 的分块对角. 那么我们看子模 \\(F[X]/f_i\\) 再对应回矩阵形式是什么样的，即找到 \\(rank=\\deg f_i\\) 的 \\(T\\) 使得 \\(f_i(T)=0\\). 显然取 \\(f_i\\) 的友矩阵 \\(C_{f_i}\\). 即是我们想要的答案. 于是我们就得到了矩阵的有理标准型： 对于任意矩阵 \\(A\\in M_{n\\times n}(F)\\)，存在一列 \\(f_1\\mid \\dots\\mid f_k\\)，使得 \\(\\sum \\deg f_i=n\\)，而矩阵同构于 \\(\\operatorname{diag}(C_{f_1},\\dots,C_{f_k})\\). \\(f_1,\\dots,f_k\\) 称为 \\(A\\) 的不变因子，而 \\(Min_A=f_k\\)，\\(Char_A=\\prod f_i\\). 如何求解有理标准型？本质就是求解不变因子. 那么实际上我们就是要求 \\(F[X]\\) 商掉的那些理想. 选定一组 \\(M\\) 的基 \\(v_i\\)，以及 \\(F[X]^{\\oplus n}\\) 的标准基 \\(e_i\\)，我们探讨 \\(F[X]^{\\oplus n}\\to M\\) 的线性映射 \\(\\varphi\\)，根据上面的结论肯定是 \\(\\varphi(\\sum g_ie_i)=\\sum g_i(T) v_i\\). 令 \\(x_i=Xe_i-\\sum_{j=1}^{n}a_{i,j}e_j\\)，则 \\(\\langle x_1,\\dots,x_n\\rangle\\) 生成的子模 \\(N\\) 即为 \\(\\ker \\varphi\\). 验证是容易的：\\(\\varphi(x_i)=T v_i-\\sum_{j=1}^{n} a_{i,j}v_i=0\\). 通过这个核 \\(N\\)，我们知道了 \\(M\\) 通过 \\(\\varphi\\) 诱导的同态 \\(\\overline\\varphi\\)，同构于 \\(F[X]^{\\oplus n}/N\\). 下面关注子模 \\(N\\). 我们已经求得子模 \\(N\\) 的矩阵表法 \\(\\langle x_1,\\dots,x_n\\rangle=XI-T\\)，那么下面只需要套用 Smith 标准型的理论，即可知存在可逆 \\(P,Q\\) 使得 \\(N=PDQ\\)，其中 \\(D=diag(d_1,\\dots,d_k)\\)，\\(d_1|d_2|\\dots |d_k\\). 于是 \\(F[X]^{\\oplus n}/N\\) 同构于 \\(F[X]^{\\oplus n}/D\\) 同构于 \\(\\bigoplus_{i=1}^{n} F[X]/d_i\\). 取所有 \\(d_i\\neq 1\\) 的 \\(d\\) 即得到所有的不变因子. 于是步骤就是：对 \\(XI-T\\) 求 Smith 标准型，然后提出所有 \\(\\neq 1\\) 的项即得到不变因子，然后再得到友矩阵组成的分块对角矩阵. 下面关注 Jordan 标准型. Jordan 标准型的出发点是幂零矩阵. 幂零矩阵的特点在于特征多项式是 \\(X^n\\)，且存在 \\(k\\) 使得最小多项式为 \\(X^k\\). 并且一个重要特征是 \\(A\\) 幂零等价于 \\(V=V_{[0]}\\). 定义 Jordan 块 \\(J_d(\\lambda)\\) 为满足对角线上为 \\(\\lambda\\)，而次对角线为 \\(1\\) 的上三角矩阵. 同理可以定义下 Jordan 块为其专制. 容易发现下 \\(J_d(0)\\) 即 \\(X^d\\) 的友矩阵，而下 Jordan 块显然相似于上 Jordan 块，所以幂零矩阵同构于若干个 \\(J_{d_i}(\\lambda)\\) 的分块对角. 然后再看到普通的矩阵. 对于矩阵 \\(A\\)，若其特征多项式分裂，则有广义特征子空间分解 \\(V=\\bigoplus_i V_{[\\lambda _i]}\\)，于是考虑对于每个广义特征子空间，其减去 \\(\\lambda_i\\) 后显然应该满足幂零. 而每个幂零矩阵可以写成若干个 Jordan 块的形式，所以 \\(A\\) 也可以写成 Jordan 块的分块对角. 注意到这是一个严格比矩阵的上三角化更加强的结论. 这可以引出 Jordan-Chevalley 分解. 对于特征多项式分裂的 \\(A\\)，存在唯一一对 \\(S,N\\) 使得 \\(S\\) 可对角化，\\(N\\) 幂零，而 \\(T=S+N\\) 且 \\(SN=NS\\). 存在性由 Jordan 标准型直接得到：同构于若干个 Jordan 块后，每个 Jordan 就可以直接拆分成对角矩阵和幂零矩阵. 唯一性考虑对于每个 \\(S\\) 的特征值 \\(\\lambda_i\\) 和特征子空间 \\(V_i\\)，由于 \\(N-\\lambda_i\\) 幂零，所以 \\(V_i\\subset V_{[\\lambda_i]}\\). 而由于 \\(V_i\\) 的直和为整个空间，所以 \\(V_i=V_{[\\lambda_i]}\\)，故 \\(\\lambda_i\\) 也为 \\(T\\) 的特征值，进而得到唯一性. 我们来看如何求解 Jordan 标准型. 这里直接给出结论——相关证明和幂零性直接相关并且可能不是很困难，但我不想证了. 以下论述对于幂零矩阵成立. Jordan 块的总数为 \\(n-rk(T)\\). \\(d\\times d\\) 的 Jordan 块的数量为 \\(rk(T^{d+1})-2rk(T^d)+rk(T^{d-1})\\). 对于非幂零矩阵，求解特征多项式之后对于每个特征子空间求解然后分块对角即可. 参考讲义：李文威-代数学讲义","path":"2025/05/19/高代笔记-矩阵的标准型/","date":"05-19","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"音数大作业-定音鼓的发声机理","text":"注：音数大作业为小组完成的项目，此处仅将个人所负责的部分（定音鼓有稳定持续的音高的原因）记载于此. 该课程论文的文件详见文末的附件. 定音鼓发声机理 同心模态和径向模态 如上所述，鼓的振动模态拆分为径向模态（Diametric Modes）和同心模态（Concentric Modes）的组合，即由 \\(n,k\\) 所决定。下面分别研究同心模态和镜像模态。 首先是同心模态。一个简单但又重要的事实是，鼓手在敲击定音鼓的时候并非在中心敲击，而是在位于距离边缘到中心的约 1/4 的地方敲击，所以同心模态的影响较小。但是同心模态就算因为错误的击打而产生了较大的初始能量，之后也会快速衰减，不会对固定音高产生影响。 在 Thomas D Rossing 的 Science Of Percussion Instruments[1]（Chapter 2 Drums with Definite Pitch，2.3 Timpani）中，记载了有关不同模态在击打后 \\(0.03s\\) 以及 \\(1s\\) 不同振动模态的振幅。我们观察到，正常击打的时候，同心模态振动的能量很小，几乎可以忽略。而若在正中央击打，则同心模态在击打后 \\(0.03s\\) 能量较大，但是 \\(1s\\) 后能量快速衰减，得到的结果与正常击打时的结果更加相似。而相比之下，径向模态（\\(n=1,k\\ge 1\\) 的模态）的能量衰减明显少于同心模态。 然后我们关注一个未被讨论的特例，即 \\(n=1\\)，\\(k=0\\) 的振动模态。这个振动模态中，整个鼓皮一起上下振动。这个模态和同心模态类似，也因为鼓手敲击位置以及空气负载的减震，导致了衰减速度变快，并不会对定音鼓的固定音高产生影响。不论是人耳听到的声音，还是通过频率分析得到的结果，这个振动模态几乎都会被忽略。 通过这个分析，我们知道了对于定音鼓来说，真正影响其固定音高的是径向模态，即 \\((1,1)\\)，\\((2,1)\\)，\\(\\dots\\)，\\((k,1)\\)。 空气负载 定音鼓相比于普通的鼓，其最大特点是有固定音高。而使得定音鼓有固定音高的主要物理现象是空气负载（Air Loading）。 由于定音鼓的膜的面积较大，所以在鼓手敲击定音鼓的时候，鼓内外的空气会对鼓皮的震动产生不可忽略的影响。 一方面，当膜产生振动的时候，膜表面的运动使得相邻空间的空气产生了加速度。而由于惯性的影响，这个加速度会阻碍膜的振动。换句话说，振动时的发生变化的物体的质量不仅要考虑膜，还要考虑膜附近的空气，于是该振动系统可以理解成一个增加了空气质量的复合系统。由于系统的质量增加，所以所以空气负荷使得振动频率下降。这个振动的影响对于不同模态是不一样的，对 \\((1,1)\\) 和 \\((2,1)\\) 的影响尤为强烈。 与前面得到的理想情况不同的是，空气负载的影响依赖于膜的各种参数。这就是定音鼓与其他鼓的一大区别：定义鼓的膜的参数使得在算上空气负载后，径向模态的频率比接近于 \\(1:1.5:2:\\dots\\)。 【Without Kettle 的仿真实验报告】 这个数据距离真实的 \\(1:1.5:2\\) 仍有差距，因为我们忽略了定音鼓的 Kettle 的影响。但是已经足够好，没有鼓壳的定音鼓在历史上也被申请过专利[2]。 有了 Kettle 后，鼓内封闭的内部空气体积的振动模式的频率高于与其耦合的膜模式的频率[3]，这对膜的振动产生影响。经实验得到，\\(n=0\\) 的模态频率上升，而其余模态频率下降。 【With Kettle 的仿真实验报告报告】 经过这样的变化后，径向模态的频率达到了 \\(1:1.51:2.01\\)，更加接近 \\(1:1.5:2\\). 另一方面，空气分子的运动产生速度梯度，空气的黏性导致振动的能量转化为热能，导致振动系统的能量快速衰减。对于频率越高的模态，空气黏性导致的衰减越显著。相关实验表明 \\((1,1)\\)，\\((2,1)\\)，\\((3,1)\\) 在 \\(1s\\) 之后变化不大，但是 \\((4,1)\\) 和 \\((5,1)\\) 则在 \\(1s\\) 后快速衰减[4]。所以前三个径向模态的和谐足以产生一个固定音高。 值得注意的一点是，理论上存在 Missing Fundamental 的现象，即在听到其泛音列时，人耳会自动听到其 Fundamental，即弥补出 Fundamental 的缺失。而由于 \\(f_{1,1}:f_{2,1}:f_{3,1}=1:1.5:2\\)（后续将展示为何是这样的比例），所以根据这个理论，人耳将听出比 \\(f_{1,1}\\) 低八度的音。但是实际上，更加高频的模态衰减速度过快且能量较小，所以不支持产生这样的现象。 最后所以最终定音鼓产生的固定音高即为 \\((1,1)\\) 振动模态所对应的频率。 [1] Rossing, Thomas D. \"Science of Percussion Instruments\", 2000, ISBN 981-02-4158-5 [2] Montagu, Jeremy. Timpani and Percussion. Yale University Press. May 2002 [3] Rossing, Thomas D. “The physics of kettledrums.” Scientific American 247/16 (November 1982): 172-178. [4] Acoustic Analysis of Timpani: Specific Mode by Striking Point，Yuya Nishimura &amp; Sohei Nishimura，https://www.ijeert.ijrsset.org/pdf/v3-i10/7.pdf 有/无固定音高乐器本质区别 定音鼓和其他鼓的对比 我们下载了一段定音鼓的声音[5]，并对其中一个衰减过后的音进行频率分析，得到了如下的结果： 最高的峰波为 \\(-10dB\\)，而 \\(&gt;-30dB\\) 的波峰在 148Hz（D3），220Hz（A3），290Hz（D4），比例为 \\(1:1.48:1.95\\)，而其余的波峰都显著低于这三个波峰。考虑到录音等造成的误差，这是一个相当令人满意且符合前面得到的结论的结果。 我们也下载了一段军鼓的声音[6]，并对其中一个衰减过后的音进行频率分析，得到了如下的结果： 最高的峰波为 \\(-20Hz\\)，而其中 \\(\\ge -36dB\\) 的峰波就有：231Hz（A#3），345Hz（F4），439Hz（A4），512Hz（C5），1018Hz（C6）。许多模态的能量仍然较高，无法形成一个和谐的泛音列。 我们还下载了一段底鼓的声音[7]，并对其中一个衰减过后的音进行频率分析，得到了如下的结果： 容易发现能量较大的几个峰波在 35Hz，80Hz，140Hz。剩下的模态衰减后能量确实很小，但是 \\(1:2.3:4\\) 的比例也无法形成和谐的泛音列。 对比以上三个结果，这一方面证实了上面所说，定音鼓能够产生固定音高，一方面得益于非径向模态的快速衰减，一方面得益于空气负载产生的频率改变。 另一方面，能量较高的模态是否产生比例和谐的泛音列则是能否产生固定音高的关键。 有/无固定音高的乐器的区别 从上面，我们可以发现有音高的乐器的最重要特点是其泛音列频率为某个基频的整数倍。注意这里基频不一定真实在频谱上可被观测到（例如定音鼓，以及 Missing Fundamental 现象）。 这里本质上是因为将所有振动模态叠加之后，由于泛音列的频率为基频的整数倍，所以其振动大致上满足了一种以基频为周期的周期性。这种周期性让我们能够感知到其音高。 而对于无音高乐器，例如军鼓，其振动模态非常多，较为混乱，无法形成一个固定周期；例如底鼓，其 \\(1:2.3\\) 的频率比并非整数倍数关系，人耳也就无法形成一个可感知的基频。 综上所述，音高本质上是人耳对形成固定周期的基频的感知，而乐器是否有固定音高本质上是其所有振动模态叠加起来是否能够产生一个固定的周期。 [5] What does the timpani sound like? (Scale) @UtahSymphonyUtahOpera https://www.youtube.com/watch?v=3tx_Oi86esk [6] Snare Drum Samples @TheDrumwerks https://www.youtube.com/watch?v=ljDcWTFpVME [7] KICK DRUM SOUND EFFECT [HD] @nBeatsofficial https://www.youtube.com/watch?v=qAn1zfzrvkk 课程论文：链接，由小组共同完成.","path":"2025/05/17/音数大作业-定音鼓的发声机理/","date":"05-17","excerpt":"","tags":[{"name":"音数","slug":"音数","permalink":"http://lgswdn.github.io/tags/%E9%9F%B3%E6%95%B0/"}]},{"title":"高代笔记-简单模论","text":"模是这样的一个东西：对于一个环 \\(R\\)，那么令 \\(R-\\)模 \\(M\\) 为由以下两个资料确定的代数结构： 一个加法群 \\((M,+)\\). 一个 \\(R\\) 对于群 \\(G\\) 内元素的纯量乘法. 即对于 \\(r\\in R\\)，\\(x\\in M\\)，能够定义出 \\(r\\cdot x\\in M\\). 这个乘法需要满足你对这样的乘法和加法的结构的一切想象. 定义了 \\(r\\cdot x\\) 的称为左 \\(R\\) 模，而也可以定义 \\(x\\cdot r\\)，称为右模. 下面默认为左模. 如果我们令 \\(R\\) 为域，那么实际上 \\(F -\\)模 可以发现就是一个 \\(F-\\)向量空间. 所以在很多情况下直观地将 \\(M\\) 看作某个向量去看也是可以理解的. 和向量空间和群的情景类似，我们可以定义如下东西： 子模：对于模 \\(M\\)，其子模是其一个子群，并且满足对纯量乘法封闭. \\(\\sum_{i\\in I} M_i=\\{\\sum_{i\\in I} m_i \\mid m_i\\in M_i\\}\\)，其中求和为有限和. 生成子模：对于子集 \\(S\\subset M\\)，定义其生成子模为 \\(\\langle S\\rangle=\\sum_{s\\in S} r_ss\\)，\\(r_s\\in R\\). 显然 \\(\\langle S\\rangle\\) 为包含 \\(S\\) 的最小子模. 循环模：将 \\(x\\) 元素的生成子模写作 \\(Rx\\). 将所有能表示成 \\(Rx\\) 的模称为循环模. 同样，同态，同构，核，商这种也和向量空间类似. 同态：\\(M,M&#39;\\) 都为 \\(R\\) 模，则定义 \\(M\\to M&#39;\\) 的同态为保加法和纯量乘法的映射. 记所有同态形成的集合为 \\(Hom(M,M&#39;)\\). 显然 \\(Hom(M,M&#39;)\\) 可以被升级为 \\(R\\) 模.（此处升级的意思为对于 \\(R\\) 定义一个纯量乘法）. 具体而言，对于 \\(f:x\\to f(x)\\)，纯量乘法只需要变成 \\((r\\cdot f)=x\\to r\\cdot f(x)\\). 商模：对于 \\(N\\) 为 \\(M\\) 的子模，则可以在商群 \\(M/N\\) 上定义纯量乘法 \\(r\\cdot (x+N)=rx + N\\). 由于子模对乘法封闭所以是良定义的. 商映射 \\(q:M\\to M/N\\) 为模同且 \\(\\ker(q)=N\\). 可以定义 \\(coker(f)=M&#39;/im(f)\\). 于是 \\(f\\) 满当且仅当 \\(coker(f)=\\{0\\}\\). 群论中的诱导同态相关的东西也可以搬到模论上. 权当对诱导同态相关概念的复习. 对于同态 \\(f\\)，子模 \\(N\\subset \\ker f\\)，则 \\(f\\) 诱导同态 \\(\\overline f:M/N\\to M&#39;\\) 满足 \\(f=\\overline fq\\). 其中 \\(\\overline f(x+N)=f(x)\\). 取 \\(N=\\ker f\\) 即给出了 \\(M/\\ker f \\to \\text{im }f\\) 的同构. 对于同态 \\(f\\)，子模 \\(N,N&#39;\\) 满足 \\(f(N)\\subset N&#39;\\)，则 \\(f\\) 诱导同态 \\(\\overline f:M/N\\to M&#39;/N&#39;\\) 满足 \\(q&#39;f=\\overline f q\\). 其中 \\(\\overline f(x+N)=f(x)+N&#39;\\). 以上的内容都是较为基础且不涉及太多关键内容的东西. 下面来看直积与直和. 这个定义和向量空间的情况没有区别. 直和分解：对于模 \\(M\\) 的一串子模 \\((M_{i})_{i\\in I}\\)，若这些子模两两无交，则称这串子模为 \\(M\\) 的直和分解. 与向量空间类似，直和分解满足每个 \\(x\\in M\\) 都可以唯一写成 \\(x=\\sum x_i\\)，\\(x_i\\in M_i\\)；并且考虑外直和 \\(\\oplus_i M_i\\) 应该与 \\(M\\) 同构. 这衍生出一个重要的概念，自由模. 对于集合 \\(X\\)，令 \\(X\\) 上的自由模为 \\(R^{\\oplus X}=\\oplus_{x\\in X} Rx\\). 每个属于该自由模的 \\(m\\) 都有唯一的表法 \\(m=\\sum_{x\\in X} r_x x\\). 我们称 \\(X\\) 为该自由模的基. 挠元. 若 \\(x\\in M\\) 满足 \\(rx=0\\) 的充分必要条件是 \\(r=0\\)，则称 \\(M\\) 无挠. 否则称 \\(x\\) 为一个挠元. 自由模没有非零挠元. 因为如果有非零挠元的话，那 \\(x\\) 的表法就不唯一了. 虽说没有非零挠元不代表一定是自由模！ 一个例子，取 \\(R=\\mathbb Z\\) 且 \\(M\\) 为 \\(\\mathbb Z\\) 向量空间，则显然 \\(R\\) 为自由模，因为取 \\(x_i=(0,\\dots,1,\\dots,0)\\)，其中第 \\(i\\) 个位置为 \\(1\\) 即可. 又比如，取 \\(R=\\mathbb Z\\) 且 \\(M\\) 为 \\(\\mathbb Z/3\\mathbb Z\\) 向量空间，则 \\(R\\) 并非自由模. 对于所有元素，取 \\(r=3\\) 即可发现其为挠元. 于是可以发现所有 \\(m\\) 的表法都是不唯一的. 我们把视线放到主理想整环上. 域上的向量空间显然是自由的，主理想整环的区别是可能会产生挠元 \\(x\\)，并且能零化 \\(x\\) 的元素构成了一个理想 \\(Ann(x)\\). 那我们就可以想，能不能将这个模拆成两个部分，一个部分是自由的，而另一部分全部是挠元. 那么对于另一个部分，可以找到所有理想的交（即 \\(lcm\\)），这个交显然零化所有的理想. 然后把这个交给商掉之后，应该还能继续区分出自由的部分和挠元的部分. 不断这么做，就引出了下面的一个很重要的定理： 主理想整环上有限生成模的分类定理 / 结构定理： 对于 PID \\(R\\) 和 \\(R\\) 模 \\(M\\)，唯一存在 \\(M\\) 的分解使得 \\(M=(R/I_1)\\oplus\\dots\\oplus(R/I_k)\\oplus E\\)， 其中 \\(E\\) 是自由模，且 \\(I_1\\supset I_2\\supset \\dots I_k\\). 这串真理想链 \\(I_1\\supset I_2 \\supset ...\\supset I_k\\supset \\{0\\}= ...= \\{0\\}\\)（\\(rk E\\) 个 \\(\\{0\\}\\)）称为 \\(M\\) 的不变因子. 证明较为困难，但是比较通俗不严谨的核心逻辑就是前面的那段话. 这个能给我们带来一个很重要的好东西，就是矩阵的标准型. 参考讲义：李文威-代数学讲义","path":"2025/05/17/高代笔记-简单模论/","date":"05-17","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"CVDL-LSTM&Attention","text":"Sequential Data 可以有两个作用. 一个是 Fitting，另一个是 Generation. 大语言模型中是基于每个 word 的. Truncation：训练的时候限制 Sequence 的长度，否则 backward 太耗时了. 每次训练使用一个长度为 \\(t\\) 的 Chunk of sequence，来形成一个 data point，进行 \\(t\\) 次 backward. 但是一个 chunk 的 \\(h_0\\) 怎么来呢？一个方法是 forward 的时候 forward 一整个 sequence，backward 的时候在 chunk 里面 backward，好处是 \\(h_0\\) 可以记录上文 context 的信息. 但是这个问题就是每个 Iteration 都要 forward 一遍，太耗时间了. 所以另一个方法就是直接 \\(h_0=0\\). 使用 Chunk 的代价是根本没有长时间的记忆，只有最多 \\(t\\) 长度的时间的记忆. 如果每次贪心选择概率最大的作为下一个词，那么只能获得一个可能的句子. 一个方法是采用 Weighted Sampling，按概率随机采样一个作为下一个词. 但是这个显然太垃圾了. Beam Search：取一个 \\(k\\) 作为 Beam Size，先 sample top K 个，然后每个再 sample top K，然后在这 \\(k^2\\) 个中挑选概率乘积最大的 top \\(k\\)，然后继续下去. 每次就是 \\(k\\to k^2\\to k\\to k^2\\to \\dots\\). 这个方法权衡了效率和优性. Vanilla RNN 的梯度消失问题. 很显然. 梯度爆炸可以用 gradient clipping，将其方向不变然后缩小. 但是 Gradient 小的时候，对噪声很敏感，所以不能放大. LSTM 中 forget gate 那一条路形成了一个 uninterrupted gradient flow. 这是因为长期记忆和新来的状态是一个 Additive Interaction. 剩下 LSTM 没啥额外的东西. Seq2seq：先对输入得到最终隐藏状态 \\(c\\)，然后输出的时候将\\(y_i\\) 和 \\(c\\) concat 起来得到然后算得 \\(y_{i+1}\\). 引入 START token 作为 \\(y_0\\)，然后和一个 STOP token. Image Capturing：给一个图，输出一段话去形容. 这个和 Seq2seq 内容类似，将 CNN 提出的 feature 作为 \\(c\\) 即可. 训练上，CNN 先用 Imagenet 的 CNN，然后要么寻 Capturing 的时候 freeze 住这个 CNN，要么做一个 fine tune. Freeze 的话，forward 的时候 memory 一样，backward 的时候 memory 开销被减小. Visual Question Answering：给图和文字然后回答问题. 也是一样的. 是一个 Multimodal Understanding 的任务. 但将模态分开处理后，就做的很烂. 信息融合（Information Fusion）可以用直接相加，相乘，或者 concatenate. Ablation Study：别的不变改变一个东西，去测试是会改变结果. 考虑我们 s2s 的一个问题，就是 decoder 只能看到一个 \\(c\\). 我们希望输出的时候能看到 Encoder 的 \\(h\\). 令一个注意力 MLP \\(f_{a}\\)，\\(e_{t,j}=f_a(s_{t-1},h_j)\\)，然后对 \\(e\\) 做 softmax 得到注意力权重 \\(a\\)，于是得到这一步的 \\(c_t=\\sum a_{t,j}h_j\\)，然后再让 \\([c_t\\ y_{t-1}\\ s_{t-1}]\\) 计算获得 \\(y_t\\).","path":"2025/05/14/CVDL-LSTM&Attention/","date":"05-14","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"程设笔记-JL降维","text":"我们希望将一个集合中的高维的点转化为较低维度的点. JL 方法希望我们这个降维是尽可能保距的. 即希望 \\(||f(x)-f(y)||^2\\in(1\\pm \\epsilon)||x-y||^2\\). 令 \\(m\\) 表示最终的维度，则 JL 方法有 \\(m=O(\\epsilon^{-2}\\log n)\\). 我们先看如果仅考虑两个点 \\(x,y\\) 有距离 \\(v=x-y\\)，我们可以有一个线性的随机映射 \\(g:R^d\\to R\\) 且 \\(E[g(v)^2]=g(v)\\). 那么我们取多组 \\(g\\) 作为多个维度就能尽量减少误差. 方法就是取一个每个维度都采自高斯分布 \\(N(0,1)\\) 的 \\(r\\) 维向量 \\(z\\)，然后做内积. 注意到这个和 Simhash 有所区别：Simhash 中要求 \\(z\\) 为单位方向向量（因为要做投影），而这个则是每个维度采样自 \\(N(0,1)\\). \\(N(0,1)\\) 变量的性质是平方的期望为 \\(1\\)，所以 \\(E[g(v)^2]=\\sum_i v_i^2E[w_i^2]=\\sum v_i^2=g(v)\\). 所以 \\((v\\mid w)^2\\) 为 \\(||v||^2\\) 的无偏估计. 然后我们再取 \\(m\\) 个这样的随机映射 \\(g_i(x)=(z_i\\mid x)\\)，然后最后令降维映射 \\(f(x)=\\frac{1}{\\sqrt{m}}(g_1(x),g_2(x),\\dots,g_m(x))\\). 这样得到的 \\(f\\) 就是我们得到的一个期望下保距的降维映射. JL 降维有一个比较复杂的误差分析. 我不会. 并且 JL 降维的这个 \\(O(\\log n)\\) 的界是紧的，不存在更优的保距的降维方法. JL 降维有一个好处，就是 Data Oblivious. 我们只需要事先生成 \\(m\\) 个 \\(d\\) 维的 \\(z_i\\)，即可对于任意 \\(d\\) 维输入向量进行降维！这在数据流中就很好. 一个小简化：可以令 \\(z_i\\) 的每个维度为 \\([-1,1]\\) 的随机值. 这符合我们上面对于 \\(E[g(v)^2]\\) 的分析. 现在我们将其应用至 Linear Regression 上. Linear Regression 要解决的问题是，有一个 \\(n\\times d\\) 的矩阵 \\(X\\) 和 \\(n\\) 维向量 \\(Y\\)，然后我们希望找到一个 \\(d\\) 维度的向量 \\(w\\)，最小化 \\(||Xw-Y||_2\\). 这个典型问题的最优解是 \\(w=(X^{T}X)^{-1}X^{T}y\\). 当然 \\(X^TX\\) 不满秩的时候会求逆的时候加上 \\(\\epsilon\\times id\\) 来近似求解广义逆. 但是对于 \\(n&gt;d\\) 的大多数随机情况，这个大抵是满秩的. 我们的计算复杂度是计算矩阵乘法的 \\(O(nd^2)\\). 但是大多数情况下 \\(n\\) 太大了，而且 \\(X\\) 很稀疏，也有很多重复表达的数据. 也就是说我们要对 \\(n\\) 这一个维进行降维. 并且由于最小化的是距离，所以我们希望降维是保距的. 那么我们的 JL 降维相当于一个 \\(m\\times n\\) 的矩阵 \\(A\\)，然后将由 \\(n\\) 个行向量组成的 \\(n\\times d\\) 的矩阵 \\(X\\) 变成 \\(m\\times d\\) 的 \\(AX\\). 最终 \\(m\\) 应该可以和 \\(O(d\\text{ poly}\\epsilon^{-1})\\) 同阶. 这个矩阵乘法需要 \\(nd^2\\) 次乘法. 这并没有优化复杂度. 但是后续的复杂度变成了 \\(O(d^3)\\). 我们现在考虑优化降维这一步骤的复杂度. 一个外星人的神秘做法：令 \\(A\\) 为每列仅有一个元素（并随机取为 \\(\\pm 1\\)）的矩阵，那么我们可以发现前面的那个无偏估计的证明还是适用的. 但唯一的区别是 \\(m\\) 需要取 \\(O(\\epsilon^{-2}d^2\\text{polylog}(\\epsilon^{-1}d))\\) 才能保证 Linear Regression 的结果适用. 取 \\(A\\) 为这样的矩阵之后，我们实际上就只需要遍历每个 \\(X\\) 中有值的元素 \\(X_{i,j}\\)，然后令 \\(A\\) 中第 \\(i\\) 列有值的行为 \\(p\\)，则贡献到 \\((AX)_{p,j}\\). 这个复杂度是 \\(O(nnz(X))\\)，其中 \\(nnz\\) 为非零值个数. 由于 \\(m\\) 变大，最后复杂度为 \\(O(md^2)=O(d^4)\\). 实际测试下来 \\(d^2\\) 是远不需要的. Trick：矩阵乘法 \\(a_{i,j}\\leftarrow b_{i,k}c_{k,j}\\) 的循环顺序应该是 \\(k,i,j\\)，这样对 Cache 比较友好.","path":"2025/05/07/程设笔记-JL降维/","date":"05-07","excerpt":"","tags":[{"name":"程设","slug":"程设","permalink":"http://lgswdn.github.io/tags/%E7%A8%8B%E8%AE%BE/"}]},{"title":"数分笔记-幂级数","text":"收敛半径 对于幂级数 \\(\\sum a_n x^n\\)，我们先关心其收敛的区间. 性质：若 \\(\\sum a_nx_1^n\\) 收敛，则对于任意 \\(|x_0|&lt;|x_1|\\)，\\(\\sum a_n|x_0|^n\\) 收敛. 证明：\\(a_n|x_0|^n=|a_nx_1^n||\\frac{x_0}{x_1}|^n\\). 由于 \\(\\sum a_n x_1^n\\) 收敛所以 \\(a_nx_1^n\\to 0\\)，而 \\(\\frac{x_0}{x_1}&lt;0\\) 故收敛. 这告诉我们一个推论，存在一个收敛半径 \\(R\\)，使得 \\(x&lt;R\\) 时绝对收敛而 \\(x&gt;R\\) 时发散. \\(x=R\\) 时则是具体情况具体分析. 如何求解 \\(R\\)？我们实际上只需要分析 \\(\\sum |a_n| x^n\\) 的正项级数即可. 令 \\(\\rho=\\overline{\\lim}\\sqrt[n]{|a_n|}\\)，则 \\(R=\\frac{1}{\\rho}\\). 这是达朗贝尔判别法的直接结果. 注意到这里可以直接取上极限. 当然对于一些情况我们也可以使用 \\(\\rho=\\lim\\frac{|a_{n+1}|}{|a_n|}\\). 但注意这里不能是上极限. 这里用上下极限只能得到关于 \\(R\\) 的不等式. 幂级数有很多很好的性质. 第一点是幂级数是内闭一致收敛的. 这是显然的. 第二点是若在端点处收敛，则在整个区间是一致收敛的. 考虑 \\(a_nx^n=a_nR^n(\\frac{x}{R})^n\\)，而由 Abel 判别法，\\(\\sum a_nR^n\\) 收敛而 \\((\\frac{x}{R})^n\\) 单调有界固一致收敛. 求导和积分 由于一致收敛性，我们可以知道幂级数的连续性是显然的，并且求导也相当于逐项求导. 幂级数的求导不会改变收敛半径. 用 \\(\\sqrt[n]{a_n}\\) 验证即可. 但是端点处收敛性是改变的，比如 \\(\\sum \\frac{x^n}{n}\\) 在 \\(-1\\) 处收敛但是求导后不收敛. 积分也是逐项积分，且也不改变收敛半径，不必多说. eg. 求 \\(\\sum_{n=1}^{+\\infty} \\frac{(-1)^{n-1}}{n}\\). 令 \\(f(x)=\\sum \\frac{(-1)^{n-1}x^n}{n}\\)，显然 \\(R=1\\). 对其求导得 \\(f&#39;(x)=\\sum (-1)^{n-1}x^{n-1}=\\frac{1}{1+x}\\). 故 \\(f(x)=\\ln(1+x)+C\\)，带入 \\(x=0\\) 知 \\(C=0\\).aaa \\(f(x)\\) 在 \\(x=1\\) 处收敛，故连续，由连续性知 \\(f(1)=\\ln 2\\). eg. 求 \\(\\sum_{n=0}^{+\\infty} \\frac{(-1)^n}{2n+1}\\). 还是令 \\(f(x)=\\sum a_nx^n\\)，显然 \\(R=1\\). 设 \\(g(x)=\\sum (-1)^n x^{2n}\\)，则对 \\(g\\) 做变上限积分得到 \\(f(x)\\). 而 \\(g(x)=\\frac{1}{1+x^2}\\)，积分后变成 \\(\\arctan x\\). 同样由收敛和连续性知 \\(f(1)=\\arctan 1 =\\frac{\\pi}{4}\\).","path":"2025/05/07/数分笔记-幂级数/","date":"05-07","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"数分笔记-函数序列","text":"令 \\(a_n(x)\\) 的部分和为 \\(S_n(x)\\)，则若 \\(S_n(x)\\) 一致收敛于 \\(S(x)\\)，则称 \\(\\sum a_n(x)\\) 一致收敛于 \\(S(x)\\). 一个简单但重要的命题：\\(\\sum a_n(x)\\) 一致收敛，则 \\(a_n(x)\\) 一致收敛于 \\(0\\). 比如这个就能说明 \\(\\sum x^n\\) 不一致收敛. DA 判别法：和之前是类似的. 还是需要 \\(a_n(x)\\) 关于 \\(n\\) 单调. \\(a_n(x)\\) 单调一致收敛于 \\(0\\)，\\(\\sum b_n(x)\\) 一致有界，则 \\(a_nb_n(x)\\) 一致收敛. \\(a_n(x)\\) 单调一致有界，\\(\\sum b_n(x)\\) 一致收敛，则 \\(a_nb_n(x)\\) 一致收敛. 下面研究一个典型例子：\\(\\sum \\frac{\\sin nx}{n}\\). 首先它不一致收敛. 证明用 Cauchy. 取 \\(x=\\frac{\\pi}{4n}\\)，则 \\(\\sum_{k=n}^{2n}\\frac{\\sin \\frac{\\pi}{4}}{n}\\ge \\frac{\\sqrt 2}{4}\\). 但是它内闭一致收敛. 这是因为 \\(\\sum \\sin nx=\\frac{2}{\\sin \\frac{2}{x}}(\\cos\\frac{x}{2}-\\cos(n+\\frac{1}{2})x)\\) 在内部闭区间上一致有界，由 DA 判别法知其一致收敛. 而它的核函数是可以求的：\\(x=0\\) 时取 \\(0\\)，而 \\(x\\in(0,\\pi)\\) 时取 \\(\\frac{\\pi-x}{2}\\). 这是一个相当反直觉的现象，所有的 \\(2k\\pi\\) 处都是很反直觉的间断的. 这是 Gibbs 现象. 我们来证明这一点. 这个证明很 Tricky，需要把 \\(\\frac{\\sin nx}{n}=-\\int_x^{\\pi} \\cos ktdt\\)，然后交换 \\(\\int\\) 和 \\(\\sum\\)，得到 \\(-\\int_x^{\\pi}\\sum_{k=1}^n\\cos ktdt\\)，然后变换成 \\(-\\int_x^{\\pi}\\frac{\\sin(n+\\frac{1}{2})t}{2\\sin \\frac{t}{2}}dt+\\frac{\\pi-x}{2}\\). 而由 Riemann Lebesgue 引理我们知道 \\(n\\to +\\infty\\) 时，这坨积分 \\(\\to 0\\). 所以结果成立. eg：\\(\\sum_{k=1}^{+\\infty} k^2e^{-xk}\\) 在 \\(x\\in(0,+\\infty)\\) 无穷阶可导. 实际上可以直接证明其内闭一致收敛然后就是每个项挨个求导. eg：\\(I=\\int_0^1 \\frac{(\\ln x)^2}{1-x}dx=2\\sum_1^{+\\infty}\\frac{1}{n^2}\\). 变成 \\(\\int_0^1 \\sum (\\ln x)^2x^n\\) 之后发现其一致收敛（\\(a_n(x)\\) 于 \\(e^{-\\frac{2}{n}}\\) 取最大值，计算发现 \\(a_n(x)\\le \\frac{2}{n^2}\\) 一致收敛）即可.","path":"2025/05/06/数分笔记-函数项级数/","date":"05-06","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"数分笔记-函数序列","text":"现在考虑一列函数序列 \\(f_n(x)\\)，以及其极限 \\(f(x)=\\lim_{n\\to +\\infty} f_n(x)\\). \\(f_n(x)\\) 的许多性质都无法直接推演到 \\(f(x)\\) 上，如： 连续性. 反例：\\(f_n(x)=x^n\\). 可积性. 反例：\\(f_n(x)=\\sqrt[n]{R(n)}\\). \\(lim\\) 和 \\(\\int\\) 的交换. 反例：\\(f_n(x)=nx(1-x^2)^n\\). \\(f(x)=0\\) 而 \\(\\int_0^1 f_n(x)dx=\\frac{n}{2n+2}\\). \\(lim\\) 和 \\(\\frac{d}{dx}\\) 的交换. 反例：\\(f_n(x)=\\frac{\\sin nx}{n}\\). \\(f(x)=0\\) 而 \\(f&#39;_n(x)=\\cos nx\\). 我们发现罪魁祸首是其收敛步调不一致导致的. 于是引入一致收敛的概念：称 \\(f_n(x)\\) 一致收敛于 \\(f(x)\\)，若 \\(\\forall \\epsilon&gt;0\\)，\\(\\exist N=N(\\epsilon)\\) 使得 \\(n\\ge N\\) 时 \\(\\forall x\\) 有 \\(|f_n(x)-f(x)|&lt;\\epsilon\\). 一致收敛的定义等价于 \\(\\sup |f_n(x)-f(x)| \\to 0\\). 当我们无法得知 \\(f(x)\\) 的时候，我们可以用 Cauchy 收敛准则：\\(\\forall \\varepsilon\\)，\\(\\exist N=N(\\varepsilon)\\) 使 \\(\\forall n,m\\ge N\\)，\\(\\sup |f_n(x)-f_m(x)|&lt;\\varepsilon\\). 在 \\(f_n(x)\\) 一致收敛于 \\(f(x)\\) 的情况下，连续性，可积性，以及 \\(\\lim\\) 与 \\(\\int\\) 的交换都是可以可以证明的. 导数则是另一回事. 连续性. \\(|f(x)-f(x_0)|\\le |f(x)-f_n(x)|+|f_n(x)-f_n(x_0)|+|f_n(x_0)-f(x_0)|\\)，三者皆可被 \\(N\\) 控制. 可积性. 令 \\(f_n(x)\\) 不连续点集为 \\(E_n\\)，并采用长度总和 \\(&lt;\\frac{\\varepsilon}{2^n}\\) 的区间覆盖 \\(E_n\\)，于是覆盖 \\(\\cup E_n\\) 的区间长度总和 \\(&lt;\\varepsilon\\)，故为零测集. 与 \\(\\int\\) 的交换. \\(\\int f(x)dx-\\int f_n(x)dx\\le (b-a)(\\sup |f(x)-f_n(x)|)\\) 可以被 \\(N\\) 控制. 当然显然这点对于无穷积分不使用. 接下来更加细致地去看连续性和导数. 连续性 Dini 定理： \\(f_n(x)\\in C[a,b]\\)，且 \\(\\forall x\\)，\\(f_n(x)\\) 单调 \\(\\to f(x)\\). 则 \\(f(x)\\in C[a,b]\\) 等价于 \\(f_n(x)\\) 一致收敛. 注意此处一定是闭区间. 开区间的反例：\\(f_n(x)=x^n\\) 在 \\((0,1)\\) 上. 不单调的反例：\\(f_n(x)\\) 在 \\(1/n\\) 处有波峰. 证明：考虑对于每个单独的 \\(x_0\\) 求证小领域内一致收敛，然后用有限开覆盖定理得知区间上一致收敛. 对于 \\(x_0\\)，存在 \\(N\\) 使得 \\(|f(x_0)-f_N(x_0)|&lt;\\varepsilon\\)，而又因连续性知存在小领域使得 \\(|f(x)-f(x_0)|\\)，\\(|f_N(x)-f_N(x_0)|&lt;\\varepsilon\\). 所以小领域内 \\(f(x)-f_N(x)&lt;\\epsilon\\)。而又因为单调性得知 \\(\\forall n\\ge N\\)，\\(|f_n(x)-f(x)|&lt;|f_N(x)-f(x)|&lt;\\epsilon\\). 所以一致收敛. Dini 定理可以用来证明 Visser 引理： 存在多项式序列 \\(P_n(x)\\) 使得 \\(P_n(x)\\) 在 \\([-1,1]\\) 上一致逼近于 \\(|x|\\). 证明：取序列 \\(a_{n+1}(x)=a_n(x)+\\frac{1}{2}(x^2-a_n(x)^2)\\). 可以归纳证明 \\(a_{n}(x)\\le |x|\\). 然后可以知 \\(a_n(x)\\) 单调，故极限存在且单调收敛. 应用 Dini 定理知一致逼近. 这个可以用来证明 Weierstrass 第一逼近定理：任意闭区间上连续函数 \\(f(x)\\)，存在多项式序列一致逼近于 \\(f(x)\\). 证明：由于 \\(f(x)\\) 一致连续，所以存在 \\(n\\) 段的折线函数 \\(g(x)\\) 使得 \\(|g(x)-f(x)|\\le \\varepsilon\\). 取 \\(H(x)\\) 为 ReLU 函数，则 \\(g(x)=g(0)+\\sum c_kH(x-x_k)\\)，其中 \\(c_k\\) 为两端折线斜率差. 然后再应用 Visser 引理，将 \\(H(x-x_k)\\) 用多项式逼近即可. 导数 上面没有讲求导可交换的条件. 实际上求导可交换需要导数一致收敛且本身收敛. 如果导函数不一致逼近则和积分的反例一样. 而如果本身不收敛则取 \\(f_n(x)=n\\) 即为反例. 证明：\\(f&#39;_n(x)\\) 一致收敛于 \\(\\phi(x)\\). 令 \\(\\Phi_n(x)=\\begin{cases}\\frac{f_n(x)-f_n(x_0)}{x-x_0} &amp; x\\neq x_0 \\\\ f&#39;_n(x_0) &amp; x=x_0\\end{cases}\\). 那么由条件知，\\(\\Phi(x)=\\begin{cases}\\frac{f(x)-f(x_0)}{x-x_0} &amp; x\\neq x_0 \\\\ \\phi(x_0) &amp; x=x_0\\end{cases}\\). \\(\\Phi_n(x)\\) 显然在 \\(x_0\\) 处连续. 并且我们要证明 \\(f&#39;_n(x)\\) 一致收敛于 \\(\\phi(x)\\)，其实相当于就是要证明 \\(\\Phi(x)\\) 也连续. 所以现在需要目标是证明 \\(\\Phi\\) 一致收敛即可. 使用 Cauchy 准则. \\(\\Phi_m(x)-\\Phi_n(x)=\\frac{1}{x-x_0}(f_m(x)-f_m(x_0)-f_n(x)+f_n(x_0))\\). 令 \\(g=f_m-f_n\\)，则相当于 \\(\\frac{1}{x-x_0}(g(x)-g(x_0))=g&#39;(\\xi)=f&#39;_m(\\xi)-f&#39;_n(\\xi)\\). 而由于 \\(f&#39;\\) 一致收敛，所以后者可以被 \\(N\\) 控制. 这个证明同时可以告诉我们 \\(f\\) 本身也是一致收敛的，因为两侧都乘上 \\(|x-x_0|\\) 则知 \\(g(x)-g(x_0)\\) 被 \\(N\\) 控制. 再加上 \\(x_0\\) 收敛所以 \\(g(x_0)\\) 可以被控制，故 \\(g(x)\\) 也可以被控制. 实际上条件可以弱化为存在 \\(x_0\\) 使得 \\(f(x_0)\\) 收敛，而不需要所有都收敛. 证明：可以直接证明其余点一定收敛. \\(f_m(x)-f_n(x)\\le |f_m(x_0)-f_n(x_0)|+|g_m(x)-g_n(x)|\\). 前一项可被 \\(N\\) 控制，后一项如上述证明，也可被 \\(N\\) 控制，所以其余点都收敛. 并且若 \\(b-a&lt;+\\infty\\) 也可推出一致收敛. 实际上导数和连续性的条件都只与单点的小领域相关，所以条件可以削弱称在 \\((a,b)\\) 上内闭一致收敛（即对所有 \\((a,b)\\) 内的闭区间一致收敛即可）.","path":"2025/05/06/数分笔记-函数序列/","date":"05-06","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"CVDL-3D Vision 1","text":"前半节是优质考试. Point Net ++ 也可以用到 2D 的 Classification 任务中. 和 Conv 类似，每次一个 Ball Query 也可以包含一个小矩阵. 不过显然 Conv 是更加强大的. 考虑对于这样一个小矩阵，PointNet 相当于每个位置做同一个 MLP 然后再全局 Pool，而 Conv 则可以对每个小矩阵中的位置设置不同的权值. 我们可以认为 PointNet 结构是一个 Isotropic 的 Kernel（各向同性），而 Conv 是一个 Unisotropic Kernel（各向异性）. 一个让 PointNet 有一定的 Unisotropic 的方法是 KP Conv. 我们在球的不同位置放 16 个 Conv，然后每个点的 Weight 为这若干个 Conv 的线性插值. 但是这个有些过于复杂了. Voxel 3d Convolution 虽说很符合我们对一个网络的想象，但是还是太 Expensive 了. 我们考虑点云为什么复杂度低，是因为点云只记录了表面，而不需要占据内部信息. 于是我们引入 Sparse Voxel. 只记录 Surface Voxel. 怎么做 Conv 呢？ Sparse Conv：\\(A\\) 卷上 \\(c\\) 得到 \\(B\\). 对于 \\((i,j,k)\\)，只有 \\(A(i,j,k)\\neq 0\\)，才会去计算 \\(B(i,j,k)=\\sum A(i&#39;,j&#39;,k&#39;),c(i-i&#39;,j-j&#39;,k-k&#39;)\\). 这样 Voxel 中的元素数量不会变大，并且运算次数也合理. 这个的 Implementation 有一定技巧性，你需要数据结构来维护一个点周边的点. 并且这个也有 Discretization Error. 一个稍微会好一点的办法是将 Voxel 中的点的坐标均值也作为 Feature 放进去. 于是这个也很适合比较大尺度的一些场景，比如区分车和人，Efficiency 也很好，并且也具有 Conv 的 Equivariance. 而点云的 PointNet 有更高的 Resolution 且更简单的实现，但是 FPS 和 Ball Query 太慢. Mesh 也是有用的. Mesh 在 3D AIGC 中就起到很大的用处，但是没讲.","path":"2025/05/06/CVDL-3D Vision 3/","date":"05-06","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"CVDL-3D Vision 2","text":"真 3D 要求能够给出任意两个点的 metric distance（度量距离）. 而显然深度图中给出的信息（一个点的 pixel 坐标以及 depth）单位对不上，所以无法得知 metric distance. Depth Sensors 不是无限远的. 有一个最近 &amp; 最远的距离（near plane &amp; far plane）. 可以测量到的空间为一个棱台. 所以比如人脸检测的时候不能靠太近. Depth Sensor 要四个东西. RGB &amp; IR Projector &amp; 2 IR Receiver. （IR：红外） 最简单的用两个 RGB 即可得到 Stereo Sensor. 即用两个水平的相机，找到同一个点在两个相机分别的位置. 设在两张纸上的 pixel 坐标，一个在 \\(u\\) 一个在 \\(u&#39;\\)，视差为 \\(u-u&#39;\\). 两个相机相隔 \\(B\\)，则 \\(u-u&#39;=\\frac{Bf}{z}\\). 我们先让两张图片处于同一水品. 然后对同一水平线做 line search. 然后找同一个点的方法是做特征提取，然后每个 Pixel 的特征两张图做点积. 点积最大的意味着对应. 图中有若干部分是左眼看得见而右眼看不见的. 可能是因为在边缘，也可能因为是被挡住了. 一个较大的问题是对应关系很难找. 比如纹理缺失的墙就根本对应不起来，重复的东西也对应不起来. 并且如果有镜子的话，左右两眼看到的同一个点对应的是不同的东西（画一画就能明白），因为我们看到的是镜子反射出的别的东西，所以就无法建立两张图图片中的正确的 correspondance. 并且透明的物体也有类似的问题，因为光的折射. 所以这个方法只能做一个 diffuse surface（漫反射表面）的工作，而做不了镜面（specular）和透明（transparent）；点是两个眼睛都能看到的 texture 缺失或者过于. 结构光：发射红外射线，形成红外光斑，来打上 texture. 不能在户外使用，因为会有别的红外干扰（太阳光）. 之前直接做的是 Passive Stereo. 而主动发射红外光线的 Active Stereo 解决了 textureless 的问题，但是却还是无法解决镜面和透明的问题. 黑色会吸收光，所以会导致 Missing Value. 而透明的可能不会有什么 Missing Value 但是会导致很多塌陷（相当于折射之后透过去了）. 反光肯定也还是测不标准的. 激光雷达：获得 Ray Depth. 但是还是无法很好地处理透明和镜面. 这是一个很难办的问题. 接下来还是先假定这些没遇到上述问题比较好. Voxels：Volumetric Pixels 清晰度很小的时候就已经量级很大了. 比如 \\(32^3\\) 所需存储量已经大于了 \\(256^2\\). 离散带来的几何偏差很大，刻画能力和精细度很差. 但是好处是确实够简单. 比如可以应用一个 3D 的 CNN. 不过参数量也很巨大. Mesh，Point Cloud 和 Implicit representation（\\(F(x)=0\\)）都可以给出表面信息. Mesh：对表面的一个分段线性（Piecewise Linear）的近似，用一个简单的线性平面来近似每一个局部. Triangle Mesh 表面近似成很多个三角形拼凑起来. Quad Mesh 则可以用四边形平面，但由于一些拓扑的问题会导致有奇点所以不常用. 最多用的就是三角面片. 实际上我们可以用一个 Graph 来表示. 每个三角形的顶点用一个 Vertex \\(V\\subset R^3\\) 表示，然后三角形的边就形成了 Edge \\(\\subset V\\times V\\)，然后三个 Vertex 可以组成一个 Face \\(\\subset V\\times V\\times V\\). 所以一个 Mesh 可以用 Vertex 集合，Edge 集合，Face 集合三个来一起表征. 下面探讨几何属性，略去材质，颜色等属性. 数据存储方法：OBJ Format，存每个 Vertex 的坐标，和每个 Triangle 由哪几个 Vertex 组成. 每个 Triangle 的点按照逆时针列出（符合右手定则）. 如果在此基础上，钦定字典序最小，那么就能唯一确定了. 如果转向错了的话，法向量就反了，会导致渲染出错，或者法向量作为 feature 的时候出错. Geodesic Distance：表面上两个点的最短距离. 算这个东西一个很 Naive 且不准确的做法是直接计算最短路. 存在一点更好且更快的近似算法. Point Cloud：只保留 \\(V\\). 是 Irregular 且 Orderless 的. 很轻量级. 点云本质是一个 Point Set. 点的顺序无意义. 在同等的存储量上，Point Cloud 的集合刻画最好. 点云是在 Surface 这个二维流形中的采样. 对于同一个 Surface，可以有很多不同的采样方式. 均匀采样：即任取一个面积，除以总面积之后，点数比约等于面积比.我们第一步需要得到一个 Mesh. 然后再去从 Mesh 上把点云采下来. 假如有 \\(M\\) 个三角形，我们按三角形的面积作为权，每次以这个为权随机选一个三角形，然后再在三角形中随机采点. 如何在三角形中随机采点？考虑镜像扩充成一个平行四边形，然后在平行四边形的坐标中采点，然后再镜像回三角形. 但是这种纯随机还是会有很多不令人满意的地方. 因为随机，只是期望正确，所以有的地方稀疏有的地方密集. Farthest Point Sampling：使得任两个 Sample 的点的距离之和最大. 这样显然会导致分布的很均匀，但这是个 NP Hard 问题. 考虑做点贪心近似. 首先做 \\(N\\) 更大的 Uniform Sampling，然后再在这些点中去找尽量的 FPS 子集. 考虑一个贪心：先找一个随机的点，然后再找一个点使得到那个点距离最大，然后再找一个点使得到这两个点距离和最大... 这样找 \\(K\\) 个点，得到 \\(O(NK)\\) 的贪心算法. 这个就更加 Evenly Distributed 了，并且更少丢掉一些 local 信息. 而实际上我们也不一定要用 Mesh 去做. 我们也可以直接从深度图中得到 \\(H\\times W\\) 个点，然后再 FPS. 如何比较两个点云呢？ Chamfer Distance：对于点云 \\(A\\) 的每个点，找其在 \\(B\\) 中的最近邻，距离和加起来. 对于 \\(B\\) 同理. 两个距离和加起来得到 Chamfer Distance. Earth Mover's Distance：找 \\(A,B\\) 的最小二分图匹配 \\(\\phi\\). 令 \\(d_{EMD}(A,B)\\) 为最小权匹配的距离和. EMD 对于每个区域的密度很敏感，而 CD 则好一些. 3D Deep Learning 直接把 \\(3n\\) 长度的 vector 喂给 MLP 很糟糕. 因为点云无关顺序，但是 vector 换个顺序输出就会变很大. 如果只是 sorting，那么删一个点增一个点，变化也太大了，很糟糕. PointNet：先对每个 \\((x,y,z)\\) 通过 MLP 变成长 \\(c\\) 向量（\\(n\\times 3\\to n\\times c\\)），然后再 MLP 变成长 \\(d\\) 向量. 然后再对每个维取一个对称函数 \\(g\\) （比如 max pool）得到一个 \\(c\\) 维向量 \\(z\\)（\\(n\\times d\\to 1\\times d\\)），然后再 MLP 做 classification. 如果我们要做 Segmentation，那么就直接暴力将 \\(z\\) 拼接在每个点的长 \\(c\\) 向量之后得到 \\(n\\times(d+c)\\) 然后直接还原. PointNet 对加删点非常鲁棒. 因为只有那个维度的 Max 被删掉才会导致发生变化. PointNet 的局限性在于并没有局部信息. 只有单点信息和全局信息. 比如所有点都 \\(+1\\) 那么就变化剧烈. 如何提取局部特征，做有层级的特征提取呢？ 考虑选一个点 \\(c\\)，考虑其半径为 \\(r\\) 的领域，用其关于 \\(c\\) 的相对坐标，再 append 上 \\(c\\) 的绝对坐标，去做 Point Net. 将最后得到的向量放在 \\(c\\) 处. 于是点数变少，通道数增加，完成了 Down Sampling. 如何做 Up Sampling 呢？我们先对于每个被删去的点，找到在保留的点中的三近邻，然后做线性插值 得到其信息. 然后再结合其 Skip Link 得到的 Feature，Concatenate 之后再对每个单点做 Pointnet 得到新的 Feature.","path":"2025/04/23/CVDL-3D Vision 2/","date":"04-23","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"AI基笔记-期中复习","text":"一个并没有什么用的，纯做针对性扫盲和复习. 机器学习 KNN 为有监督无参方法. 但是有超参 \\(k\\)（要选奇数） 和距离函数（metric distance）. 聚类 K means 聚类：先随几个点，然后聚类之后，然后每个类再选取中心，再聚类，循环往复. 对初始化过于敏感. 需要初始种子尽量远离，并尝试不同的初始种子. 对 outlier 非常敏感. Validation 划分为训练集，验证集，测试集. 可采用 Cross Validation. 最大似然 \\(x\\) 独立同分布（iid）采样于模型 \\(\\theta\\)，则 \\(p(X\\mid \\theta)=\\prod p(x_i\\mid \\theta)\\)，取 log 得到 \\(\\sum \\log p(x_i\\mid \\theta)\\). log 的原因：很多 \\(p\\) 相乘太小了，精度不够. 取 \\(\\theta_{MLE}=\\arg\\max \\sum \\log p(x_i\\mid \\theta)\\). 最大后验概率 将数据看作参数，\\(\\theta\\) 采样自一个分布. 则 \\(p(\\theta\\mid X)=\\frac{p(X\\mid \\theta)p(\\theta)}{p(X)}\\)，正比于 \\(p(X\\mid \\theta)p(\\theta)\\)，即 \\(\\theta_{MAP}=\\arg\\max p(X\\mid \\theta)p(\\theta)\\). 比如可以用于对 \\(\\theta\\) 加正则化. 线性回归 \\(\\frac{1}{n}(A\\beta-Y)^{T}(A\\beta-Y)\\)，求导后可知 \\(\\beta=(A^{T}A)^{-1}A^TY\\). 用 MAP 理解来说，若 \\(A^{T}A\\) 不可逆则加入正则化 \\(\\lambda ||\\beta||^2\\)，于是变成 \\((A^{T}A+\\lambda I )^{-1}A^TY\\). 这个正则化限制了让所有参数都尽量小. 而用 L1 正则化，则让尽量多的元素为 \\(0\\). 深度学习 目标检测 在 Faster R-CNN 提出了用 Region Proposal Network 代替 Selective Search 做 Regional Proposal. YOLO：将目标检测问题看作回归问题，分成很多个格子每个格子做回归问题. 图像分割：Semantic &amp; Instance. 使用 UNet（反卷积 + Skip Connection）是像素级分类问题.s CNN 感受野计算公式：\\(l_k=l_{k-1}+((f_k-1)\\prod_{i=1}^{k-1}s_i)\\)，其中 \\(s_i\\) 为 stride 大小. Transformer 不考. 无监督学习：聚类，降维，密度估计，K-Mean，GAN，Auto Encoder 密度估计的 loss 使用 NLL loss. 逻辑回归：做分类任务，损失函数用交叉熵，比均方梯度更大. 二分类输出用 Sigmoid 压缩到 \\([0,1]\\) 之间；多分类用 Softmax 作为模型输出. Sigmoid 的组合意义：\\(y=\\frac{1}{1+e^{-x}}\\)，则 \\(\\log \\frac{y}{1-y}=x\\). 使用 NLL 而非 square error 的原因是因为前者梯度更大. 并且 square error 的梯度有 \\(\\theta(x)(1-\\theta(x))\\) 项，在两端的时候梯度最应该大的时候梯度反而接近 0. 偏置的意义：决策边界可以上下调整，不用经过 origin. Tanh：\\(f(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\). SoftMax 激活函数涉及多个数值，其他函数只作用于一个数值. 精度率（Precision）：关心假阳性. 召回率（Recall）：关心假阴性. Average Precision ：P-R 曲线的面积. R-CNN ：Selective Search 找方格，然后调整为固定大小后每个单独输给 VGG 跑结果，通过回归得到边界框位置. SPP Net ：整个图像输入到 CNN 提取局部特征，Selective Search，然后使用空间金字塔池化（SPP）调整为相同尺寸，通过回归得到边界框位置. Fast R-CNN ：改用 ROI 池化更为简单，并且分类和回归器一起训练，不再使用 SVM. Faster R-CNN：使用 Regional Proposal Network. YOLO：划分为 \\(13\\times 13\\) 个框，每个框提 \\(6\\) 个方案，每个方案包括真实框的大小及其置信度，并每个框做物体辨别识别. 图像分割的 Dice 系数：\\(\\frac{2|A\\cap B|}{|A|+|B|}\\). 值都是 \\(0,1\\) 时等于 \\(\\frac{2|A\\cdot B|}{|A|^2+|B|^2}\\). 图像分割使用 Mirror Padding 防止丢失边缘信息. 图像分割的损失加权：给边缘增加权重，根据物体大小调整权重. Vanilla GAN 使用了全连接，DC GAN 使用了卷积. 显然相比之下前者就很 L. 相比于GAN，使用均方误差的 VAE 容易忽略面积较小的重要特征. Cycle GAN： Cycle Consistency Loss：使得 \\(A\\) 和 \\(G(A)\\) 确实在表达类似的事. Identity Loss：加入 \\(||A-G(A)||_1\\)，使得不会过度翻译（比如真实照片蓝色偏多导致把黄色的画翻译成蓝色了）. One-hot vector：维度灾难，词之间独立. Bags of words：维度灾难，忽略了时序. Continuous Bags of words（CBOW）：根据上下文预测中间的词. Skip Gram：通过中间词预测上下文. 预测每个词是否属于上下文，使用 Sigmoid. 直接对所有词的话样本太多了，于是使用随机采样若干个正样本以及负样本. 天气预测是 Many to Many 的同步模型，翻译是异步的. LSTM：遗忘门 \\(f_t\\)，输入门 \\(i_t\\)（\\(c&#39;=f\\odot c+i\\odot \\hat c\\)），输出门 \\(o_t\\)（\\(h=o\\odot c&#39;\\)）. One to many 的损失可以使用所有步的平均损失. 网络是 Softmax 输出，可以使用 Top-K 采样获得多种描述句子. 同步 Many to Many 训练时要预先定义一个合适的序列长度.","path":"2025/04/21/AI基笔记-期中复习/","date":"04-21","excerpt":"","tags":[{"name":"AI基","slug":"AI基","permalink":"http://lgswdn.github.io/tags/AI%E5%9F%BA/"}]},{"title":"CVDL-3D Vision 1","text":"人的视觉不是显示地表达 3D 信息的，而只是两个眼镜的观测形成双目图（Stereo Image），通过双眼的视差来感知到立体的结构. 这是一种基于 2D 的 3D 的表征. 也可以在相机中通过将多视角图缝合成一张图，形成全景图. 而因为隐式的，所以人眼没法精确感觉绝对的距离，只能相对地知道远近. 并且人的观察是连续的，所以可以通过视觉来完成一系列动作. 通过计算机去做出真 3D. 真 3D 视觉可以给出非常精准的信息. 3D 的信息有很多不同的 Representation. 有 Regular form 的比如多视图，深度信息，以及体素（和像素对应）；而也有 Irregular 的表面网格，以及点云表达. 从 3D 得到 2D 的投影的建模就是相机模型. 即从 3D 中的 \\((x,y,z)\\) 打到 \\((x,y)\\) 的函数. 最简单的是直接小孔成像，这样就能通过这个小孔来形成真实世界与相纸的一一对应. 这样就形成了一个 Pinhole Camera. 中心通过的线形成了 Optical Axis. 令 \\(P=[x,y,z]\\)，成像的点是 \\(P&#39;\\)，则 \\(x&#39;=f\\frac{x}{z}\\) 而 \\(y&#39;=f\\frac{y}{z}\\). 但如果 aperture 开太大就很模糊，开太小就很暗. 考虑加透镜来解决这样的问题. 一个点发出的，所有经过凸透镜的光线会聚集于一个点. 这个的局限是一旦在交平面之外就会出现模糊. 通过所有平行光线皆经过焦点. 对于近轴的光线，【？？？】\\(x&#39;=f\\frac{x}{z&#39;}\\)，令 \\(z&#39;=f+z_0\\). 仿射变换会让平行的线不平行，平行线会交于一点. 相机的内在参数 （Intrinsics，相机本身的东西，无法改变）和外在参数（Extrinsics，在世界坐标系中相机的位置等，可以改变）. 考虑 Pinhole Camera 的内在参数. Retina Plane 的坐标系和相纸的 Pixel 坐标系不一样. 并且其中所有的单位都是现实世界中的单位（比如 \\(m\\)）. 令光轴交相纸的点在相纸的 Pixel 是 \\((c_x,c_y)\\)，则需要 \\(x\\to fk\\frac{x}{z}+c_x\\)，\\(y\\to fl\\frac{y}{z}+c_y\\)，其中 \\(k,l\\) 的量纲为 \\(pixel/m\\). 令 \\(\\alpha=fk\\)，\\(\\beta=fl\\)，则得到 \\((x,y,z)\\to (\\alpha\\frac{x}{z}+c_x,\\beta\\frac{y}{z}+c_y)\\)，称为投影变换. 注意其不是线性变换，所以无法直接表达成矩阵形式. 但是我们发现如果同时乘一个 \\(z\\) 就是线性变换了. 于是引入齐次（Homogeneous）坐标系统. 从 \\(E\\to H\\) 就是再加一个 \\(1\\)，即 \\((x,y)\\to [x\\ y\\ 1]^T\\)，\\((x,y,z)\\to [x \\ y \\ z \\ 1]^T\\). 而 \\(H\\to E\\) 就是 \\([x\\ y \\ z]^{T}\\to (x/z,y/z)\\). 准确来说就是容许一个对所有维度的同时乘 / 除. 于是我们就可以 \\([x\\ y\\ z \\ 1]^T\\to [\\alpha x+c_xz\\quad \\beta y +c_yz\\quad z]^T\\). 这个线性变换就是左乘 \\(\\left(\\begin{matrix}\\alpha &amp; 0 &amp; c_x &amp; 0 \\\\ 0 &amp; \\beta &amp; c_y &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\end{matrix}\\right)\\). 而相机的内在参数全在前 \\(3\\times 3\\) 的矩阵内. 称这个矩阵为相机的内在参数 \\(K\\)，于是就有 \\(P&#39;=K[I \\quad 1]P\\). 我们还可以对一些角度 \\(\\theta\\) 的歪曲进行修正，也可以融入矩阵 \\(K\\). 考虑外在参数. 我们其实可以发现之前的 \\((x,y,z)\\) 都是以光心为轴的坐标系的坐标，并非世界坐标系. 平动变换：假如我们要让 \\(x\\to x+T_x\\)，令 \\(T=[T_x\\ T_y\\ T_z]^{T}\\)，则可以利用 \\(T\\) 写成一个 \\(4\\times 4\\) 的对齐次坐标系统的矩阵作为变换. 旋转变换：我们可以利用三维的旋转矩阵 \\(R\\) 也写成一个 \\(4\\times 4\\) 的矩阵操作. 注意一个旋转矩阵只有 \\(3\\) 个自由度. 运用上面两种变换，令 \\(P_c\\) 为相机坐标系的坐标，\\(P_w\\) 为世界坐标系的坐标，则可以写成 \\(P_c=RP_w+T\\). 于是全部串在一起就有 \\(P&#39;_{3\\times 1}=K_{3\\times 3}[R\\ T]_{3\\times 4}P_{w\\ 4\\times 1}\\). 可以用这个式子来证明线还会映成线，并且产生近大远小（因为 \\(/z\\)）. 上面的模型称为 Perspective Camera. 当 \\(z\\) 可以看成常数（物体深度差相比于 \\(z\\) 很小），那么就不用齐次坐标了，称为弱投影模型. 纯线性，不用除 \\(z\\). 还有一种方式叫正交投影（三视图那样）. 每个点用和相纸垂直的平行光投影到平面. 其好处是完全真实反应了物理世界的东西，因为它是保距的同态. 但就一点距离信息都没有了. Camera Calibration Camera Calibration 的目标是从拍到的图片求解出外在 &amp; 内在的参数. 使用 Calibration Rig，即一个三维的架子，上面画着棋盘格. 我们以这个架子为世界坐标系. 这个 架子上的每一个棋盘格点我们都知道它在世界坐标下的坐标. 我们又能拍照片知道它在相纸上对应的坐标. 注意到自由度是 \\(11\\). 所以至少需要 \\(11\\) 个方程，即 \\(6\\) 个对应关系（一个 pixel 的 \\(x,y\\) 可以列两个方程）. 最后可以列成一个 \\(Pm=0\\). 于是限定 \\(|m|=1\\) 求解方程. 所以还是一个 constraint minimize problem. 求得解 \\(\\tilde M\\) 之后，我们想要 unnormalize 它. 反正都很复杂. 注意，一个棋盘不够，必须是一个架子. \\(K\\) 中 \\(\\alpha,\\beta\\) 参数决定了摄像机射出的光线的夹角. 即 Field of View. \\(K\\) 可以决定光线间隔多大. 而 \\(T\\) 又是一个近大远小. 如果所有采样点都在一个平面，那么 \\(K,T\\) 可以不同而造就同一个图.（对于任意一组 \\((K,T)\\)，\\(T\\) 近一点，然后角度放大一点，图就一样了）. 给一张有 Radial Distortion 的时候（比如鱼眼相机），就要引入对畸变的修正. 我们就需要拟合一个非线性的 mapping. 这在现实中是很常见的. 如何判断好不好呢？考虑通过 Reproduction Errors：我们先推断出 Projection Matrix 之后，再用这个 Matrix 去推断在相纸上的投影. 实际上并不用做成一个 grid. 可以拍很多张不同视角的棋盘的照片，或者固定相机随便动棋盘（移动都是相对的，等价于固定棋盘移动相机）. 深度图像是一个 2.5D 的表达. Ray Depth：光线距离 Z Depth：深度图记录的深度 有了深度图，有了 \\(K\\)，就可以通过深度图上的 \\((u,v,z)\\) 推断出其真实坐标 \\((x,y,z)\\). 直接运用 \\(P\\) 和 \\(P&#39;\\) 的关系就行了. 这个过程称为 depth backprojection，形成 depth pointcloud.","path":"2025/04/16/CVDL-3D Vision 1/","date":"04-16","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"程设笔记-欧式空间的近似","text":"下面问题皆考虑二维情形. 格点离散化 将整个平面划分为 \\(l\\times l\\) 的正方形块，然后将平面上的每个数据点归到最近的格点（正方形中心）. 一个简单的问题：\\(1+\\epsilon\\) 近似直径. 在上面的离散化中，两个点的距离变化不会超过 \\(\\sqrt{2}l\\). 先通过取任一点求最远点找到直径的 \\(2-\\) 近似 \\(T\\). 于是 \\(1+\\epsilon\\) 近似只需要取 \\(l=T\\epsilon/\\sqrt{2}\\). 这个方法求点集的 \\(1+\\epsilon\\) 近似直径只需要复杂度为 \\(O(n+\\frac{1}{\\epsilon^2})\\). 然后看一个最小包围圆的问题. 引入一个 Coreset 的概念：定义点集 \\(S\\) 的 \\(\\epsilon-\\)coreset 为 \\(T\\) 使得 \\(T\\subset S\\) 且 \\(T\\) 的答案为 \\(S\\) 的 \\(\\epsilon\\) 近似. 可以发现上述取 $l=R'/2 $ 得到的离散化后的点集随便改一改就是一个 coreset. 其中 \\(R&#39;\\) 为 \\(2-\\)近似. Coreset 支持一个分治. 可以先求出 \\(S\\) 和 \\(T\\) 的 coreset，然后再这两个 coreset 并起来再求一个 coreset，就得到了 \\(S\\cup T\\) 的 coreset. 四分树 考虑对于原本的平面，横竖各切一半得到四个子平面，然后不断这样递归下去，直到边长为 \\(1\\) 或里面只有一个点. 那么这棵树至多 \\(\\log V\\) 层，并且节点数是 \\(O(n\\log V)\\). \\(d\\) 维度的时候可以通过简单的优化使得只需要乘上 \\(d\\)（空的节点不需要占用空间）. 有一些优化. 比如像压缩 trie 一样可以同样压缩四分树使得节点数线性. 考虑如何用四分树支持动态的最近邻查询. 假如我们查询点 \\(x\\) 的最近邻. 假如我们到某个阶段的时候，有着当前答案 \\(q&#39;\\)，在节点 \\(u\\)，节点 \\(u\\) 有四个儿子 \\(v_k\\)，那么我们希望只有在 \\(v_k\\) 中不存在 \\((1+\\epsilon)q&lt;q&#39;\\) 的 \\(q\\) 时才不递归进入 \\(v_k\\). 判断这点的时候考虑使用近似：我们对每个节点随便选择一个代表元 \\(y\\)，那么将 \\(q\\) 放缩至 \\(dis(x,y_{v_k})-diam(v_k)\\)，其中 \\(diam(v)\\) 可以用 \\(v\\) 的长度的 \\(\\sqrt 2\\) 倍放缩. 正确性大抵比较清晰. 考虑一下复杂度. 首先如果 \\(diam(u)&lt;\\epsilon \\times ans/2\\) 那就肯定不会继续递归下去了. 然后感受一下，一个儿子如果被递归了，那么其 \\(x\\) 到代表元不会太远（实际上 \\(O(\\epsilon^{-1}diam)\\)），所以所有深度相同的代表元都在一个圆中，而半径为 \\(R\\) 的圆中的 \\(l\\) 正方形数量至多有 \\((\\frac{R}{l})^2\\) 个，所以复杂度最后在 \\(O(\\epsilon^{-2}\\log V)\\). WSPD 有 \\(n^2\\) 个点对，考虑对于点对离散化. 比如有两坨点，每坨点之间间距不大，但是这两坨点间距很大，那么就不用分别记录两坨点中每个两个点的距离. 定义 \\(\\epsilon-\\)WSPD 为一个序列，序列中的每个元素都是两坨点形成的 pair \\((A_i,B_i)\\)，其中 \\(\\max (diam(A_i),diam(B_i))&lt;\\epsilon dis(A_i,B_i)\\). 并且 \\(\\cup_i A_i\\times B_i=P^2\\). WSPD 可以用四分树高效表示. 具体而言，令 \\(work(u,v)\\) 表示处理节点 \\(u\\) 和节点 \\(v\\) 形成的 pair 的 WSPD，那么不妨设 \\(diam(u)&gt;diam(v)\\). 那么就如果 \\((u,v)\\) 是合法 pair 直接返回，否则递归所有 \\((u_i,v)\\). \\(u_i\\) 为 \\(u\\) 的儿子. 这个算法构造的 \\(\\epsilon-\\)WSPD的项数是 \\(O(n\\epsilon^{-O(d)}\\log V)\\) 的. 一个应用：精确求解最近点对. 因为如果最近点对 \\(p,q\\) 满足 \\(p\\in A_i\\) 而 \\(q\\in B_i\\) 那么肯定 \\(|A_i|=|B_i|=1\\). 另一个应用是近似直径：取 \\(\\epsilon/2-\\)WSPD，然后求 \\(A_i,B_i\\) 的代表元距离. 欧式图的 \\(t-\\)Spanner：对欧几里得平面的完全图，找一个子图，使得 \\(dis(x,y)\\le t||x-y||\\). 取 \\(\\epsilon/8-\\)WSPD，代表元连边然后每个点再连代表元即可. 运用 \\(t-\\)Spanner 就可以做欧式平面的最小生成树的近似. 但是你要取 \\(\\epsilon/8\\) 的话很有可能常数就超过了你的 \\(n\\). 而实际上，取 \\(t\\) 较大（甚至 \\(=1\\)）就已经能2 有很好的结果.","path":"2025/04/13/程设笔记-欧式空间的近似/","date":"04-13","excerpt":"","tags":[{"name":"程设","slug":"程设","permalink":"http://lgswdn.github.io/tags/%E7%A8%8B%E8%AE%BE/"}]},{"title":"CVDL-CNN Training3","text":"Classification 一张图必须要看全了才能有综合的判断. 我们希望到最后一个 Pooling 后每个 pixel 的感受野都是全图. 三个 \\(3\\times 3\\) 的卷积层的感受野为 \\(7\\times 7\\)，和一个 \\(7\\times 7\\) 的卷积层相同. 但是三个 \\(3\\times 3\\) 的卷积层的非线性更强，并且参数量更少. \\(3\\times 3^2=27\\) 而 \\(7^2=49\\). 所以实际上我们用更少的参数，得到了更强的表达能力. 于是 VGG 和 AlexNet 相比的优势就在于此. VGG 表明要逐渐对信息进行提取和抽象，使得感受野逐级变大. Resnet 中，每次使用 Stride=2 的卷积时，Channel 数量提升. 于是 Neuron 数量只会减半，并且感受野增大一倍. 最初的 Resnet 先用了一个 Stride=2 的 \\(7\\times 7\\) 卷积，然后每个残差块由两个卷积层组成，每若干个残差块就安排一次 Stride=2 的卷积. 最后降低到 \\(7\\times 7\\times 512\\) 之后每个 Channel 做一个全局的池化然后进入全连接. 于是一个有关模型大小的值就是每多少个残差块安排 Resolution 的减半. 但是空间开销和参数量是随着这个值线性增长的. \"Bottleneck\" 层：先从 \\(c\\) 个 Channel 做 \\(1\\times 1\\) 卷积变成 \\(c/4\\) 个 Channel，然后再 \\(3\\times 3\\) 卷积，最后再 \\(1\\times 1\\) 卷积回 \\(c\\) 个 Channel. 然后再加上原来的输入. 分析一下使用 Bottleneck 的一个节省. 参数量的话，原本是 Residual Block 是 \\(2\\times 3^2c^2=18c^2\\)，而现在是 \\(c^2/4+9c^2/16+c^2/4=17/16c^2\\)；而空间的话，原本是 \\(2w^2c\\)，而现在是 \\(w^2(c/4+c/4+c)=3/2w^2c\\). 这个对加深层数有很大的帮助！ 于是 Resnet 用诸多的技巧成功加深了深度. Mobile Net 尝试解决的问题是如何在边缘设备上实现这些东西，比如手机，比如自动驾驶. 在自动驾驶上，不仅空间受到限制，推理的时间也非常短（要在 0.1s 完成一次推理，只能最多有若干亿的参数）. 我们也可以尝试去搜索出一个比较好的简化的网络. Neural Architecture Search：将网络的变化看作强化学习中的动作，然后做试训练的结果得到 Reward. 但这也是一个很复杂的事情. Segmentation 将有相同语义的 pixel 给 group 成同一个 mask. Semantic Segmentation：将不同的语义给分割开来. Instance Segmentation：要把不同的个体实例给分割开来. 两者结合起来就是既要分割开来又要追求每个个体的语义. 语义分割的训练集由于标注代价高，所以类别会很少. Auto-Encoder：将输入 \\(x\\) 经过 Encoder，维数降低变成 \\(z\\)，然后再经过 Decoder 得到 \\(\\tilde x\\). 本质是因为原始 \\(x\\) 存在大量冗余. 于是我们提取出高度凝练的信息，就能尽量还原出输入. 如果 \\(\\dim z\\) 太小，那么可能会出现很多不可逆的信息丢失. 这样的东西可以用来处理语义分割. 而且由于语义分割的输出只需要每个 pixel 做分类，所以确实不怕信息的压缩. Encoder 是一种下采样（Downsampling）. Pooling 就是一种 Rule based 的下采样. 而比如 CNN 的带有 Striding 的卷积，就是一种可学习的下采样. 而 Decoder 是上采样（Upsampling）. Rule based，比如 Max Unpooling 就非常不靠谱. 我们考虑如何做可学习的上采样. 反卷积（Transposed Conv）. 我们把 input 的每个 pixel，乘上 \\(3\\times 3\\) 卷积核的九个数上，然后做一个滑动窗口的贡献. 注意比如我们要使用 Stride=2，不然图就不会变大. 本质上是考虑如果将其写成矩阵形式，其中 \\(B\\) 为 kernel. \\(A\\times B=C\\)，那么就是交换要对 kernel 所代表的矩阵做一个转置，就能做 \\(B^TC^T\\) 的矩阵乘法就能得到 \\(A^T\\) 的大小. 这样的好处是，首先 Memory 变小了. 然后每个 Pixel 都有了一个上下文 / 周遭的信息，有一个很大的感受野. 但是有一个问题，这样子就变成纯给一个向量然后生成一张图片了，而这个向量只是拥有一个全局信息而已，就很不靠谱. UNet：添加一个从对应的 Encoder -&gt; Decoder 的 Skip Link，然后 Encoder 的图 concatenate 到 Decoder 的输出. 这样就不需要记住每一个 Pixel 的神秘细枝末节的空间信息，并且也着实让答案变得更精确. 并且也没有显著增加开销. 再看一下如何选择准确率. 我们发现 per pixel 的 \\(\\frac{TP+FN}{TP+FN+TN+FN}\\) accuracy 有一个问题，就是不同类别的占比太不均匀了，出现较多的类别占比也太大（比如草原的照片就 accuracy 全在看草）. 对于每个 Mask，IoU：\\(\\frac{|t\\cap p|}{|t\\cup p|}\\). 其中 \\(t\\) 为 target，\\(p\\) 为 prediction. 于是我们就是对于每个 Mask 去做统计 Accuracy. 然后我们再对于每个类别，按照其所有 Ground Truth 中的所有 Mask 的平均 IoU. 然后再所有类别再平均一下，得到最终一个较为均衡的 IoU，称为 mIoU. mIoU 其实很难. coco 数据集上做到 \\(50\\%\\) 就很不错了. 参考讲义：Lecture 8 - Deep Learning V.pdf","path":"2025/04/09/CVDL-CNN Training3/","date":"04-09","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"CVDL-CNN Training2","text":"Residual Net 当 CNN 变得很深的时候，反而 Train Error 变高了. 一个问题是，30 层的网络其实训一个 identity 是困难的. 那原本 20 层的网络，再经过 30 层的网络，都可能无法保证这 20 层的网络的好的结果能够被保留下来. 于是训练就很困难. 于是就有一个 Residual Link / Skip Link：对于一系列操作 \\(F\\)，原本就是 \\(x\\to F(x)\\)，现在变成 \\(x\\to x+F(x)\\). 最不济 \\(F(x)\\) 学成 \\(0\\) 就好了（由于 Relu 这是极容易的），至少也能保证一个 identity. 这样构成了一个 Residual Block. 这些 Block 可以无害地无限垒叠起来. 而从另一个角度去理解，最上面的梯度要穿过很多层的梯度才能传到最底下，只要有若干个小的后面就梯度没了，导致底下很难训练. 而加上了残差之后梯度可以直接沿着 Skip Link 往下走. 而又另一方面，神经网络足够深的时候，Loss Landscape 也会变得很古神. 而加了残差的就平滑软糯了很多. Overfitting 解决 Overfitting 可以做数据增强. 最简单的是水平的翻转. 除非要分类左右手. 还可以旋转，移动，拉近，模糊，对比度，明亮度，饱和度，仿射变换（模拟其他视角）等. 但是不能做过了. 检测的办法就是随便采样一些图片让人看一看. Regularization. Loss 加上 \\(\\lambda R(W)\\). \\(\\lambda\\) 是一个重要的超参. 其中可以选择 L2 Regularization \\(R(W)=\\sum W_{i,j}^2\\)，以及 L1 Regularization \\(R(W)=\\sum |W_{i,j}|\\). 或者可以将这两个拼起来，\\(R(W)=\\beta W_{i,j}^2+|W_{i,j}|\\). 一个训练的方法是先不加 Regularization，先确保网络 Main Loss 下降，再去调 \\(\\lambda\\). Dropout. Foward 中每次随机 \\(p\\) 概率点设置成 \\(0\\). Dropout 强迫网络能重复表达一个东西，并且尽量避免 co-adaptation. Batchnorm. 很神奇，Batchnorm 也可以减少过拟合. 并且很多时候用了 Batchnorm 让 Dropout 不是特别需要了. 其实 Batchnorm 对模型的表达加了一些限制（迫使网络的输出在 Relu 前服从高斯分布），所以也是一种正则项. 数据增强和 Batchnorm 是很好用的. 但最后一层 FC - SoftMax 中间就不用 BatchNorm 了. 因为最后答案真不是高斯分布. 下面是有关图像分类的东西. 图像分类 KNN 对于空间中的点，选取离自己最近的 \\(k\\) 个关键点，然后按关键点的颜色做 majority vote. 对一些别的东西有点用. 对图片分类没啥用. Softmax 一个对 Sigmoid 的扩展. \\(p_i=\\frac{e^{x_i}}{\\sum e^{x_i}}\\). KL 散度 两个分布 \\(P,Q\\) 的 KL 散度 \\(D(P||Q)=\\sum_x P(x)\\log(\\frac{P(x)}{Q(x)})\\). 化简一下就知道 \\(-\\sum P(x)\\log Q(x)+\\sum P(x)\\log P(x)\\). 令 \\(P\\) 为真实分布，那么 \\(\\sum P(x)\\log P(x)\\) 是常量. 得到交叉熵 \\(-\\sum P(x)\\log Q(x)\\). 参考讲义：Lecture 7- Deep Learning IV","path":"2025/04/02/CVDL-CNN Training2/","date":"04-02","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"CVDL-CNN Training","text":"相比于全连接层，CNN 有的优势为一方面参数量较少，一方面有 Equivariance. 而另一方面，全连接层由于过于坑坑洼洼所以导致虽说表达能力强，但是优化起来反而效果并不好. 区分 Iteration 和 Epoch：一个 Iteration 是一次 Batch 的操作，每个 Iteration 会更新一次参数. 而一个 Epoch 的经典定义是过一整遍 Data，相当于 \\(N/B\\) 次 Iteration. 每过一个 Epoch，会画 Loss Curve，测 Validation，并且保存模型（checkpoint）. 但是也不用墨守成规. 大一点的模型就可以每个小时做一次上述的工作. 并且对于特别大的模型，可能就只能完整过一遍数据. 那么就可以重新定义 Epoch. Data Preparation Mini-batch SGD：每次 Sample 一个批量的数据，然后用这些数据作 Forward 推理，计算损失函数，然后再做 Backprop 得到梯度，然后更新参数. 注意需要对数据作 Shuffle 再将数据按顺序分为若干个 batch. 因为原本训练数据可能和顺序有强烈关系（比如分类），导致一开始的几个 batch 全都是猫或者全都是狗，那么就很不对. 我们希望每一个 Batch 的梯度的期望和整个数据的梯度的期望是相同的，只不过噪声大一些，而并非偏离. 分 batch 的时候是在 data loader 中干的活. 然后我们要先对数据做一些预处理. Zero-center：数据分部一般中心都不是原点. 而考虑到 ReLU 对是否 &gt;0 很敏感，所以我肯定还是希望数据在正负间均匀分布. 所以我们可以让数据减掉平均值使得期望在 \\(0\\). Normalize Data：我们还可以让每一个维度除以它的标准差，使得每个维度大致服从高斯分布. 这样就能防止某个维度特别大导致 learning rate 就不能兼顾. Normalization 的优势的另一个解释，是就拿最简单的二分类为例，如果数据点都在离原点比较远，那么稍微动一下参数就可能导致在数据点的位置变化很大，导致优化起来困难. 但是要注意一件事情，就是不能导致 Information Loss. 若这些信息是最终任务所需要的，而却被 Normalize 掉了，那么就弄巧成拙了. 于是我们再看一下上面的预处理操作. 这个 Mean 应当对所有数据所有像素点，每个通道求出一个 Mean 去减掉. 一方面如果每张图片都求 Mean 那么会导致纯色图片直接没了，另一方面如果每个像素求 Mean 那么导致丧失平移不变性. 所以我们要求的是 Per-Channel 的 Mean 和 Std. 但这样的 Per-Channel 其实也是有 Information Loss 的. 比如假如数据集中红色非常不重要，所有的点 R 值都差不多，std 很小，但你做了上述操作就导致放的特别大，图的信息其实就扭曲了. 所以要这个要对不同的数据集做具体分析，有些时候除掉 std 反而是反作用. Weight Initialization 我们一个简单的想法就是一个固定方差的期望为 \\(0\\) 的 Gaussian 分布. 这对于比较浅的网络没啥问题. 但是我们加深网络. 假如我们使用 tanh 激活函数，那么会有一个问题，就是越往深处跑，input 的分布越来越窄，故梯度的分布也越来越窄. 到后面梯度会变得很小. 如果增加 std 呢？那么会导致第一层数据绝大多数都分布在 \\(\\pm 1\\)，然后到后面就也持续保持在很大的位置. 所以减少 std 会导致后面分布太窄，增加 std 会导致第一层分布都集中在两端. 所以其实问题就是，第一层 std 不能太大，而后面层 std 要适当增大. 这就引出了 Xavier Initialization. 对于 \\(I\\times O\\) 的层，那么取 std 为 \\(\\sqrt{\\frac{1}{I}}\\). 这可以使得每一层的方差期望大概不变. 一个简单的推导是 \\(Var(y)=Var(\\sum x_iw_i)=I\\times Var(x_iw_i)=IVar(x_i)Var(w_i))\\)，所以我们希望 \\(Var(w_i)=\\frac{1}{I}\\)，所以取 std 为 \\(\\frac{1}{\\sqrt{I}}\\). 其实我们上面没考虑到 TanH. 而 tanh 本来就是压缩 Variance 的所以确实还会减少，但速率明显变慢. 而对于卷积层，上面的参数 \\(I=k^2\\times c\\)，其中 \\(c\\) 为 input channels. 但是 ReLU 就不对了. 因为 ReLU 会改变均值并删掉 &lt;0 的部分，于是对分布的 variance 会每次 /2. 所以不能和上面一样忽略 tanh 的影响. 所以我们需要做一个修正，具体而言其实取 std 为 \\(\\sqrt{\\frac{2}{I}}\\) 就行啦. 这是 He 在 2015 得到的一则结果，极大提升了使用 ReLU 的 CNN 的稳定性. 其实 Initialization 是可以有很多事情的活. 但就先这样了. Optimization 有两个重要的东西，Optimizer 和 Learning Rate. SGD：最简单的减少梯度乘上 Learning Rate. 但是有一些问题. 一个问题是不同维度间太不统一了. 意思就是，比如我们在一个扁扁的椭圆的等高线上，那么纵向梯度大而横向梯度小，但是我们却最主要的方向是需要横向移动. 训练的时候纵向的抖动就特别严重. 第二个问题就是鞍点（Saddle Points），即导数为 \\(0\\) 但是并非极值. 并且在高维的情况下这是很普遍的，会导致 loss 在平台若干时间，然后运气好就从逃逸方向逃出来了，很神秘的“顿悟”感. 第三个问题就是小批量噪声很多. 第四个问题就是为我们一方面希望在不好的 Local minima 跳出去，而又希望在好的地方停留，是矛盾的. 一个改善方法是加 Momentum. 假如我们上一步的步伐是 \\(v_{i-1}\\)，那么我们就让这次走的步伐是梯度再加上 \\(\\rho v_{i-1}\\)（最后乘学习率），其中 \\(\\rho\\) 是一个较大的值，比如 \\(0.99\\). 即保持一个惯性. 比如这就解决了第一个问题. 纵向的 Momentum 会抵消纵向的抖动，而横向的 Momentum 会累积起来. 而第二个问题就可以尽量减少在 Saddle Point 的停留，而增加逃逸速度. 噪声也可以因为 Momentum 从而进行平滑. 加上对 Momentum 的 Bias correction，并且记录前两个时刻的速度方向，那么就得到了一个 Adam. Adam 的 Default 是 \\(\\beta_1=0.9\\)，\\(\\beta_2=0.999\\)，而学习率为 \\(10^{-3}\\). 实际上这些超参数是可以进行理论推导，是可以根据模型和数据找到比较好的超参数的. Learning Rate 大可能会导致最后在谷底附近弹跳，小可能会导致收敛过慢. 一般来说分类问题 \\(10^{-7}\\) 到 \\(10^{-3}\\) 是一个比较合理的学习率. 还有一个方法就是做 Learning Rate 的 Schedule. 一开始设置较大的学习率，之后再变小. 例如 ResNet 就采用了 30, 60, 90 个 Epoch 后分别学习率乘上 \\(0.1\\)，而也有一些别的 Schedule 方法，比如让线性下降，或 \\(\\alpha_t=\\alpha_0/\\sqrt{t}\\) 等等. 但需要注意，这些都是应该主要是要根据 Iteration 做判断. Data size 的变化会改变经典 Epoch 所跑的 Iteration 数. Batch size 变化也会要有所影响. 并且也需要和计算资源相关. 一个来自经验的 Rule：Batch Size 变大 \\(N\\) 倍，Initial Learning Rate 也可以变大 \\(N\\) 倍. 如果 Learning Rate 一开始太大那容易炸. 于是一个办法可能是，有一个 Linear 的 Warm up，一开始先花很快的时间线性上涨. Normalization Underfit 一般有两个原因：模型 Capacity 太小，以及学得不够好. 有两个减少 Underfit 的办法. 第一个是 Batch Norm. Batch Norm 插入在线性层和激活函数中间. 形成 L-B-R 结构. 每个 Batch Norm 层有两个可学习的参数 \\(\\beta\\) 和 \\(\\gamma\\). 对于 Input \\(x\\)，计算出每个 Channel 的均值 \\(\\mu_j\\) 和方差 \\(\\sigma^2_j\\). 然后我们就能通过这两个来获得归一化之后的 \\(\\hat x_{i,j}\\). 但归一化可能使得模型容量受限. 所以我们要允许设置我们想要的 Bias \\(\\beta\\) 和 std \\(\\gamma\\). 也就是说 Batch Norm 层输出的结果是 \\(\\gamma\\hat x+\\beta\\). 这两个参数一开始 \\(\\beta=0\\) 而 \\(\\gamma=1\\). 而在测试的时候，使用训练时候见过的 \\(\\mu\\) 和 \\(\\sigma^2\\) 的 Running Mean. 于是在测试的时候，Batch Norm 就变成了一个简单的线性操作. Batch Norm 的好处有：赋予了 ReLU 更强的操控性，让很深的神经网络训练更快更平稳，可以容忍更高的学习率，对 Initialization 更 Robust. 但代价是什么呢？首先 Train 和 Eval 的表现是不一样的. 这在 Batch Size 小的时候是很有问题的. 所以可以发现这会导致 Eval Mode 的影响很不好. Layer Norm 对所有的 Channel 共同算出一个 Mean. 在 CNN 中，不同的 Channel 代表不同的 Feature，Layer Norm 其实表现是不好的. 而 Instance Norm 则对不同的 Channel 不同的 Batch 都分别做归一化. 而 Group Norm 则将 Channel 分成若干个 Group，每个 Group 做 Layer Norm. Group Norm 在 Batch Size 小的情况下，超越了 Batch Norm. 这三个 Norm 的共同特点是没有 Test time 和 Training time 的差异. 参考讲义：Lecture 6 - Deep Learning III","path":"2025/03/26/CVDL-CNN Training/","date":"03-26","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"数分笔记-级数","text":"先考虑所有项都 \\(\\ge 0\\) 的级数. 对于数列 \\(a_n\\)，定义部分和 \\(S_n=\\sum_{i=1}^k a_i\\). 若 \\(\\lim_{n\\to +\\infty}S_n\\) 存在，则称其收敛，否则发散. 正项级数可以发现和广义积分大差不差. 但是其实有一个比较重要的区别：若正项级数收敛则 \\(a_n\\to 0\\)， 但对于广义积分而言，收敛并不代表 \\(f(x)\\to 0\\). 一些最基本的判定，比如 Cauchy 准则，比如比较判别，都是和广义积分没区别的. 下面直接看一些独属 于级数的一些方法. 首先是一个使用情况可能更多一些的比较判别. 考虑 \\(\\frac{a_{n+1}}{a_n}\\le \\frac{b_{n+1}}{b_n}\\). 那么显然 \\(a\\) 应该收敛地比 \\(b\\) 要更好. 于是若 \\(b\\) 收敛则 \\(a\\) 收敛，若 \\(a\\) 发散则 \\(b\\) 发散. 严谨地证明考虑把这一连串的东西乘起来就是朴素的比较判别. 同样我们可以根据 \\(x^l\\) 的收敛性，获得极限版本. 考虑 \\(\\frac{a_{n+1}}{a_n}\\)，上极限若 \\(&lt;1\\) 则收敛，下极限 \\(&gt;1\\) 则发散. 例子：\\(a_n=\\frac{x^nn!}{n^n}\\). 比一下发现是 \\(x\\times \\frac{n^n}{(n+1)^n}\\). 转换乘 \\((1-\\frac{1}{n+1})^n\\) 后知道极限即为 \\(xe^{-1}\\). 难点都在处理极限 \\(=1\\) 的情形，即 \\(x=e^{-1}\\). 已知的结论可以告诉我们（运用 Stirling 公式或者数学归纳）\\(e^nn!&gt;n^n\\)，所以求和肯定不收敛. 然后我们来看另一种判别法. 考虑 \\(\\sqrt[n]{a_n}\\) 的极限 \\(l\\). 若上极限 \\(&lt;1\\) 那么就收敛. 否则就算是上极限 \\(&gt;1\\) 也可以知道发散（因为有无穷多个 \\(&gt;1\\) 的元素）. 而前者考虑直接放缩 \\(a_n\\le (l+\\varepsilon)^n&lt;1\\)，而 \\((l+\\varepsilon)^n\\) 之和是收敛的. 实际上，这两个判别法有点关系. 实际上，对于下极限而言 \\(\\lim a_{n+1}/a_n\\le \\lim \\sqrt[n]{a_n}\\)，而上极限则相反. 证明以下极限为例. 假设下极限为 \\(l\\)，那么有一个技巧是证明对于任意 \\(\\varepsilon\\)，\\(\\lim \\sqrt[n]{a_n}\\ge l-\\varepsilon\\). 这就好办了，直接用极限的定义知存在 \\(N_0\\) 使得 \\(n&gt;N_0\\) 时前后比 \\(\\ge l-\\varepsilon\\)，然后还是从最开始往后乘起来就证明好了. 在上述判别法都是 \\(1\\) 无法判断，并且含有阶乘的时候可以尝试 Raabe 判别法. 我们考虑 \\(a_n=\\frac{1}{n^p}\\). 我们很容易发现这个的无论是前后比还是 \\(\\sqrt[n]{a_n}\\) 都是 \\(1\\). 但是我们知道在 \\(p&gt;1\\) 是收敛的而 \\(p\\le 1\\) 发散. 考虑前后比 \\(\\frac{a_n}{a_{n+1}}=(1+\\frac{1}{n})^p=1+\\frac{p}{n}+o(\\frac{1}{n})\\)，如果我们减一后乘上一个 \\(n\\) 就 \\(\\to p\\). 所以 Raabe 判别法的核心就是，求出 \\(\\lim n(\\frac{a_n}{a_{n+1}}-1)\\)，若 \\(&gt;1\\) 收敛，若 \\(&lt;1\\) 发散. 这里可以类似得到上下极限的版本. 但如果 \\(=1\\) 那还是做不了. 这没什么办法. 下面考虑任意项级数. 我们想得到 DA 判别法在级数上的表现. Abel 变换：令 \\(b\\) 的部分和为 \\(B\\)，则 \\(\\sum a_ib_i=\\sum a_i(B_i-B_{i-1})=\\sum B_i(a_i-a_{i+1})+a_nB_n\\)，于是绝对值 \\(\\le (\\sum |a_i-a_{i+1}|+a_n)\\max |B|\\). 若 \\(a\\) 单调，则 \\(\\le (2|a_n|+|a_1|)\\max |B_k|\\). 这立刻得到 Abel 判别法：若 \\(a\\) 单调有界，\\(\\sum b\\) 收敛，则 \\(\\sum a_ib_i\\) 收敛. 我们也可以得到 Dirichlet 判别法：若 \\(a\\) 单调收敛至 \\(0\\)，\\(\\sum b\\) 有界，则同样 \\(\\sum a_ib_i\\) 收敛. 严谨的证明可以通过 Cauchy 准则得到. 还有一个 Leibnitz 级数. 如果 \\(a\\) 单调收敛至 \\(0\\)，则 \\(\\sum (-1)^{n-1}a_n\\) 收敛. 这是显然的.","path":"2025/03/26/数分笔记-级数/","date":"03-26","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"高代笔记-四元数","text":"令 \\(\\mathbb H\\) 为带有基 \\(1,i,j,k\\) 的 \\(\\mathbb R-\\)向量空间. 并携带有运算 \\(\\forall x\\)，\\(1x=x1=x\\). \\(i^2=j^2=k^2=-1\\). \\(ij=-ji=k\\)，\\(jk=-kj=i\\)，\\(ki=-ik=j\\). 可以发现四元数的加法和乘法成环，但不交换. \\(\\mathbb C\\) 可以自然作为子环嵌入四元数. 并且四元数也可以表示成 \\(\\mathbb C-\\)向量空间. 具体而言就是将 \\(q\\in \\mathbb H\\) 表示为 \\(z+jw\\)，其中 \\(z,w\\in \\mathbb C\\). 对于四元数，定义其共轭为 \\(a-bi-cj-dk\\)，迹为 \\(q+\\bar q=2a\\)，范数 \\(N(q)=q\\bar q=a^2+b^2+c^2+d^2\\). 容易发现 \\(\\mathbb H\\setminus\\{0\\}\\) 是除环，并且 \\(q^{-1}=N(q)^{-1}\\bar q\\). 定义 \\(\\mathbb H_0\\) 为满足 \\(Tr(q)=0\\)，即 \\(a=0\\) 的 \\(\\mathbb H\\) 的子空间，则可以将 \\(\\mathbb R^3\\) 等同于 \\(\\mathbb H_0\\). 对于 \\(x\\in \\mathbb H^{\\times}\\)，容易发现 \\(q\\to xqx^{-1}\\) 为 \\(\\mathbb H_0\\) 的自同构，记为 \\(R_x\\). 显然 \\(\\det R_x=1\\). 所以我们可以将三维空间的旋转和 \\(\\mathbb H^{\\times}\\) 做一个对应. 并且 \\(R_xR_y=R_{xy}\\) 实际上，任意三维空间的旋转 \\(T\\)，都能找到 \\(x\\in \\mathbb H^{\\times}\\) 使得 \\(N(x)=1\\) 且 \\(T=R_x\\) 且 \\(x\\) 精确到乘以 \\(\\pm 1\\) 是唯一的. 先看存在性. 考虑回到欧拉角. 如果我们能证明绕轴旋转等价于 \\(R_x\\) 那么通过上面的 \\(R_xR_y=R_{xy}\\) 就能说明 \\(T=R_{x&#39;}\\). 对于任意 \\(N(x)=1\\)，\\(x=\\cos\\theta+\\sin \\theta i\\). 于是我们就能知道 \\(xjx^{-1}=\\cos2\\theta j +\\sin 2\\theta k\\)，而 \\(xkx^{-1}=-\\sin 2\\theta j+\\cos2\\theta k\\)，\\(xix^{-1}=i\\). 所以写成矩阵的形式就发现是一样的. 然后就好了. 然后看唯一性. \\(R_x=R_y\\) 等价于 \\(R_{xy^{-1}}=id\\). 下面只需要证明 \\(R_x=id\\) 等价于 \\(x=\\pm 1\\). 而 \\(R_x=id\\) 蕴含了 \\(xi=ix\\)，\\(xj=jx\\)，\\(zj=jz\\). 故 \\(x=\\pm 1\\). 于是考虑以 \\(u\\) 为转轴（\\(u\\in \\mathbb H^{\\times}\\)），\\(R_{u}(\\theta)\\) 所对应的 \\(R_x\\)，满足 \\(x=\\cos {\\frac{\\theta}{2}} +u\\sin \\frac{\\theta}{2}\\). 这也是一个有若干用处的结果，因为这既避免了旋转矩阵的复杂性，又解决了欧拉角的万向锁问题. 下面给出用四元数解决 Rodrigues 旋转公式的优雅解法. 参考讲义：李文威-代数学讲义","path":"2025/03/23/高代笔记-四元数/","date":"03-23","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"高代笔记-三维旋转与欧拉角","text":"对于单位向量 \\(u\\) 以及一组标架 \\((u,u_2,u_3)\\). 定义 \\(R_u(\\theta)\\) 为以 \\(u\\) 为转轴，转 \\(\\theta\\) 角. 在矩阵形式即为 \\[ R_u(\\theta)=\\left(\\begin{matrix} 1\\\\ &amp; R(\\theta) \\end{matrix}\\right) \\] 但更多情况下，我们希望只以标架为转轴的情况下，从一个标价过渡到另一个标架. 具体而言，对于原来的标价 \\(e_1,e_2,e_3\\)，我们知道旋转是一个 \\(\\det = 1\\) 的正交变换 \\(T\\)，那么令 \\(u=(Te_1,Te_2,Te_3)\\)，那么旋转其实将标架 \\(e\\) 旋转至标架 \\(u\\). 我们想用 \\(R_{x}(\\theta)\\) 的合成来表达出这个旋转. 在具体使用中，有两种表示方法. 第一种是内在的描述. 我们的旋转是从“物体”的内在的，主观的角度去描述的，即所有转轴都为当前过渡到的标架中的一个轴. 而第二种是外在的描述，即转轴为 \\(e_1,e_2,e_3\\). 可以说明我们需要三步的轴旋转达成这个目标. 我们先看第一种内在的描述. 首先需要注意到关键在于，只有以 \\(x,y\\) 构成平面的的法向量 \\(f\\) 为转轴可以将 \\(x\\) 转到 \\(y\\).那么我们的想法是，前两步将 \\(e_3\\) 转到 \\(u_3\\)，那么最后一步显然是可以转成功的. 问题是如何将 \\(e_3\\) 转到 \\(u_3\\). 我们考虑求出 \\(e_3,u_3\\) 平面的法向量 \\(f_2\\)，那么以 \\(f_2\\) 为轴就可以将 \\(e_3\\) 转到 \\(u_3\\). 又由于 \\(f_2,u_2\\) 垂直于 \\(e_3\\) 所以第一步就是将 \\(e_2\\) 转至 \\(f_2\\). 这里的 \\(f_2\\) 其实就是平面 \\((e_1,e_2)\\) 与 \\((u_1,u_2)\\) 的交. 思路清楚了，实践起来就简单了. 首先先求出 \\(f_2=e_3\\times u_3\\). 然后取 \\(\\psi\\) 使得 \\(R_{e_3}(\\psi)e_2=f_2\\). 然后取 \\(\\theta\\) 使得 \\(R_{f_2}(\\theta)e_3=u_3\\). 最后取 \\(\\varphi\\) 使得 \\(R_{u_3}(\\varphi)e_1=u_1\\). 那么旋转就是这三者的合成. 我们称旋转由 Euler 角 \\((\\varphi,\\theta,\\psi)\\) 描述. 即 \\(T=R_{u_3}(\\varphi)R_{f_2}(\\theta)R_{e_3}(\\psi)\\). 然后我们再看看外在版本. 直接给出式子，是 \\(T=R_{e_3}(\\psi)R_{e_2}(\\theta)R_{e_1}(\\varphi)\\). 顺序刚好反了过来. 引理：令 \\(\\epsilon=\\det P\\)，则 \\(R_{Pu}(\\epsilon\\theta)=PR_u(\\theta)P^{-1}\\). 即以 \\(Pu\\) 为转轴的旋转，相当于先换基为 \\(P\\)，再以 \\(u\\) 为转轴的旋转. 感性上很容易理解. 用了上述引理之后证明其实也就是暴力展开了. 首先 \\(f_2=R_{e_3}(\\psi)e_2\\)，故 \\(R_{f_2(\\theta)}=R_{e_3}(\\psi)R_{e_2}(\\theta)R^{-1}_{e_3}(\\psi)\\). 而 \\(u_3=R_{f_2}(\\theta)e_3=R_{e_3}(\\psi)R_{e_2}(\\theta)e_3\\)（绕 \\(e_3\\) 转显然不会改变 \\(e_3\\)），于是 \\(R_{u_3}(\\varphi)=R_{e_3}(\\psi)R_{e_2}(\\theta)R_{e_3}(\\varphi)R_{e_3}(\\theta)^{-1}R_{e_3}(\\psi)^{-1}\\)，然后代入即证. 万向节锁：以某些方式在局部上连续地变动欧拉角，使得旋转不变. 例如 \\(\\theta=0\\) 时旋转只与 \\(\\varphi+\\psi\\) 有关. 参考讲义：李文威-代数学讲义","path":"2025/03/23/高代笔记-三维旋转和欧拉角/","date":"03-23","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"程设笔记-Locally Sensitive Hashing","text":"Hamming Distance 问题：给定 \\(d\\) 维 01 向量集合和若干次问询，每次问询求出最小 Hamming Distance 的 \\(c-\\)近似. 人类智慧：在对字符串做随机排列意义下，Hamming Distance 小意味着 LCP 有较高概率大. 所以考虑随机 \\(M\\) 个排列，然后对于每个排列，问询时求出 LCP 最大的。然后比较这 \\(M\\) 则结果即可. \\(M\\) 要取在 \\(O(n^{\\frac{1}{c}})\\). 下面是分析. 令答案为 \\(r\\)，\\(p_1=1-r/d\\)，\\(p_2=1-cr/d\\)，\\(k=\\log_{p_2}(1/n)\\). 那么对于集合中的 \\(q&#39;\\) 和问询点 \\(q\\)，若距离 \\(&gt;cr\\)，则在两者 LCP \\(\\ge k\\) 的概率至多 \\(p_2^k=1/poly(n)\\). 而另一方面，最优点和询问点 LCP \\(\\ge k\\) 的概率为 \\(p_1^k=1/n^{\\frac{1}{c}}\\). 于是令 \\(M=O(n^\\frac{1}{c})\\) 就能大概率使得存在一个排列使得两点 LCP \\(\\ge k\\). Sim Hash Cosine Similarity：对于两个向量 \\(x,y\\)，定义他们的 Cosine Similarity 为两者的夹角的 Cosine 值，即 \\(\\frac{x\\cdot y}{|x||y|}\\). 我们想要找到一个函数 \\(H(x)\\)，使得 \\(Pr(H(x)=H(y))\\) 能反应两者的距离. 考虑取随机向量 \\(w\\)，令 \\(H(x)=\\operatorname{sgn}(w\\cdot x)\\). 分析一下即可知道 \\(Pr(H(x)=H(y))\\) 等于 \\(1-\\theta/\\pi\\). 若我们随机取 \\(T\\) 个独立的这样的 \\(H(x)\\)，那么每个向量就变成了一个 01 向量，然后距离也就转化为了上面的 Hamming 距离. Locality Sensitive Hashing 给出一个一般化的 Locality Sensitive Hashing： 在 \\({\\mathbb R}^{d}\\)向量空间中，LSH 为一族随机哈希函数 \\(H\\)，满足： \\(||x-y||\\le r\\) 的 \\(x,y\\) 有 \\(Pr(H(x)=H(y))\\ge p_1\\). \\(||x-y||\\ge cr\\) 的 \\(x,r\\) 有 \\(Pr(H(x)=H(y))\\le p_2\\). 对于这样的 Hashing，定义 \\(\\rho=\\frac{\\log p_1}{\\log p_2}\\). 那么有了这个 Hashing，我们就能在 \\(O(n^{1+\\rho})\\) 预处理 \\(O(n^{\\rho})\\) 查询的时间复杂度下解决 \\(c-\\)近似的判定问题. 所以 LSH 的目标就是设计一个哈希，使得 \\(\\rho\\) 尽量小. 上面提到的 Hamming 空间的 LSH 就有 \\(\\rho\\le O(1/c)\\). 而比如单位球空间上欧式空间中，若使用上述的 Sim Hash，那么也有 \\(\\rho\\le O(1/c)\\). 一个更好的算法是，对向量做随机旋转（即被随机旋转矩阵 \\(M\\) 作用），然后返回最近的标准基单位向量. 然后若干个这样的独立的 LSH 拼接得到最终的 LSH. 这样的 \\(\\rho\\le O(1/c^2)\\).","path":"2025/03/22/程设笔记-Locally Sensitive Hashing/","date":"03-22","excerpt":"","tags":[{"name":"程设","slug":"程设","permalink":"http://lgswdn.github.io/tags/%E7%A8%8B%E8%AE%BE/"}]},{"title":"数分笔记-瑕积分","text":"若一个点在任何一个小领域内都无界，则称这个点是瑕点. 考虑最经典的瑕积分 \\(\\int_0^1\\frac{dx}{x^p}\\). 带入定义得应该等于 \\(\\lim_{\\delta\\to 0}(\\frac{1}{1-p}x^{1-p})|_{\\delta}^1\\). 那么在 \\(p\\ge 1\\) 时发散，而 \\(p&lt;1\\) 时收敛且等于 \\(\\frac{1}{1-p}\\). 注意这和 广义积分的情景恰好相反. 广义积分的 Cauchy 准则，放缩，极限，DA 判别法对于瑕积分同样适用. 下面来看两个比较经典的瑕积分的例子. Beta 函数：\\(\\Beta(p,q)=\\int_0^{1} x^{p-1}(1-x)^{q-1}dx\\). 处理 Beta 函数需要拆分成两段 \\(\\int_0^\\frac{1}{2}\\) 和 \\(\\int_\\frac{1}{2}^{1}\\). 第一段我们知道要满足 \\(p&gt;0\\) 才收敛，而第二段要 \\(q&gt;0\\) 才收敛. Gamma 函数：\\(\\Gamma(\\alpha)=\\int_0^{+\\infty}x^{\\alpha-1}e^{-x}dx\\). 还是拆成 \\(\\int_0^1\\) 和 \\(\\int_1^{+\\infty}\\). 对于后者，取 \\(g(x)=\\frac{1}{x^2}\\) 可以得知 \\(f/g\\to 0\\)，故收敛. 而对于前者，取 \\(g(x)=\\frac{1}{x^{1-\\alpha}}\\) 可以得知 \\(f/g\\to 1\\)，故要求 \\(\\alpha&gt;0\\). 如果我们对 Gamma 函数做分部积分，可以得到 \\(\\Gamma(\\alpha+1)=\\alpha\\Gamma(\\alpha)\\)，故可以看作一个实数域上的阶乘的拓展.","path":"2025/03/19/数分笔记-瑕积分/","date":"03-19","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"数分笔记-广义积分","text":"定义 \\(\\int_a^{+\\infty}f(x)dx\\) 为 \\(\\lim_{A\\to +\\infty}\\int_a^A f(x)dx\\). 于是我们就可以定义收敛和发散. 并且如果上下都是 \\(\\infty\\) 那么要求从 \\(\\int_0^{+\\infty}\\) 和 \\(\\int_{-\\infty}^{0}\\) 都收敛. 不然你平移一下可能敛散性就变了. 但是如果 \\(\\lim_{A\\to +\\infty}\\int_{-A}^Af(x)dx\\) 收敛那么有时候也有用，称其为在主值意义下收敛. 这个不是我们主要研究的对象，还是回到最本质的这个广义积分上来. 判断敛散性有若干方法. 第一种是直接求出原函数. 这种方法我们可以得到两个相当重要的敛散性结论： \\(\\int_1^{+\\infty}\\frac{dx}{x^p}\\) 在 \\(p&gt;1\\) 时收敛，\\(p\\le 1\\) 时发散. \\(\\int_2^{+\\infty}\\frac{dx}{x(\\ln_x)^p}\\) 在 \\(p &gt;1\\) 时收敛，\\(p\\le 1\\) 时发散. 同样，判断极限是否收敛显然也可以使用 Cauchy 准则. 即若对于任意 \\(\\varepsilon\\) 存在 \\(\\hat A\\) 使得 \\(\\forall a,b&gt;\\hat A\\) 有 \\(|\\int_a^bf(x)dx|&lt;\\varepsilon\\) 那么就有 \\(f\\) 收敛. 这给我们带来一个显而易见的推论，就是若 \\(|f(x)|\\le g(x)\\)，则若 \\(\\int g\\) 收敛则 \\(\\int f\\) 收敛. 证明由 Cauchy 准则直接得到. 下面是这个放缩的两个例子. \\(\\int_0^{+\\infty} e^{-x^2}dx\\). 放缩成 \\(\\int_0^{+\\infty} e^{-x}dx\\). \\(\\int_0^{+\\infty}\\frac{\\cos x\\sin \\frac{1}{x}}{\\sqrt{x}}dx\\). 将 \\(\\cos x\\) 放缩掉后，再将 \\(\\sin \\frac{1}{x}\\) 放缩成 \\(\\frac{1}{x}\\)，变成 \\(\\int_0^{+\\infty}\\frac{1}{x^{\\frac{3}{2}}}\\). 这个结论还可以继续强化. 考虑 \\(\\lim_{x\\to +\\infty} \\frac{f(x)}{g(x)}=a\\). 则若 \\(a\\in(0,+\\infty)\\) 则 \\(f,g\\) 显然敛散性相同. 而若 \\(a=0\\)，则只能知道若 \\(g\\) 敛则 \\(f\\) 敛；\\(a=+\\infty\\) 则若 \\(g\\) 散则 \\(f\\) 散. 下面是这个强化结论的例子. \\(\\int_2^{+\\infty}\\frac{1}{x^p(\\ln x)^q}\\). 若 \\(p&gt;1\\)，则可以取 \\(\\delta&gt;0\\) 使得 \\(p-\\delta&gt;1\\). 取 \\(g(x)=\\frac{1}{x^{p-\\delta}}\\)，于是 \\(f/g=\\frac{1}{x^{\\delta}(\\ln(x))^q}\\to 0\\)，于是收敛. \\(p&lt;1\\) 时同理知发散. \\(p=1\\) 即之前的经典结论. 接下来介绍一个很有用的 Dirichlet - Abel 判别法. 这个判别法基于第二中值定理. \\(\\int_a^Af(x)g(x)dx=f(a)\\int_a^{\\xi}g(x)dx+f(A)\\int_\\xi^Ag(x)dx\\). 我们要让极限收敛，那么只有两种可能：\\(f(a)\\) 有界且 \\(\\int g(x)\\) 收敛；或者 \\(f(a)\\to 0\\) 且 \\(\\int g(x)\\) 有界. 于是我们得到了两种判别法. 注意这两者条件都一定要 \\(f(x)\\) 单调，否则无法使用第二中值定理. （Dirichlet）若 \\(f(x)\\) 单调 \\(\\to 0\\)，且 \\(|\\int_a^{A}g(x)dx|\\le M\\)，则收敛. （Abel）若 \\(f(x)\\) 单调有界，且 \\(\\int_a^{+\\infty}g(x)dx\\) 收敛，则收敛. 下面看几个使用 DA 判别法的例子. \\(\\int_1^{+\\infty}\\frac{\\sin x}{x}\\). 令 \\(f(x)=\\frac{1}{x}\\)，\\(g(x)=\\sin x\\). 使用 Dirichlet 判别法知收敛. \\(\\int_1^{+\\infty}\\frac{\\sin x\\arctan x}{x}\\). 令 \\(f(x)=\\arctan x\\)，\\(g(x)=\\frac{\\sin x}{x}\\). 使用 Abel 判别法知收敛. 绝对收敛和条件收敛：如果 \\(\\int_a^{+\\infty}|f(x)|dx\\) 收敛，则称 \\(f(x)\\) 绝对收敛. 否则如果收敛称为条件收敛. 一个例子是 \\(\\frac{\\sin x}{x}\\) 并非绝对收敛. 考虑 \\(|\\frac{\\sin x}{x}|\\ge \\frac{\\sin^2 x}{x}=\\frac{1-\\cos 2x}{2x}\\). \\(\\frac{1}{2x}\\) 发散而 \\(\\frac{\\cos 2x}{2x}\\) 收敛，故不收敛. 一个比较复杂的例子：\\(I=\\int_1^{+\\infty}\\ln(1+\\frac{\\sin x}{x^p})\\) 的敛散性. 考虑泰勒展开. \\(I=\\int_1^{+\\infty}(\\frac{\\sin x}{x^p}-\\frac{\\sin^2x}{2x^{2p}}+o(\\frac{1}{x^{2p}}))dx\\). 将 \\(\\sin^2 x\\) 化成 \\(\\frac{1-\\cos 2x}{2}\\) 后可以将其写成 \\(I_1+I_2+I_3\\) 的形式，其中 \\(I_1=\\frac{\\sin xdx}{x^p}\\)，\\(I_2=\\frac{\\cos2xdx}{4x^{2p}}\\)，\\(I_3=(\\frac{1}{4}-o(1))\\frac{dx}{x^{2p}}\\). 在 \\(p\\in(0,\\frac{1}{2}]\\) 时 \\(I_1,I_2\\) 收敛而 \\(I_3\\) 发散，故发散；\\(p\\in(\\frac{1}{2},1]\\) 时 \\(I_1\\) 条件收敛，\\(I_2,I_3\\) 绝对收敛，故条件收敛；而 \\(p\\ge 1\\) 时绝对收敛.f","path":"2025/03/19/数分笔记-广义积分/","date":"03-19","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"音数笔记-2","text":"一维振动方程 乐器有气（例如管乐器）/弦（弓弦，弹弦，击弦...）/电/体（木琴等）/膜（鼓，卡祖笛）鸣乐器. 弦乐 令 \\(u(x,t)\\) 表示弦上位置 \\(x\\) 在时间 \\(t\\) 的位移. 我们希望知道 \\(u\\). 设弦受到张力 \\(T\\)，线密度为 \\(\\rho\\)，令 \\(c=\\sqrt{T/p}\\) 为波速. 则一维振动方程 \\(\\frac{d^2u}{dt^2}=c^2\\frac{d^2u}{dx^2}\\). 由于弦两端固定，所以 \\(u\\) 有边值条件 \\(u(0,t)=u(L,t)=0\\). 运用神秘数学技巧，解得 \\(u(x,t)=\\sum_{n=1}^{\\infty}(a_n\\cos\\frac{n\\pi c}{L}t+b_n\\sin\\frac{n\\pi c}{L}t)\\sin(\\frac{n\\pi}{L}x)\\).sweq435 这告诉我们其实弦振动产生的声音一定是无限多个声音的叠加. 令 \\(u_n\\) 为第 \\(n\\) 项，令 \\(\\omega_n=\\frac{n\\pi c}{L}\\)，则利用辅助角公式知道 \\(u_n(x,t)=\\sqrt{a_n^2+b_n^2}\\sin(\\omega_nt+\\theta_n)\\sin(\\frac{n\\pi}{L}x)\\). 称 \\(u_n(x,t)\\) 为第 \\(n\\) 个振动模态. 于是第 \\(n\\) 个振动模态的频率为 \\(f_n=\\frac{n}{2L}c\\). 特别地，\\(n=1\\) 时 \\(f_1=\\frac{1}{2L}\\sqrt{\\frac{T}{\\rho}}\\). 这符合我们的直觉：\\(L\\) 越大频率越低，\\(T\\) 越大频率越高，\\(\\rho\\) 越大（粗）频率越低. 但是注意是正比于 $ $. 并且我们发现弦的振动频率虽说有很多，但是都是 \\(f_1\\) 的整数倍. 将 \\(f_1\\) 称为基频，相应的声音称为基因. 而 \\(n&gt;1\\) 的则称为泛音. \\(f_2\\) 为第一泛音，\\(f_3\\) 为第二泛音. 令 \\(f=f_1\\)，则有泛音列（Harmonic series）\\(f,\\ 2f,\\ 3f,\\ ...\\). 对于固定的 \\(n\\)，我们看振幅 \\(\\sqrt{a_n^2+b_n^2}\\sin(\\frac{n\\pi}{L}x)\\). 有些位置振幅为 \\(0\\)，称为波节；而 \\(k=1,3,\\dots\\) 时取到 \\(\\pm 1\\)，称为波腹. 赫尔姆霍兹的泛音列重合理论：音程和谐，相当于泛音列重合多. 管乐 振动主体为空气柱，不满足上述的 \\(u(0,t)=u(L,t)=0\\). 开口为波腹，闭口为波节. 在开管模型（长笛）下，有 \\(\\lambda=2L\\). 于是开管时泛音列 \\(f,2f,3f...\\). 而闭管（单簧管）则 \\(L\\) 只能等于 \\(\\frac{(2k+1)\\lambda}{4}\\)，故泛音列只有 \\(f,3f,5f...\\)，即偶次泛音，并且基频低一倍. 在管乐中有超吹（overblow），即产生泛音列第二项的音. 故长笛超吹得到高八度，单簧管得到高十二度的音. 方波：\\(f(x)=\\frac{4}{\\pi}(\\sin x+\\frac{\\sin 3x}{3}+\\frac{\\sin 5x}{5}+...)\\) 方波和单簧管有所相似，因为泛音列都是只有奇数. 律制 三分损益：从宫（81）开始，乘 4/3 得到徵（108），再乘 2/3 得到商（72），再乘 4/3 得到羽（96），再乘 2/3 得到角（64）. 这就能得到五省音阶. 接着三分损益得到变宫（85）和变徵（57）. 将宫音对 C，则得到七声音阶宫（C），商（D），角（E），变徵（#F），徵（G），羽（A），变宫（B）. 称为雅乐音阶 / 古音阶. 十二律：做 11 次三分损益（其中某一次要连升两次）得到十二个音. 但是显然做到最后显然回不到宫音，称之为旋宫不归. 五度相生：五度比例为 2:3. 不断往上做纯五度. 做 12 次，然后降 7 个八度，得到 \\((\\frac{3}{2})^{12}(\\frac{1}{2})^7\\). 这理论上应该得到 C，但是最终回到比 1 多一些的地方. 这个叫毕达哥拉斯音差. 音分：\\(1200\\log_2(\\frac{f_2}{f_1})\\). 在平均律中，一个半音为 \\(100\\) 音分. 平均律中大三度比理想大三度高 14 个音分，而大六度则高 16 个音分. 音乐会音高：A4 = 440Hz. 但演奏的时候也可以随便改. 这是变高过的. 贝多芬时代大概都在 420Hz. 这对女高音很不友好.","path":"2025/03/17/音数笔记2/","date":"03-17","excerpt":"","tags":[{"name":"音数","slug":"音数","permalink":"http://lgswdn.github.io/tags/%E9%9F%B3%E6%95%B0/"}]},{"title":"程设笔记-1","text":"随机算法概论 Union Bound：若算法失败概率为 \\(\\delta\\)，则 \\(Q\\) 次询问中存在一次失败的概率可以放缩至 \\(\\le Q\\delta\\). 所以对于多组询问，只需要将失败概率降低至 \\(\\frac{1}{Q}\\) 即可. Markov Inequality：随机变量 \\(X\\ge 0\\)，有 \\(Pr[X\\ge a]\\le E[x]/a\\). 考虑离散的情形会发现这是显然的. 这个告诉我们当我们某个算法有较好的期望结果，那么随机算法正确率也应该不低. Eg：给定无向图求一个边数大于一半的割. 考虑如果随机选择点作为左部的话那么每条边都有 \\(1/2\\) 概率在割中，所以期望是对的，于是直接这么做就好了. Chernoff Bound：一组独立同期望的随机变量 \\(X_1,\\dots,X_n\\)，其中期望 \\(\\mu\\ge t\\). 令 \\(X=\\sum X_i/n\\)，则对任何失败概率 \\(\\delta\\)，有 \\(Pr[|X-\\mu|\\ge \\sqrt{\\frac{\\log(1/\\delta)}{nt}}\\mu]\\le \\delta\\). 即相对误差能被随机次数以 \\(O(\\frac{1}{\\log \\delta})\\) 的量级去控制. 随机排列：一种生成方式是，从 \\(j=n\\to 1\\)，随机 \\(k\\in[1,j]\\) 并交换 \\(a_j,a_k\\). 还有一种方法是每个数随机赋一个 \\([0,1]\\) 实数然后排序. 后者由于每个元素独立采样有局部性，易于动态维护；前者复杂度线性. Rejection Sampling：给定均匀 \\(\\{0,1\\}\\) 生成器 \\(F\\)，如何构造一个 \\(p\\) 概率的 \\(\\{0,1\\}\\) 生成器？考虑 \\(p\\) 的二进制构造，从最高位开始，若 \\(F\\) 得到 \\(1\\) 就返回 \\(p_h\\). 1-median 问题 一个题目：给定 \\(k\\) 维空间的 \\(n\\) 个点 \\(a_i\\)，每次查询给定一个问询点，估计这个问询点到所有点的距离和. 一种特殊情况：所有数据点都到某个 \\(c\\) 的距离在 \\([r,2r]\\). 我们直接随机采样，然后以这些样本来估计答案. 听上去很奇怪. 但是令 \\(f_i\\) 表示第 \\(i\\) 个点的距离，则由于在环上，所以 \\(|f_i-f_j|\\le |a_i-a_j|\\le 2r\\). 我们计算一下误差，由相加误差版本的 Chernoff Bound 知 \\(Pr[X-E[x]\\ge z]\\le 2\\exp(-\\frac{2z^2}{n(r-l)^2})\\)，\\(x_i\\in[l,r]\\). 那么我们就可以知道上述误差在 \\(\\epsilon|P|r\\). 于是取采样数 \\(m=O(1/\\epsilon^2\\log(1\\delta))\\) 即可. 现在考虑总的情况. 我们固定一个点 \\(c\\)，然后令 \\(D=\\sum |c-a_i|\\)，\\(r_0=\\epsilon D/n\\)，\\(r_i=2r_{i-1}\\)，令属于第 \\(i\\) 个环的集合为 \\(P_i\\)，那么就有总误差 \\(\\epsilon\\sum |P_i|r_i\\le \\epsilon \\sum_i \\sum_{x\\in P_i} |x-c|=\\epsilon D\\). 于是尽量取 \\(D\\) 小的 \\(c\\) 即可. 一个完全可行的做法是取中位数. 这个算法也可以推广到 \\(k-median\\)，即给定 \\(k\\) 个闻讯点，然后计算最近邻距离之和. 做法是做聚类，然后每类 做环划分. 哈希 Consistent Hashing 现在一个场景，有很多份文件，每个文件要单独存到一个服务器上. 但是不仅文件会变多，服务器也会变多. 那么考虑如下方法：设计一个任意将文件 &amp; 服务器都能映射到 一个随机整数的函数 \\(h\\)，将 \\(h\\) 的值域写成一个环，那么每个文件和服务器就都能唯一对应着环上的一个位置. 然后我们每次无论是新加服务器还是新加环，只需要像某个叫 Toilet 的 Ucup 题那样做就行了（即对于每个文件，找到其往逆时针走的第一个空着的服务器）. Count-min Sketch 输入整数，然后输入结束过后要近似求出每个数的出现频率. 当然是要小于线性的空间. 直接简述方法：设计若干个哈希，然后求 \\(i\\) 的出现频率只需要取这若干种哈希中的结果的 \\(\\min\\) 即可. 这听上去就非常的对！ 毛估估一下，首先对于一个哈希，设 \\(m\\) 是哈希值域. 期望是 \\(c_x+n/m\\). 然后用 Markov Inequality 得知成功概率为 \\(\\frac{1}{\\epsilon m}\\). 取 \\(m=\\frac{2}{\\epsilon}\\) 就有 \\(1/2\\) 的概率. 于是多哈希几次显然正确. Heavy Hitter求出至少出现了 \\(n/k\\) 次的元素，并且要满足返回的元素每个都不少于 \\(n/k-\\epsilon n\\) 次. 即在保证找出所有 heavy hitter 的情况下，假阳性也足够 heavy. 我们可以直接使用上述的 Count-min sketch 来解决这个问题. Bloom Filter 随机 \\(T\\) 个哈希函数. 每次插入的时候将所有 \\(A[h_j(a_i)]\\)，\\(j\\in[1,T]\\) 设为 \\(1\\). 并且在问询的时候，只有所有 \\(A[h_j(a_i)]=1\\) 才返回 \\(1\\). 容易发现这样只会有小概率的假阳性. 推导暂时省略. \\(A\\) 的大小只需要开 \\(\\frac{n\\ln \\delta^{-1}}{(\\ln 2)^2}\\)，\\(T\\) 开 \\(\\log_2(\\delta^{-1})\\). 就能将失败概率控制在 \\(\\delta\\).","path":"2025/03/16/程设笔记-1/","date":"03-16","excerpt":"","tags":[{"name":"程设","slug":"程设","permalink":"http://lgswdn.github.io/tags/%E7%A8%8B%E8%AE%BE/"}]},{"title":"高代笔记-正交变换","text":"考虑配备标准内积的 \\(\\mathbb R^n\\) 空间中探讨正交变换. 引理：正规算子 \\(T\\)，若 \\(T^k=0\\) 则 \\(T=0\\). 证明考虑放到复数域中，进行酉对角化，然后发现显然所有特征值都为 \\(0\\)，故 \\(T=0\\). 现在考虑 \\(n=2\\) 的空间，即平面. 对于矩阵 \\(\\left(\\begin{matrix}a \\ b \\\\ c \\ d\\end{matrix}\\right)\\)，要让其正交，则必须满足：\\(a^2+c^2=b^2+d^2=1\\)，且 \\(ab+cd=0\\). 取 \\(\\theta\\) 使得 \\(a=\\cos\\theta\\)，\\(c=\\sin \\theta\\)，那么容易发现这个矩阵要么是 \\(\\left(\\begin{matrix}\\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta\\end{matrix}\\right)\\)，要么是 \\(\\left(\\begin{matrix}\\cos \\theta &amp; \\sin \\theta \\\\ \\sin \\theta &amp; -\\cos \\theta\\end{matrix}\\right)\\). 前者行列式为 \\(1\\)，后者为 \\(-1\\). \\(\\det=1\\) 的时候，这个矩阵的意义就是进行一个 \\(\\theta\\) 角度的旋转，记作 \\(R(\\theta)\\). \\(\\det =-1\\) 的话还要附上一个翻转. \\(\\pm 1\\) 的矩阵之间可以互相变换. 令 \\(\\det P=\\epsilon\\in\\{1,-1\\}\\)，则 \\(P^{-1}R(\\theta)P=R(\\epsilon\\theta)\\). 以上就是 \\(n=2\\) 的情形. 现在考虑更高阶的拓展. 任何正交 \\(T\\) 都可以通过换基来表达成如下的形式： \\[ \\left(\\begin{matrix} 1_{a\\times a}\\\\ &amp; (-1)_{b\\times b}\\\\ &amp; &amp; R(\\theta_1)_{2\\times 2}\\\\ &amp; &amp; &amp;...\\\\ &amp; &amp; &amp; &amp;R(\\theta_m)_{2\\times 2} \\end{matrix}\\right) \\] 其中 \\(a+b+2m=n\\). 直观上理解是容易的. 下面来看一看这个很神奇的证明. 先一个引理：\\(T+T^{-1}\\) 自伴且 \\(\\lambda\\in[-2,2]\\). 首先 \\((T+T^{-1})^*=T^*+T^{**}=T+T^{-1}\\) 故显然自伴，于是 \\(\\lambda\\in \\mathbb R\\). 然后 \\(T,T^{-1}\\) 显然可以同步对角化且相加得到的特征值为 \\(\\lambda_T+(\\lambda_T)^{-1}\\). 由于 \\(|\\lambda|=1\\) 所以加起来也肯定 \\(\\in[-2,2]\\). 然后考虑证明该定理. 令 \\(S=T+T^{-1}\\)，令 \\(S\\) 对角化后得到的特征值 \\(\\lambda\\) 和特征子空间 \\(V_{\\lambda}\\)，显然 \\(S,T\\) 交换，故 \\(V_{\\lambda}\\) 也为 \\(T-\\)不变子空间. 于是划归到 \\(\\lambda\\in[-2,2]\\) 的情况. \\(\\lambda=\\pm 2\\) 的情况就是上面的 \\(\\pm1\\). 考虑 \\(\\lambda\\in(-2,2)\\). 那么此时有 \\((T+T^{-1})v=\\lambda v\\)，即 \\(T^2-\\lambda T+1=0\\). 于是 \\(f(x)=x^2-\\lambda x+1\\) 为 \\(T\\) 的最小多项式，故 \\(T\\) 无实特征根. 取任意 \\(v,Tv\\) 构成 \\(V_{\\lambda}\\). 下面只需证明 \\(V_{\\lambda}^{\\perp}\\) 为 \\(T-\\)不变子空间即可完成证明. 首先由于 \\(T^2v=\\lambda Tv-v\\) 故 \\(V_{\\lambda}\\) 为 \\(T-\\)不变子空间. 故 \\(V_{\\lambda}^{\\perp}\\) 为 \\(T^*\\) 不变子空间. 这意味着 \\(T^{-1}(V_\\lambda)=T^*(V\\lambda)\\subseteq V_{\\lambda}^{\\perp}\\)，而由于两者 \\(\\dim\\) 相等故相等. 两侧复合上 \\(T\\) 即可得到 \\(V_{\\lambda}^\\perp=T(V_{\\lambda}^{\\perp})\\). 所以确实不变. 参考讲义：李文威-代数学讲义","path":"2025/03/12/高代笔记-正交变换/","date":"03-12","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"高代笔记-复内积结构3-谱定理与酉对角化","text":"谱定理：对于 \\(T\\in End(V)\\)，\\(T\\) 为正规算子当且仅当 \\(T\\) 可酉对角化，即存在 ONB 使得 \\(Tv_i=\\lambda_iv_i\\). 用矩阵形式表达即 \\(A^{\\dagger}A=AA^\\dagger\\) 当且仅当存在酉矩阵 \\(P\\) 使得 \\(P^\\dagger A P\\) 为复对角矩阵. 从右推到左是显然的. 考虑从左推到右. 谱定理的证明会比实数情景简单一些，因为复数域是代数封闭的. 先考虑 \\(T\\) 自伴的情况. 这个和实数情况是相似的. 引理：\\(V_0\\) 是 \\(T-\\)不变子空间，则 \\(V_0^\\perp\\) 为 \\(T^*-\\) 不变子空间. 证明：若 \\(V_0\\) 为 \\(T-\\)不变，则取 \\(v\\in V_0^{\\perp}\\)，\\((T^*v|v_0)=(v|Tv_0)=0\\)，故 \\(V_0^\\perp\\) 为 \\(T^*-\\)不变子空间. 于是由于复数域是代数闭域所以就随便取一个特征值 \\(\\lambda\\) 和特征向量 \\(v\\)，然后令 \\(V_0=&lt;v&gt;\\)，那么由于 \\(T=T^*\\) 故 \\(T^*\\) 为 \\(V_0^\\perp-\\)不变子空间，于是可以往下归纳. 然后反自伴其实是一样的. 乘一个 \\(i\\) 可以直接规到自伴的情况. 然后考虑普通的正规算子 \\(T\\). 显然我们可以将 \\(T=T_0+T_1\\) 而 \\(T_0\\) 自伴 \\(T_1\\) 反自伴，并且由于 \\(T\\) 正规所以两者交换，故可同步对角化. 于是完成了证明. 关于自伴算子，还有一个性质就是其特征值都是实数. 考虑取任意特征值 \\(\\lambda\\in C\\) 与特征向量 \\(v\\)，考虑 \\((Tv|v)=\\bar\\lambda (v|v)\\)，而另一方面 \\((Tv|v)=(v|Tv)=\\lambda(v|v)\\)，故 \\(\\lambda\\) 一定是实数. 另一个简单的结论是，如果 \\(T\\) 是酉算子那么 \\(|\\lambda_i|=1\\). 这就很显然了. 运用酉对角化，我们很容易将有关实内积空间的一些理论给照搬到复内积上： 对于 Hermite 形式 \\(f\\)，\\(f\\) 正定当且仅当特征值均 \\(&gt;0\\). 证明大概就由于酉对角化也同时构造了二次型的对角化，于是 \\(f\\) 等价于 \\(\\sum \\lambda |x_i|^2\\). \\(TT^*\\) 半正定，且正定当且仅当 \\(T\\) 满. \\((TT^*v|v)=(Tv|Tv)\\ge 0\\). 对于(半)正定 \\(T\\)，存在唯一(半)正定 \\(S\\) 使得 \\(S^2=T\\). 记为 \\(\\sqrt T\\). 酉对角化之后特征值开根. (RU 分解) \\(T\\) 可逆，当且仅当存在唯一 \\(T=RU\\) 使得 \\(R\\) 自伴正定且 \\(U\\) 酉. 取 \\(R=\\sqrt{TT^*}\\)，\\(U=R^{-1}T\\). (SVD 分解) \\(A=PIQ^{\\dagger}\\)，其中 \\(P,Q\\) 为 \\(V,W\\) 单位正交基且 \\(I\\) 为对角矩阵. \\(A^\\dagger A\\) 有实特征值，开根即可. 参考讲义：李文威-代数学讲义","path":"2025/03/12/高代笔记-复内积结构3-谱定理与酉对角化/","date":"03-12","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"数分笔记-定积分的应用","text":"对于参数方程给出的封闭曲线 \\(x=x(t)\\)，\\(y=y(t)\\)，假设随 \\(t\\) 增长曲线沿逆时针方向，那么我们有曲线围成的面积 \\[ S=\\sum_{\\lambda(\\Delta)\\to 0} y_i(x_{i-1}-x_i)=-\\sum_{\\lambda(\\Delta)\\to 0} y_i x&#39;(t_i)\\Delta t_i=-\\int_a^by(t) dx(t) \\] 同理，\\(S=-\\int_a^by(t)dx(t)=\\int_a^bx(t)dy(t)=\\frac{1}2{[\\int_a^bxdy-\\int_a^bydx]}\\). 对于极坐标给的方程 \\(r(\\theta)\\)，可以推得 \\(S=\\int_0^{2\\pi} \\frac{1}{2}r(\\theta)^2d\\theta\\). 现在考虑如何求弧长. 我们先考虑如何定义弧长. 我们分割成若干部分，然后每一个用直线估计. \\[ \\lim_{\\lambda(\\Delta)\\to 0} \\sum \\sqrt{(x_k-x_{k-1})^2+(y_k-y_{k-1})^2}dt \\] 我们声称所有 \\(C^1\\) 的函数都是可求长的，且 \\(=\\int_a^b\\sqrt{x&#39;(t)^2+y&#39;(t)^2}dt\\). 证明考虑由于连续所以一直连续，根号相减小于相减后开根，而相减直接可以被一直连续性控制. 同样可以推得极坐标给出的方程，\\(L=\\int_a^b\\sqrt{r(\\theta)^2+r&#39;(\\theta)^2)}dt\\). 旋转体表面积：对于曲线 \\(x(t),y(t)\\)，考虑将其绕着 \\(x\\) 轴转一圈得到旋转体. 我们关心这个旋转体的表面积. 我们还是分割成若干部分，然后每一段用圆台估计 \\[ \\lim_{\\lambda(\\Delta)\\to 0}\\sum \\pi[y_{k-1}+y_k]\\sqrt{(x_k-x_{k-1})^2+(y_k-y_{k-1})^2}dt \\] 仍然所有 \\(C^1\\) 的曲线都是可以求表面积的，且 \\(=\\int_a^b2\\pi y(t)\\sqrt{x&#39;(t)^2+y&#39;(t)^2}dt\\). 比如我们想要求球的表面积，直接 \\(y=\\sqrt{R^2-x^2}\\)，带进去后发现全都消掉了，剩下一个 \\(4\\pi R^2\\). 投针估 \\(\\pi\\)：平面上若干相隔 \\(h\\) 的平行线，现随机投 \\(N\\) 根长度为 \\(l\\) 的针. 令其中有 \\(n\\) 根针与平行线有相交，则相交概率约为 \\(p=\\frac{n}{N}\\). 现在考虑如何算出 \\(p\\). 令针的中心距离平行线为 \\(y\\in [0,\\frac{h}{2}]\\)，角度为 \\(\\theta\\in[0,\\pi]\\)，那么与线相交即 \\(y\\le \\frac{1}{2}l\\sin \\theta\\)，积起来即 \\(\\int_0^{\\pi}\\frac{1}{2}l\\sin \\theta=\\frac{2l}{h\\pi}\\). 于是就可以估算 \\(\\pi\\).","path":"2025/03/10/数分笔记-定积分的应用/","date":"03-10","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"CVDL-Corner Detection","text":"Image Matching 的一种方法：先从两个照片中找到关键点，然后再做关键点匹配，形成 Correpondance，再去解两个视角的关系. 关键点检测的第一个要求是显著性（Saliency），必须足够具有特征. 第二个要求是重复性（Repeatability），对于不同的图（视角，尺寸等）要能检测出相同的. 第三个是能有足够精确的位置（Accurate localization）. 并且最后希望量足够大（Quantity）. 一种简单的 Keypoint 是 Corner. Corner 的特征是，有多个方向的梯度，而非像 Edge 一样只有正交方向. Harris Corner Detection：将一个滑动窗口，看向各个方向移动，intensity 会不会有什么明显变化. Edge 的话只会有一个方向有极大的变化，而 Corner 在每个方向都有剧烈变化. 考虑定义一个 Window Function. 最简单的用 1 表示在窗口内，0 表示不在窗口内. 对于 \\((x,y)\\)，定义强度差为 \\(D(x,y)=[I(x+u,y+v)-I(x,y)]^2\\)，而又因为对于中心 \\((x_0,y_0)\\)，\\(w_{x,y}=w_{x-x_0,y-y_0}\\)，于是我们强度差总和为 \\(\\sum w(x-x_0,y-y_0)D(x,y)=\\sum w(x_0-x,y_0-y)D(x,y)\\)，写成卷积就是\\((w*D)(x_0,y_0)\\). 上述是对于固定的 \\(u,v\\) 来说的. 如果将 \\(I(x+u,y+v)-I(x,y)\\) 做一阶泰勒近似，得到 \\(uI_x+vI_y\\)，其中 \\(I_x\\) 为 \\(x\\) 方向 \\(I\\) 的偏导数，\\(I_y\\) 为 \\(y\\) 方向的偏导数，那么 \\(D(x,y)=[u,v]\\left[\\begin{matrix}I_x^2(x,y)&amp; I_xI_y(x,y)\\\\I_xI_y(x,y)&amp; I_y^2(x,y)\\end{matrix}\\right][u\\ v]^T\\). 也就是说对于位置 \\((x,y)\\)，\\((u,v)\\) 方向的强度差可以用一个双线性形式表示. 那么我们考虑令中间那个矩阵为 \\(B_{x,y}\\)，那么我们考虑用卷积的形式（注意，\\(B\\) 的每个元素是一个矩阵），那么对于位置 \\(x_0,y_0\\)，这个总和的双线性形式就是 \\(M_{x_0,y_0}=(w*B)_{x_0,y_0}\\). 我们把内部的 \\((x,y)\\) 给提出去（这样矩阵的元素就是算子），可以得到 \\(M=\\left[\\begin{matrix}w*I_x^2&amp; w*I_xI_y\\\\w*I_xI_y&amp; w*I_y^2\\end{matrix}\\right]\\). 上一步是为了方便并行计算. 无论如何，我们已经能得知了 \\((x_0,y_0)\\) 处的双线性形式，现在就是该如何判断一个点是否是 Corner 或者 Edge. 我们要关注的对象是是否对于所有向量 \\(v\\)，\\(vM_{x_0,y_0}v^T\\) 都比较大. 那么我们自然地想到对 \\(M_{x_0,y_0}\\) 进行对角化，得到特征值 \\(\\lambda_1,\\lambda_2\\). 若两者都较大，那么就意味着是 Corner. 若一者相比于另一者过大，那就以为这是 Edge. 否则就是 Flat. 我们想要寻找一个合理的算子来体现这件事情. 首先是要都比较大. Harris Corner Detector 的这套理论选择了使用 \\(\\lambda_1\\lambda_2-2t\\) 作为贡献. 然后是比值要比较大. 这里选择了 \\(\\lambda_1\\lambda_2-2a(\\lambda_1+\\lambda_2)^2&gt;0\\)，可以解出来 \\(a=\\frac{1}{2}(k+1/k)^2\\)，其中 \\(\\lambda_1/\\lambda_2\\in(\\frac{1}{k},k)\\). \\(k=3\\) 的时候有 \\(\\alpha=0.045\\). 然后我们把这两项叠加起来，化简得到 \\(\\theta=\\lambda_1\\lambda_2-\\alpha(\\lambda_1+\\lambda_2)^2-t\\). 有人知道为什么会这么设计这个函数吗？？？？？ 反正我们发现这个其实就是原矩阵的行列式，减去 \\(\\alpha\\) 倍的 Trace 的平方，再减去 \\(t\\). 并且还有一件事情，就是 \\(w\\) 太方方正正了，不满足对旋转的不变性. 所以不如选择一个 Gaussian Filter \\(g\\) 作为 Window Function. 然后我们再看回算子，得到 \\(\\theta=(g(I_x^2)g(I_y^2)-[g(I_xI_y)]^2)-\\alpha[g(I_x^2)+g(I_y^2)]^2-t\\). 总体来说操作其实就像是，写好几个卷积核，然后跑. 然后再做一个 \\(\\theta&gt;0\\) 的 Thresholding 和 Non-maximum Suppression 就好了. 实际上上面所谓的不变性其实应该叫一种等变性 (Equivariance). 即图片发生变化的时候，输出也跟着发生相同的变化. 等变性最基础的定义就是，对于 \\(f:V\\to V\\)（比如卷积，网络）和简单的变换 \\(T\\)（比如旋转，平移），\\(T[f(x)]=f(T(x))\\). 如果是不变性，那么就是 \\(f(x)=f(T(x))\\). 语义类的（比如分类问题）我们希望不变，而检查类的（比如位置方向）我们希望等变. 有关平移的等变性是操演定义的. 考虑如何检查旋转的等变性. 我们将图片先看成连续光滑函数. 如果将图片旋转 \\(\\theta\\)，那么 Gradient 操作，由于 \\(I_x I_y\\) 的对称性显然是等变的. 而卷积的变化在于卷积中心位置绕着旋转中心转了，而我们希望卷积核是旋转不变的. 但实际上真实操作，由于像素的离散性就杜绝了旋转的严格等变的可能性. 并且实现上 Gaussian Filter 写起来也是一个 \\(k\\times k\\) 的卷积核，所以其实旋转不变性也没法保证. 并且卷积对于拉伸也是无法等变的. 参考讲义：Lecture 3 - Classic Vision II 和 Lecture 4 - Deep Learning I.","path":"2025/03/05/CVDL-Corner Detection/","date":"03-05","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"CVDL-Line Fitting","text":"现在有一个 Line Detection 的目标，描绘出图像中的那些直线，例如车道线. 先前的 Edge Detection 有一定的噪声，并且像素点的表示是离散的，我们想更加具体地去得到一条 \\(y=mx+b\\) 的坐标表示. 对于已有的一列点，我们可以通过最小二乘法来做一个 Line Fitting，最小化 \\(E=\\sum (y_i-mx_i-b)^2\\). 其中 \\(y_i-mx_i-b\\) 称为残余（residual），我们希望每个残余都比较小. 而我们对每个残余取 L2-Norm，加起来得到 Energy. 取不同的 Norm，我们改变了优化问题的 Energy Landscape. 考虑以 \\(m,b,E\\) 为三个维度，那确实就形成了一个地势图. 取不同的 Norm 我们要考虑是否最小的时候有一个解析解. Matrix Cookbook：一个记录了大量矩阵微积分公式的开源文档. 但是最小二乘法对于离群点（Outlier）非常的敏感. 离群点会造成非常大的误差. 并且当线垂直的时候就完蛋了. 先解决第二个问题，用 \\(ax+by-d=0\\) 来表述一条线，残余就变成成了 \\((ax_i+by_i-d)\\). 也就是说变成了 \\(\\left(\\begin{matrix}x_1\\ y_1 \\ 1 \\\\...\\\\ x_n\\ y_n\\ 1\\end{matrix}\\right)\\left(\\begin{matrix}a \\\\b\\\\d\\end{matrix}\\right)\\). 但是你发现当后者等于 \\((0\\ 0\\ 0)^T\\) 的时候就直接变成 \\(0\\) 的平凡解，所以我们要添加一个限制. 比如 \\(||h||=1\\)（\\(h=(a\\ b\\ d)^T\\)）. 于是现在的问题是：在 \\(||h||=1\\) 的情况下，最小化 \\(||Ah||\\). 我 们发现实际上对 \\(A\\) 做 SVD 即可（见 高代笔记-SVD与广义逆）. 考虑 \\(Av_i=\\lambda_i v_i\\)，我们选择最小的 \\(\\lambda_i\\) 对应的单位向量，显然是最优的. 在矩阵的语言下，就是 \\(A=UDV^T\\)，取 \\(V\\) 的最后一列作为 \\(v_i\\). 现在要解决对于噪声的鲁棒性（Robustness）问题. L2-Norm 对于小的噪声的鲁棒性是很好的，但是 有 Outlier 就很恐怖了. RANSAC: Random Sample Consensus. 用两个点形成一个假设线，因为这样最小化含有 Outlier 的概率. 然后设置一个阈值，统计与这条线距离超过阈值的点数. 找到 Outlier 数最少的（获得最多群众支持的）假设线. 然后再用这个假设线剔除掉 Outlier. 于是 RANSAC 的具体流程就是：随机选择最少的能确定直线/平面/...的点数，解出假设的方程. 计算 Inlier 的数量 ，若数量足够大，计算这个假设的 Least Square（作为 Tie Breaker）. 不断这么找即可. 而我们发现不同的假设之间没有先后关系，可以取并行化地检验. RANSAC 的超参数：假设的数量，Outlier 的阈值. RANSAC 的好处在于很 general 且很简单，但是缺点在于维度高的话，生成假设的代价太大，且 Outlier 一多就死了. 当 Outlier 多的时候，有另外一种方法叫 Hough Transform. 现在考虑将直线 \\(y=mx+n\\) 看成二维平面上的一个点. 那么原来的点 \\((x,y)\\) 就可以写成 \\(y=mx+n\\)，即 \\(m=\\frac{1}{x}(y-n)\\) 的直线. 然后这些直线画出来然后做一点 Accumulation 和 Voting.比较粗略. 这个的一个好处在于对数量多一些的 Outlier 不那么敏感了. 但是鲁棒性不是很好. 参考讲义：Lecture 3 - Classic Vision II","path":"2025/03/05/CVDL-Line Fitting/","date":"03-05","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"高代笔记-复内积结构2","text":"对于半双线性形式 \\(V\\times V \\to \\mathbb C\\)，\\(\\epsilon\\in\\{-1,1\\}\\)，称该形式为 \\(\\epsilon\\)-Hermite 形式，若 \\(\\epsilon\\overline{B(w,v)}=B(v,w)\\)，即对应的矩阵 \\(\\epsilon A^{\\dagger}=A\\)，将这样的矩阵称为 \\(\\epsilon\\)-Hermite 矩阵. 反 Hermite 矩阵和 Hermite 矩阵实质上只是差了一个 \\(i\\). 所以下面将讨论限定在 Hermite 形式上. 对于非退化的 Hermite 形式 \\(B\\)，\\(T\\) 的左伴随与右伴随等同. 并且若 \\(TT^*=T^*T\\)，则称 \\(T\\) 为正规算子. 同样我们也可以定义出自伴与反自伴算子. 这两者都显然是正规算子. 并且 \\(T\\) 自伴当且仅当 \\(iT\\) 反自伴. Hermite 形式也可以像实的情况一样做分类. 这等同于做变量的替换，也等同于矩阵做合同变换. 于是任何 Hermite 形式都可以化作 \\(\\sum_{i=1}^p |x_i|^2--\\sum_{i=p+1}^{p+q}|x_i|^2\\)，其中 \\(r=p+q\\) 为该 Hermite 形式的秩. 惯性定理也与实情况别无二致. 同样的，我们可以定义 Hermite 内积. 即满足正定性的 \\(V\\) 上的 Hermite 形式. 定义长度 \\(||v||=\\sqrt{(v|v)}\\). 满足 \\((v|w)=0\\) 的向量是正交的，于是我们可以定义正交子集. 正交子集线性无关，因为对于任意 \\(v=\\sum a_iv_i\\)，这个表示一定长成 \\(a_i=\\frac{(v_i|v)}{(v_i|v_i)}\\). 于是我们可以定义正交直和分解. 我们还可以运用 Gram-Shmidt 正交化获得 QR 分解. \\(||v+w||=(v+w|v+w)=||v||^2+||w||^2+(v|w)+(w|v)=||v||^2+||w||^2+2Re(v|w)\\). 这也就意味着 \\((v|w)\\) 由距离确定.（因为 \\(Im(v|w)=Re(iv|w)\\)）. 这也就意味着保距的同构也保内积. 然后有 Cauchy 不等式 \\(|(v|w)|^2\\le (v|v)(w|w)\\). 还是从取等条件入手，\\(0\\le ||v+tw||^2=(v|v)+(tw|tw)+2Re(v|tw)\\)，带入 \\(t=-\\frac{(w|v)}{(w|w)}\\)，得到 \\((tw|tw)=\\bar tt(w|w)=\\frac{(v|w)(w|v)}{(w|w)}\\)，而 \\(2Re(v|tw)=2Re(t(v|w))=-2\\frac{(v|w)(w|v)}{(w|w)}\\). 两者叠加立刻得到 \\((v|v)-\\frac{|(v|w)|^2}{(w|w)}\\ge 0\\)，并且线性相关时取等. 标准复内积：\\((v|w)=\\bar v\\cdot w\\). 任何的有限维复内积都可以通过取单位正交基，取 \\(Tv_i=e_i\\) 以同构于 \\(\\mathbb C^n\\) 上的复内积空间. 定义 Unitary Operator（酉算子）为复内积空间的自同构. 也就是说有 \\(T^{*}=T^{-1}\\). 于是也可以定义酉矩阵为 \\(A^{\\dagger}=A^{-1}\\) 的复矩阵. 参考讲义：李文威-代数学讲义","path":"2025/03/05/高代笔记-复内积结构2/","date":"03-05","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"数分笔记-积分中值定理","text":"积分第一中值定理：\\(f(x)\\in C[a,b]\\)，\\(g(x)\\in R[a,b]\\) 且不变号，则存在 \\(\\xi\\in(a,b)\\)，使得 \\(\\int_a^bf(x)g(x)dx=f(\\xi)\\int_a^bg(x)dx\\). 证明大抵是容易的. 不妨假设 \\(g(x)\\ge 0\\) 且 \\(\\int_a^bg(x)dx\\neq 0\\). 然后由于 \\(m\\le \\int_a^bf(x)dx\\le M\\) 所以用连续函数的介质定理即可. 但是 \\(\\xi\\) 可以取到端点. 这里做若干细致的讨论就可以发现其实完全可以避免取到端点. 一个例子：\\(f(x)&gt;0\\) 单调增，\\(\\in C[a,b]\\)，证明 \\(\\int_a^b xf(x)dx\\ge \\frac{a+b}{2}\\int_a^bf(x)dx\\). 考虑 \\(LHS-RHS=\\int_a^{\\frac{a+b}{2}}(x-\\frac{a+b}{2})f(x)+\\int_{\\frac{a+b}{2}}^b (\\frac{a+b}{2}-x)f(x)\\). 使用中值定理即可知道 \\(LHS-RHS=-A(f(\\xi_1)-f(\\xi_2))\\)，其中 \\(A&gt;0\\)，\\(\\xi_1&lt;\\frac{a+b}{2}&lt;\\xi_2\\)，完成证明. 一个应用是柯西积分余项. 首先对于 \\(f(x)=f(x_0)+\\int_{x_0}^xf&#39;(t)dt=f(x)=f(x_0)-\\int_{x_0}^x f&#39;(t)d(x-t)\\)，运用分部积分即可得知 \\(f(x)=f(x_0)+f&#39;(x_0)(x-x_0)-\\int_{x_0}^x f&#39;&#39;(t)(x-t)dt\\)，而后面那个积分是 \\(\\int_{x_0}^x f&#39;&#39;(t)d(\\frac{1}{2}(x-t)^2)\\)，然后不断这么分部积分下去就可以得到带有柯西积分余项的泰勒展开： \\[ f(x)=\\sum_{i=0}^n \\frac{f^{(i)}(x_0)(x-x_0)^i}{i!} +\\frac{1}{n!}\\int_{x_0}^x f^{(n+1)}(t)(x-t)^ndt \\] 然后运用积分第一中值定理，令 \\(F(t)=f^{(n+1)}(t)(x-t)^n\\)，\\(g(t)=1\\)，那么得到 \\(R_n=\\frac{f^{(n+1)}(\\xi)(x-\\xi)^n(x-x_0)}{n!}\\)，代入 \\(\\xi=x_0+\\theta(x-x_0)\\) 得到 \\(R_n=\\frac{f^{(n+1)}(\\xi)(1-\\theta)^n(x-x_0)^{n+1}}{n!}\\). 即泰勒展开的柯西余项. 下面看积分第二中值定理. 这是一个比较抽象的定理. 设 \\(f(x)\\) 递减且 \\(\\ge 0\\)，\\(g(x)\\in R[a,b]\\)，则存在 \\(\\xi\\) 使得 \\(\\int_a^b f(x)g(x)dx=f(a)\\int_a^\\xi g(x)dx\\). 注意 \\(g(x)\\) 是可变号的. Abel 变换：令 \\(B\\) 为 \\(b\\) 的前缀和，则 \\(\\sum a_ib_i=\\sum a_i(B_i-B_{i-1})=\\sum B_i(a_i-a_{i+1})+a_nB_n\\). 写作积分的形式就知道 \\(\\int_a^b f(x)g(x)=f(b)G(b)+\\lim_{\\lambda(\\delta)\\to 0}(f(x_i)-f(x_{i+1})G(x_i))\\). 根据这个，我们就能知道上式 \\(\\in [mf(a),Mf(a)]\\)，其中 \\(m,M\\) 为 \\(G\\) 的最值. 由于 \\(G\\) 连续，由介值定理就可以证明其正确. 第二中值定理有若干等价描述. 首先 \\(f(x)\\) 递增的情况是对称的，换成 \\(f(b)\\int_\\xi^bg(x)dx\\) 就好了. 然后我们还可以知道如果 \\(f(x)\\) 单调那么 \\(=f(a)\\int_a^\\xi g(x)dx+f(b)\\int_\\xi^bg(x)dx\\)，因为只需要将两边减少 \\(f(b)\\int_a^b g(x)dx\\) 就可以划归到第一种情况. 一个例子，考虑 \\(f(x)=\\int_0^x\\sin\\frac{1}{t}dt\\)，且 \\(f(0)=0\\). 问 \\(f&#39;(0)\\) 存在性. 注意如果使用洛必达那么会发现不存在，意味着这不满足洛必达的使用条件. 这里有些比较技巧性的东西. \\(f(x)=\\lim_{\\delta\\to 0^+}\\int_{\\delta}^x\\sin \\frac{1}{t}dt\\)，然后我们做一下翻转知道 \\(f(x)=\\lim \\int_{\\frac{1}{x}}^{\\frac{1}{\\delta}}\\sin td\\frac{1}{t}\\)，即 \\(\\int_{\\frac{1}{x}}^{\\frac{1}{\\delta}} \\frac{\\sin u}{u^2}du\\)，使用第二中值定理得到 \\(x^2\\int_{\\frac{1}{x}}^\\xi \\sin udu\\le 2x^2\\)，那么除以 \\(x\\) 后便 \\(\\to 0\\). 故 \\(f&#39;(0)=0\\).","path":"2025/03/05/数分笔记-积分中值定理/","date":"03-05","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"音数笔记-1","text":"音乐是什么？因声波的震动而存在（声音），在时间中展现的，通过人类的听觉而引起各种情绪反应和情感反应的艺术门类。这里重要的是，音乐是一个主观的，一种艺术门类。但是这个不一定要是积极的。可以是很搞笑很糟糕的情感体验. 声波是一种纵波，即波与传播方向平行. 声音的物理属性： 音高（pitch） —— 振动频率（frequency） A4 的频率是 440Hz 人的听觉对于频率的感知是非线性的 借助音高可以把音由低到高排成音阶（scale） 力度（dynamics）—— 震动幅度（amplitude） 注意到响度（loudness）是由于鼓膜收到的压强决定的，需要区分. 声压水平（SPL），\\(L_p=20\\log_{10}\\frac{p}{p_0}\\)，\\(p\\) 为实际声压，\\(p_0=20\\mu Pa\\). 单位为分贝（dB）. 时值（duration）—— 时间长度 振幅包络（Envelope） ADSR：Attack（起音），Decay（衰减），Sustain（延迟），Release（释音） 先在 A 从无声达到最大音量，然后经过 D 达到稳定音量，然后经过 S 的时间后，R 恢复无声. 音色（timbre）—— 振动波型（waveform） 频谱图：横坐标表示时间，纵坐标表示频率，用亮度表示音量 把各个频率成分从低到高排列起来形成的序列为 泛音列（Harmonic Series） 根据傅里叶级数那套理论，任何周期函数都可以用若干的简单正弦&amp;余弦函数叠加产生. 做傅里叶变换可以把时域给变换成频域，然后就能区分出这些正弦/余弦波. 传统音乐中使用的噪音主要来源为打击乐器. 打击乐器分为有音高和无音高的. 一些比较搞笑的是柴可夫斯基 1812 序曲中的加农炮. 乐音体系：具有固定音高的全体乐音构成一个集合，称为乐音体系，音列中的各个音称为音级. 每个音级可以有不同音名. 唱名取自赞美诗的每句的首字母. 然后是一些五线谱和记号. 比较简单. 音程：高的音为冠音，低的音为根音（root）. 先后发声的是旋律音程，一起发生的是和声音程. 具体音程怎么表达也很 trivial. 不写了. 毕达哥拉斯理论：振动频率之比越简单，越和谐. Eg：纯五度 3:2，纯四度 4:3，大六度 5:3，大三度 5:4. 而小二度 16:15，增四度 45:32. 拍音理论：三角函数和差化积，对于 \\(\\omega\\) 和 \\(\\omega+\\delta\\) 两个频率的音，\\(\\sin(2\\pi\\omega t)+\\sin(2\\pi(\\omega+\\delta) t)\\)，三角函数和差化积后得到 \\(2\\sin(2\\pi(\\omega+\\frac{\\delta}{2}))\\cos(\\pi \\delta t)\\). 故音量会产生周期变化. 当 \\(\\delta\\) 较小时，就会产生拍音（beat）.","path":"2025/03/03/音数笔记-1/","date":"03-03","excerpt":"","tags":[{"name":"音数","slug":"音数","permalink":"http://lgswdn.github.io/tags/%E9%9F%B3%E6%95%B0/"}]},{"title":"CVDL-Edge Detection","text":"图片可以看作 \\(R^2\\to R^M\\) 的函数. 对于灰度（Grayscale）图，则为 \\(f:[a,b]\\times [c,d]\\to [0,255]\\)，其中后者为 Intensity，有一个 channel. 而对于彩色图片，则可以看作 \\([r(x,y),g(x,y),b(x,y)]\\) 的向量，有三个 channel. 计算机图片一个很大的特点是空间是离散的（像素点），而颜色也是离散的，形成一个网格状（Grid-like）的数据. 灰度图片可以看作一个 \\(H\\times W\\) 的矩阵，而彩色图片则形成了 \\(H\\times W\\times 3\\) 的 tensor. 既然图片是函数，那么我们可以求这个函数的梯度. 但由于是离散的，所以用有限的差分去替代这个梯度. 例如 \\(f\\) 在 \\(x_0\\) 的梯度直接用 \\(\\frac{f(x_0+1,y_0)-f(x_0-1,y_0)}{2}\\) 计算. 梯度的方向由黑向白. 先在 \\(x\\) 上求导，再在 \\(y\\) 上求导，然后可以得到 Gradient Magnitude \\(||\\nabla f||=\\sqrt{(\\frac{df}{dx})^2+(\\frac{df}{dx})^2}\\). 当且仅当一个地方有一个剧烈的颜色变化，即 Edge，才会有很大的 Gradient Magnitude. Gradient 的方向垂直于 Edge. 于是仅用求梯度的方法，我们可以获取图片的边缘相关信息. Filtering：通过原来的 \\(f\\)，进行一系列算子的操作，得到一个新的图片. Filtering 是信号处理的术语，而图片可以视作一个二维的信号. 一个一维离散信号的 Filter 的例子：Moving Average. 取 \\(h_n\\) 为 \\(f_{[n-k,n+k]}\\) 的均值. 想法是噪声独立随机的话，取一个均值就可以把噪声给消去. 这个 Filter 由于是对原信号进行线性组合，所以是一个 Linear Filter. 这个东西可以扩展到一维离散卷积. 取一个数组 \\(g[n]\\)，定义 \\(h[n]=(f*g)[n]=\\sum f[m]g[n-m]\\). 相当于对 \\(g\\) 做翻转后，从左到右做一个邻域的带权相加. 称 \\(g\\) 为卷积核. 考虑离散傅里叶变换 \\(\\mathcal F\\)，根据我们所熟知的 FFT 那套理论，有卷积定理：\\(\\mathcal F(f*g)=\\mathcal F(f)\\cdot \\mathcal F(f)\\)，于是我们将实域的卷积变成了频域的点乘. 理解一个 Filter 干了什么，可以考虑看它做傅里叶变换后的频谱. 而从频域的角度看，Moving Average 的频谱集中在了 \\(0\\) 附近，高频处值很小，所以其实就是一个 Low-pass Filter. 但其实我什么也不懂. FFT，谁会？ 对于任何 Linear Filter \\(\\cal G\\)，一定能够通过卷积的形式来描述. 考虑扩展到二维的图片上. 定义二维的卷积核（Convolution Kernel）\\(g\\)，那么可以定义卷积 \\((f*g)[m,n]=\\sum_{k,l} f[k,l]g[m-k,n-l]\\). 对二维图片做 Moving Average 会让边界变模糊. 一个非线性的例子：二值化 Binarization（via Threshold），即设立一个阈值，并令 \\(h[m,n]=[f[n,m]&gt;\\tau]\\). 下面可以通过上述两个很简单的操作实现 Edge Detection. 这个方法也是至今使用的方法. Edge 的定义：沿着一个方向有剧烈的 Intensity 的变化，并且沿着正交方向变化较小的区域. 注意 Gradient Magnitude 大不等于是 Edge. 比如 Corner 就是沿着多个方向变化都大. Edge Detection 的 Criteria：Precision = \\(\\frac{TP}{TP+FP}\\)，Recall = \\(\\frac{TP}{TP+FN}\\). Localization &amp; Response. 考虑之前做的求梯度. 如果图片有高频的噪声，那么就会变得很搞笑，因为哪里都有巨大的梯度. 所以就要抑制噪声，对整个图片做一个 Low Pass Filter. Gaussian Filter：取 \\(g=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{x^2}{2\\sigma^2})\\) 做卷积. 做傅里叶变换后有 \\(\\cal F(g)=\\exp(-\\frac{\\sigma^2\\omega^2}{2})\\). 首先这个 Filter 满足 \\(0\\) 处很高，且积分为 \\(1\\)（不然会导致整张图片能量产生变化）. 并且这个 Filter 受到 \\(\\sigma\\) 控制. \\(\\sigma\\) 越大，高频抑制作用越大. \\(\\sigma \\to 0\\) 时相当于啥都不做. 而这个 Filter 好就好在不管在哪个域，都是一个高斯函数. 运用 Gaussian Filter 之后，影响 Edge Detection 的高频噪音就被去除了. 我们的步骤就是先卷积，再求导. 卷积求导定律：\\(\\frac{d}{dx}(f*g)=f*\\frac{d}{dx}g\\). 于是可以从两步变成一步. 这本质其实就是因为求导本身也是卷积. 但是由于要求 Magnitude 所以要对 \\(x\\) 和 \\(y\\) 两个维度分开求. 然后再做一个二值化. 但是就算这样，一个 edge 的宽度太大了. 于是我们需要做一个去冗余的操作. Non-Maximal Suppression：对于留下来的点 \\(q\\) 和 gradient magnitude \\(g(q)\\)，沿着 \\(g(q)\\) 找两个邻居 \\(r=q+g(q)\\) 和 \\(p=q-g(q)\\). \\(r,p\\) 未必是整点，所以需要插值计算 \\(g(r)\\)，\\(g(p)\\). 如果 \\(g(q)\\) 为三者中最大值，那么保留，否则抛弃. 插值采用 Bilinear Interpolation. 对于 \\((x,y)\\) 邻域的四个形成正方形的格点，能分割成四个长方形。按照对角的长方形的面积作为权值，进行带权平均. 也相当于先算两个关于 \\(x\\) 轴的投影的点的值通过线性插值算出来的值，在用这两个点对于目标点做一个线性插值. 一个另一个简化的 NMS：先看 Gradient 的方向按照上下左右以及斜上下左右去分类，然后直接比较两个 pixel 的 \\(g\\) 值. 现在我们已经获得了 Pixel 的信息，现在问题是如何将这些 Pixel 给穿起来，形成 Edges. 即 Edge Linking. Hysteresis Thresholding：从高可信的地方开始画线，然后串线的时候可以容忍低可信度. 对于所有通过 NMS 的点，求得平均 magnitude，然后据此设置 maxVal 和 minVal. 具体的原则就是起笔的条件是 &gt;maxVal，断掉的条件是 &lt;minVal. 这个 Edge Detector 的名字是 Canny Edge Detector. 其中有若干超参：Gaussian Filter 的 \\(\\sigma\\)，NMS 的步长，Linking 的 minVal maxVal. 其中 \\(\\sigma\\) 越大，细的 edge 就更可能会被 blur 掉. 参考讲义：Lecture 2 - Classic Vision I.","path":"2025/02/27/CVDL-Edge Detection/","date":"02-27","excerpt":"","tags":[{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"}]},{"title":"数分笔记-定积分的性质与计算","text":"首先一些显然的，比如可加性，以及 \\([a,c]=[a,b]+[b,c]\\) 的就不说了. 考虑一个性质：对于在 \\([a,b]\\) 上 \\(f(x)\\ge 0\\) 的函数，\\(\\int_{a}^b f(x)dx=0\\)，那么有 \\(f(x)\\equiv 0\\). 证明：假如存在 \\(f(x_0)&gt;0\\)，那么存在一个小邻域使得邻域内 \\(f(x)&gt;\\frac{f(x_0)}{2}\\)，那么这段积分也就 \\(&gt;0\\)，矛盾. 然后一个最基本的介值定理：对于连续函数 \\(f\\) 存在 \\(\\xi\\in(a,b)\\) 使得 \\(f(\\xi)=\\frac{1}{b-a}\\int_{a}^b f(x)dx\\). 这其实就是连续函数的介质定理，然后再特判掉全相等的情况. 接下来看两个例子. 令 \\(I_n=\\int_{0}^{\\frac{\\pi}{2}}\\sin^n x\\)，证明 \\(I_n\\to 0\\). 看上去应该是显然的，因为 \\(&lt;\\frac{\\pi}{2}\\) 的部分 \\(\\sin &lt;1\\)，于是当然应该 \\(\\to 0\\). 于是考虑拆成 \\([0,\\frac{\\pi}{2}-\\delta]\\) 和 \\([\\frac{\\pi}{2}-\\delta,\\frac{\\pi}{2}]\\)，那么前者在 \\(N\\) 足够大的时候可以任意小，后者 \\(&lt;\\delta\\). 然后就好了. 积分意义下的 Hölder 不等式. 即 \\(\\int_a^{b}f(x)g(x)dx\\le (\\int_a^b |f(x)|^p)^{\\frac{1}{p}}\\int_a^b |g(x)|^q)^{\\frac{1}{q}}\\)，$p,q $ 且 \\(\\frac{1}{p}+\\frac{1}{q}=1\\). 考虑用积分的定义化为原本的 Hölder 不等式. \\(\\int_a^b f(x)g(x)dx=\\sum f(x_k)g(x_k)\\Delta_k\\). 将其变化为 \\(\\sum (f(x_k)(\\Delta_k)^{\\frac{1}{p}})(g(x_k)(\\Delta_k)^{\\frac{1}{q}})\\) 后直接套用原来的不等式即可. 考虑如何计算定积分. 首先如果存在原函数 \\(F(x)\\) 使得 \\(F&#39;(x)=f(x)\\)，那么就可以简单用 \\(F(b)-F(a)\\) 计算. 证明直接用微分中值定理：\\(\\sum F(x_i)-F(x&#39;_i)=\\sum f(\\xi i)\\Delta_i\\). 但是也有其他的方法直接计算. 分别是运用定积分的换元法和分部积分法. 换元法：定积分的换元法不需要保证 \\(\\phi\\) 函数可逆. 令 \\(\\phi(\\alpha)=a\\)，\\(\\phi(\\beta)=b\\)，直接进行换元即可：\\(\\int_a^bf(x)dx=\\int_{\\alpha}^{\\beta} f(\\phi(t))\\phi&#39;(t)dt\\). 注意这需要 \\(f,\\phi\\in C[a,b]\\). 同样我们有分部积分 \\(\\int_a^b Fg=FG|_a^b-\\int_a^b fG\\)，其中 \\(F,G\\) 分别为 \\(f,g\\) 的变上限积分，即 \\(F(x)=\\int_a^x f(t)dt\\). 如果 \\(f=F&#39;\\) 那么其实就是不定积分的分部积分. 但要严谨证明这个 \\(f\\) 未必是 \\(F\\) 的扩展的办法的话会更加复杂一些. 首先考虑引理 \\(\\lim_{\\lambda(\\Delta)\\to 0}\\sum_{i=1}^nf(\\xi_i)\\int_{x_{i-1}}^{x_i}g(x)dx=\\int_a^b f(x)g(x)dx\\). 考虑将右边写成极限的形式后，有 \\(RHS-LHS\\le|\\lim_{\\lambda(\\Delta)\\to 0} \\sum f(\\xi_i)(\\int_{x_{i-1}}^{x_i}g(x)dx-g(\\xi_i))|\\le\\lim_{\\lambda(\\Delta)\\to 0} M\\sum(\\omega_g)_k\\Delta_k\\to 0\\). 然后观察到 \\(FG|_a^b=\\sum F(x_i)(G(x_i)-G(x_{i-1}))+G(x_{i-1})(F(x_{i}-F(x_{i-1})))\\). 这步实际上就是先化成差分然后强行拆成两个式子. 然后由引理知道这个就等于 \\(\\int_a^bFg+\\int_a^b Gf\\).","path":"2025/02/26/数分笔记-定积分的性质与计算/","date":"02-26","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"高代笔记-复内积结构1","text":"将实内积结构直接扩展到复数域上有一个比较大的问题，就是不能保证 \\((v|v)\\ge 0\\)，于是很多好的性质也就不能保持. 但是也并非没有办法. 注意到对于 \\(z\\in \\mathbb C\\)，\\(\\bar zz\\ge 0\\)，于是我们就思考能否改变一下乘法，让内积的结构同样保持. 对于拥有加法 \\(+\\) 和乘法 \\(\\cdot\\) 的复数域上的向量空间 \\(V\\)，定义一个 \\(\\odot\\) 为 \\(z\\odot v=\\bar z\\cdot v\\). 然后就可以定义 \\(V\\) 的共轭向量空间 \\(\\overline V\\) 为将 \\(\\cdot\\) 替换为 \\(\\odot\\) 的向量空间. 注意到首先 \\(V\\) 做两次共轭还等于自身. 其次 \\(V\\to \\overline V\\) 存在同构，只需要映 \\(v\\) 为 \\(\\bar v\\) 即可（因为 \\(\\overline{z\\cdot v}=\\bar z\\cdot \\bar v=z\\odot \\bar v\\)）. 对于复数域上的向量空间 \\(V\\) 和 \\(W\\)，定义半线性映射为满足满足 \\(T(v_1+v_2)=Tv_1+Tv_2\\)，且 \\(T(zv)=\\bar z Tv\\) 的 \\(V\\to W\\) 的映射. 对于 \\(V=\\mathbb C^n\\)，如果我们取标准基 \\(e_1,\\dots,e_n\\)，那么可以认为半线性映射同构于用一个矩阵来乘 \\(\\bar v\\). 因为 \\(T(zv)=A\\overline{zv}=\\bar zA\\bar v=\\bar zT(v)\\). 半线性映射和线性映射之间用共轭联结. \\(T(zv)=z\\oplus Tv\\)，所以 \\(V\\to W\\) 的半线性映射和 \\(V\\to \\overline W\\) 的线性映射是一回事情. 同理 \\(T(z\\odot v)=zTv\\)，所以和 \\(\\overline V\\to W\\) 的线性映射也是一回事情. 考虑 \\(Hom(V_1,V_2)\\)，即所有 \\(V_1\\to V_2\\) 的线性映射. 我们知道 \\(Hom(V_1,V_2)\\) 应该构成向量空间. 现在考虑 \\(\\overline{Hom(\\overline{V_1},\\overline{V_2})}\\)，这个东西应当等于 \\(Hom(V_1,V_2)\\). 取 \\(T\\in Hom(V_1,V_2)\\)，其肯定 \\(\\in \\overline{Hom(\\overline{V_1},\\overline{V_2})}\\) 的集合，并且满足加法线性也是显然的，于是只需要验证数乘条件，即 \\((z\\odot T)(v)=T(z\\odot v)\\)，那这显然是对的. 然后我们考虑一个叫半双线性形式的东西（Sesquilinear）. 对于所有 \\(V\\times W\\to \\mathbb C\\) 的映射 \\(B\\)，若 \\(B\\) 关于 \\(V\\) 半线性，关于 \\(W\\) 线性，那么称 \\(B\\) 是半双线性形式. 在 \\(\\mathbb C^n\\) 下，取标准基，那么我们有半双线性形式与矩阵的同构，其中 \\(A\\) 映为 \\(v^{\\dagger}Aw\\). 同样，我们可以定义左根和右根和非退化形式. 并且 \\(\\dim V=\\dim W\\) 时，左根 \\(=\\{0\\}\\) 与右根 \\(=\\{0\\}\\) 等价. 证明 Sesquilinear 相关的性质可以直接调用 Bilinear 的性质，因为 \\(B:V\\times W\\to C\\) 和 \\(B&#39;:\\overline V\\times W\\to C\\) 相等. 同样，非退化当且仅当所对应的矩阵 \\(A\\) 可逆. 参考讲义：李文威-代数学讲义","path":"2025/02/25/高代笔记-复内积结构1/","date":"02-25","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"数分笔记-Riemann可积","text":"考虑在 \\([a,b]\\) 上定义的函数 \\(f\\). 我们取一个分割 \\(\\Delta:a=x_0&lt;x_1&lt;\\dots&lt;x_n=b\\)，划分成 \\(n\\) 个段. 然后我们从每段中随便取一个 \\(\\xi_i\\in[x_{i-1},x_i]\\)，作为这个段的代表. 那么就可以得到一个有关类似\"面积\"或\"期望\"的式子 \\(\\sum f(\\xi_i)\\Delta_i\\)，其中 \\(\\Delta_i=x_{i}-x_{i-1}\\). 令 \\(\\lambda(\\Delta)=\\max \\Delta_i\\)，那么在 \\(\\lambda(\\Delta)\\to 0\\) 的时候，\\(\\sum f(\\xi_i)\\Delta_i\\) 就应该趋向于一个真实的\"面积\". 于是，若 \\(\\lambda(\\Delta)\\to 0\\) 的时候，存在极限 \\(\\lim \\sum f(\\xi_i)\\Delta_i\\)，那么就称 \\(f(x)\\) 是 Riemann 可积的. 同时，将这个极限 \\(I\\) 记作 \\(\\int_a^b f(x)dx\\). 关于判断一个函数是否 Riemann 可积，有 Lesbegue 定理：函数在闭区间上 Riemann 可积等价于函数有界且间断点为零测度集. 后者的定义就是说，对于任意 \\(\\varepsilon\\)，存在可数覆盖，将点集覆盖并且总长不超过 \\(\\varepsilon\\). 直观上是很容易理解的. 但是严谨证明是略复杂的. 下面引入一个比较有道理的东西. Darboux 和：上面那个定义给人最不便的地方是 \\(\\xi_i\\) 是任选择的. 那么对于一段 \\([x_{i-1},x_i]\\)，令 \\(m_i\\) 为下确界，\\(M_i\\) 为上确界，那么可以引入达布小和 \\(\\underline{S}(\\Delta)=\\sum_{k=1}^{n}m_k\\Delta_k\\)，以及大和 \\(\\overline{S}(\\Delta)=\\sum_{k=1}^{n}M_k\\Delta_k\\). 若两者能有相同极限 \\(I\\)，那么显然函数就是 Riemann 可积的. 相反，若黎曼可积，那么显然两者也有相同极限 \\(I\\). 先研究有关达布大/小和的性质. 首先对于分割 \\(\\Delta\\)，如果我们将它加细（即多划几刀），那么 \\(\\underline S\\) 会变大，\\(\\overline S\\) 会变小. 这启发我们当 \\(\\Delta\\) 越来越细致，\\(\\lambda(\\Delta)\\to 0\\) 的时候，\\(\\lim \\underline S=\\sup \\underline S\\) 而 \\(\\lim \\overline S=\\inf \\overline S\\). 当然注意这是在 \\(f\\) 有界 \\(M\\) 的情况下. 证明考虑用定义. 对于任意 \\(\\varepsilon\\)，令上确界为 \\(I\\) 由上确界的定义可知存在 \\(\\Delta_0\\) 使得 \\(\\underline S&gt;I-\\frac{\\varepsilon}{2}\\). 令段数为 \\(n\\)，那其他的 \\(\\underline{S}(\\Delta)\\) 最小也不会比这个减掉 \\(2nM\\) 更小了，于是取 \\(\\delta=\\frac{\\varepsilon}{4nM}\\) 即可. 这带来了一个重要的有关 振幅 的引理：令一段的振幅 \\(w_i=M_i-m_i\\)，那么 Riemann 可积等价于对于任意 \\(\\varepsilon\\)，存在 \\(\\Delta\\) 使得 \\(\\sum w_k\\Delta_k&lt;\\epsilon\\). 接下来就是考虑去推导 \\(\\sum w_k\\Delta_k&lt;\\epsilon\\) 和间断点为零测度集之间的等价关系了. 首先前者推后者. 令间断点集合为 \\(E\\). 对于 \\(E\\) 中的点 \\(x\\in E\\)，令 \\(w_x\\) 表示 \\(x\\) 周围小领域的振幅的极限. 令 \\(P_k\\) 为满足 \\(w_k&gt;\\frac{2M}{k}\\) 的点组成的集合. 对于任意 \\(\\delta\\)，我们只需要对任意 \\(k\\) 证明存在一个覆盖使得总长 \\(&lt;\\frac{\\delta}{2^k}\\)，那么就大功告成了. 另一方面，对于任意一个 \\(\\varepsilon\\)，取其相应的 \\(\\Delta\\). 令 \\(Q\\) 为 \\(\\Delta\\) 中包含 \\(P_k\\) 中点的段的集合，那么 \\(\\varepsilon&gt;\\sum w_i\\Delta_i&gt;\\frac{2M}{k}\\sum \\Delta_i\\)，也就是说对于任意的 \\(k\\)，\\(\\sum \\Delta_i\\) 即覆盖总长，完全可以被这样一个 \\(\\varepsilon\\) 给简单控制. 取 \\(\\varepsilon=\\frac{\\delta k}{2^k2M}\\) 即可. 然后后者推前者. 我们的大体想法是，间断点虽振幅不可控但是 $$ 可控，于是运用有限开覆盖的 Lesbegue 数将其控制. 但实际上我们可以推对任意 \\(\\lambda(\\Delta)&lt;\\delta\\) 的 \\(\\Delta\\) 都成立. 对于任意 \\(\\varepsilon\\)，首先对于每个 \\(x\\in E\\) 取小领域 \\(U(x,\\delta_x)\\) 使得总长 \\(&lt;\\frac{\\varepsilon}{4M}\\). 同时，对每个连续点取小领域使得振幅 \\(&lt;\\frac{\\varepsilon}{2(b-a)}\\). 用这些小区间形成 \\([a,b]\\) 的有限开覆盖. 于是存在 Lesbegue 数 \\(\\hat \\delta\\)，使得任 \\(|x_1-x_2|&lt;\\hat\\delta\\) 使得存在开区间同时覆盖 \\(x_1,x_2\\). 那么对于任意 \\(\\lambda(\\Delta)&lt;\\delta\\) 的 \\(\\Delta\\)，任意的分割的段都已经被控制了，于是显然 \\(\\sum w_k\\Delta_k&lt;\\varepsilon\\).","path":"2025/02/24/数分笔记-黎曼可积/","date":"02-24","excerpt":"","tags":[{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"}]},{"title":"AI基笔记-一些简单的数学","text":"协方差 对于变量 \\(x,y\\)，定义协方差 \\(cov(x,y)=E_{x,y}((x-E(x))(y-E(y)))\\)，化简得到 \\(E_{x,y}(xy)-E(x)E(y)\\). 于是对于两个向量变量 \\(\\mathbb{x}\\) 和 \\(\\mathbb y\\)，可定义协方差矩阵 \\(D=E(\\mathbb{x}^{T}\\mathbb{y})-E(\\mathbb x)^{T}E(\\mathbb{y})\\). 最大似然估计（Maximum Likelihood Estimation） 对于模型 \\(\\theta\\)（例如高斯分布中，\\(\\theta=(\\mu,\\delta^2)\\)），\\(p(x\\mid \\theta)\\) 为 \\(x\\) 出现的频率，令独立同分布 (i.i.d) 的数据样本为 \\(X\\)，则似然为 \\(p(X \\mid \\theta)=\\prod p(x \\mid \\theta)\\). 但是乘法的话由于 \\(p\\) 太小所以太掉精度了，所以我们整体取 $$ 后也就变成了相加. 于是我们希望求得的 \\(\\theta_{MLE}\\) 就是 \\(\\arg \\max_{\\theta} \\sum_{i=1}^{n} \\log p(x_i\\mid \\theta)\\). 对于高斯分布的话求个偏导就好了。最终推导出来就是我们所熟知的均值和方差. 高斯分布 \\(N(\\mu,\\sigma^2)\\) 的概率密度函数为 \\(f(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\). 最大后验概率（Maximum A Posterior） 将参数 \\(\\theta\\) 视为随机变量，从先验分布 \\(p(\\theta)\\) 中采样而来。 我们希望最大化后验概率 \\(p(\\theta \\mid X)\\)，即 \\(\\arg\\max_\\theta p(\\theta \\mid X)=\\arg\\max_\\theta p(\\theta \\mid x)p(\\theta)\\). 信息论（Information Theory） 对于事件 \\(x\\)，定义事件 \\(x\\) 的自信息 \\(I(x)=-\\log p(x)\\). 其中 \\(p(x)\\) 为 \\(x\\) 发生的概率. 对于概率分布 \\(P\\)，定义香农熵 \\(H(x)=E_{x\\sim P}[I(x)]=-E_{x\\sim P}[\\log P(x)]\\). KL 散度：对于两个分布 \\(P,Q\\)，定义 KL 散度 \\(D(P||Q)=E_{x\\sim P}[\\log P(x)/Q(x)]\\). 在信息论，中可以看出该式子的意义是，如果我们使用拟合分布 \\(Q\\) 比真实分布 \\(P\\) 少了多少信息. 采用 \\(D(P||Q)\\) 保证了 \\(P\\) 中低概率的部分拟合的更好，\\(D(Q||P)\\) 保证了高概率的部分拟合的更好. 交叉熵：\\(H(P,Q)=H(P)+D(P||Q)\\). 通常来说，由于 \\(H(P)\\) 是常量，故本质上还是交叉熵. 线性回归（Linear Regression） 令样本 \\((\\mathbb x_i,y_i)\\)，模型为 \\(f(\\mathbb x_i)=\\mathbb w\\mathbb x_i+b\\). 令 \\(A_i=(\\mathbb x_i | 1)\\)，\\(\\beta=(\\mathbb w^{T}|b)^{T}\\)，那么损失 函数即为 \\(\\frac{1}{2}\\sum (A_i\\beta-y_i)^2=\\frac{1}{2}(A\\beta-Y)^{T}(A\\beta -Y)\\). 求导后可以发现最优解中 \\(A^{T}A\\beta=A^{T}Y\\). 但是 \\(A^TA\\) 不一定可逆. 但是由于这玩意儿是凸的所以梯度下降可以得到最优解.","path":"2025/02/24/AI基笔记-一些简单的数学/","date":"02-24","excerpt":"","tags":[{"name":"AI基","slug":"AI基","permalink":"http://lgswdn.github.io/tags/AI%E5%9F%BA/"}]},{"title":"高代笔记-SVD和广义逆","text":"首先回忆线性映射 \\(T:V\\to W\\) 的伴随 \\(T^*\\) 的相关性质. 这里 \\(V,W\\) 都是 \\(\\mathbb{R}\\) 上的有限维内积空间，用 \\((x|y)\\) 表示内积. \\(\\dim V=m\\)，\\(\\dim W=n\\)，\\(p=\\min\\{n,m\\}\\). 定义（伴随）：\\(T^*\\) 为满足 \\((Tv|w)=(v|T^*w)\\) 的线性映射. 可以证明 \\(T^*\\) 唯一，并且将 \\(T\\) 视作矩阵 \\(A\\)，则 \\(T^*\\) 的矩阵即为 \\(A^{T}\\). 从简单的矩阵乘法即可知道 \\(T^*T\\) 自伴. 并且，\\((T^*Tv|v)=(Tv|Tv)\\ge 0\\)，故 \\(T^*T\\) 半正定. 并且我们还可以知道 \\(\\ker T=\\ker T^*T\\). 首先后者包含前者是显然的. 而对于 \\(v\\in \\ker T^*T\\)，\\(0=(T^*Tv|v)=(Tv|Tv)\\)，于是 \\(v\\) 也显然 \\(\\in \\ker T\\). 由此，我们也可以知道 \\(\\text{rk }T=\\text{rk }T^*T\\). 然后我们进入 Singular Value Decomposition，即奇异值分解. 定义（SVD）：存在 \\(V\\) 的一组单位正交基 \\(P=(v_1|\\dots|v_m)\\)，\\(W\\) 的一组单位正交基 \\(Q=(w_1|...|w_n)\\)，非负实数 \\(\\sigma_1\\ge\\dots\\ge\\sigma_p\\)，使得 \\(Tv_j=\\sigma_jw_j\\)（对于 \\(j\\le p\\)），且 \\(Tv_j=0\\)（对于 \\(j&gt;p\\)）. 用矩阵描述，即 \\(A=Q\\Sigma P^{T}\\)，或 \\(\\Sigma=PAQ^{T}\\). \\(\\Sigma\\) 为对角线递减的对角矩阵，且唯一存在. 我们会发现奇异值分解是一种和“正交对角化”有所相似的操作. 实际上，由于可以推出 \\(A^T=P\\Sigma Q^{T}\\)，简单带入即可发现 \\(T^*Tv_j=T^*\\sigma_jw_j=\\sigma^2_jv_j\\). 所以 \\(\\sigma_j^2\\) 为 \\(T^*T\\) 的所有非零特征值，并且配有特征向量 \\(v_j\\). 同理可以得到 \\(w_j\\) 为 \\(TT^*\\) 的特征向量. 将两者扩成 \\(V,W\\) 的基即可完成分解. 这个过程同时证明了存在性和唯一性. 并且非常地构造性. 定义（广义逆）：存在 \\(T\\) 的广义逆 \\(S: W\\to V\\)，使得 \\(TS|_{\\text{im } T}=id|_{\\text{im T}}\\). 换句话说，\\(TST=T\\). 普通的广义逆的存在性很好说明. 将 \\(\\text{im }T\\) 的基 \\(w_1,\\dots,w_r\\) 扩充为 \\(W\\) 的基，那么显然对于 \\(j\\le r\\) 存在 \\(Tv_j=w_j\\)，然后再将 \\(v\\) 扩充为 \\(V\\) 的基即可. 定义（Moore-Penrose 广义逆）：再上述条件下，还需满足：\\(STS=S\\)，\\(TST=T\\)，\\((TS)=(TS)^*\\)，\\((ST)=(ST)^*\\). 这样 \\(S\\) 存在且唯一. 一个想法是参考 \\(Tv=w\\) 的最小二乘解. 构造最小二乘解的过程中，我们先取 \\(w\\) 对 \\(\\text{im }T\\) 的投影 \\(w&#39;\\)，然后再取 \\(Tv&#39;=w&#39;\\)，并取 \\(v&#39;\\) 对 \\((\\ker T)^{\\bot}\\) 的投影就行了. 于是，\\(ST\\) 是 \\(V\\to (\\ker T)^{\\bot}\\) 的正交投影，\\(TS\\) 是 \\(W\\to \\text{im }T\\) 的正交投影，故自伴. 唯一性的证明更加神秘. 假若 \\(S,R\\) 都是 \\(T\\) 的广义逆，那么会发生： \\(S=STS=S(TS)^*=SS^*T^*=SS^*T^*R^*T^*=STSTR=STR\\) 但同理，我们也能知道 \\(R=STR\\). 所以 \\(S=R\\). 下面考虑如何求解该广义逆. 实际上，取奇异值分解 \\(T=Q\\Sigma P^{T}\\) 后，取 \\(S=P\\Sigma^{-1}Q^{T}\\) 就可以了. 其中对 \\(\\Sigma\\) 的取逆只需操作非零项. 看上去也十分可以理解. 而我们考虑一下这个本质. 实际上奇异值分解所作的就是分别建立了 \\(V,W\\) 的\"坐标系\"，然后再在这个坐标系上用一个简单的伸缩表达 \\(T\\). 这样来看，广义逆中有关正交投影的部分，就和上述操作不谋而合了. 参考讲义：李文威-代数学讲义","path":"2025/02/22/高代笔记-SVD和广义逆/","date":"02-22","excerpt":"","tags":[{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"}]},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment \\[ math \\text{test}\\pm \\]","path":"2025/02/22/hello-world/","date":"02-22","excerpt":"","tags":[]}],"categories":[],"tags":[{"name":"ICS","slug":"ICS","permalink":"http://lgswdn.github.io/tags/ICS/"},{"name":"具身","slug":"具身","permalink":"http://lgswdn.github.io/tags/%E5%85%B7%E8%BA%AB/"},{"name":"腾讯星火营","slug":"腾讯星火营","permalink":"http://lgswdn.github.io/tags/%E8%85%BE%E8%AE%AF%E6%98%9F%E7%81%AB%E8%90%A5/"},{"name":"中国的亚洲内陆边疆","slug":"中国的亚洲内陆边疆","permalink":"http://lgswdn.github.io/tags/%E4%B8%AD%E5%9B%BD%E7%9A%84%E4%BA%9A%E6%B4%B2%E5%86%85%E9%99%86%E8%BE%B9%E7%96%86/"},{"name":"波伏娃","slug":"波伏娃","permalink":"http://lgswdn.github.io/tags/%E6%B3%A2%E4%BC%8F%E5%A8%83/"},{"name":"总结","slug":"总结","permalink":"http://lgswdn.github.io/tags/%E6%80%BB%E7%BB%93/"},{"name":"数分","slug":"数分","permalink":"http://lgswdn.github.io/tags/%E6%95%B0%E5%88%86/"},{"name":"高代","slug":"高代","permalink":"http://lgswdn.github.io/tags/%E9%AB%98%E4%BB%A3/"},{"name":"CVDL","slug":"CVDL","permalink":"http://lgswdn.github.io/tags/CVDL/"},{"name":"音数","slug":"音数","permalink":"http://lgswdn.github.io/tags/%E9%9F%B3%E6%95%B0/"},{"name":"程设","slug":"程设","permalink":"http://lgswdn.github.io/tags/%E7%A8%8B%E8%AE%BE/"},{"name":"AI基","slug":"AI基","permalink":"http://lgswdn.github.io/tags/AI%E5%9F%BA/"}]}